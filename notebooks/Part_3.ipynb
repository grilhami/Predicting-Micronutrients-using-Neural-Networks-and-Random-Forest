{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to prepare dataset\n",
    "from utils import read_and_clean_data, split_X_y\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and clean data\n",
    "dataset = read_and_clean_data(\"../data/ABBREV.xlsx\")\n",
    "\n",
    "# Split dataset into X and y\n",
    "X, y = split_X_y(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scl_X = RobustScaler().fit(X_train)\n",
    "scl_y = RobustScaler().fit(y_train)\n",
    "\n",
    "input_x_train = scl_X.transform(X_train)\n",
    "input_y_train = scl_y.transform(y_train)\n",
    "\n",
    "input_x_test = scl_X.transform(X_test)\n",
    "input_y_test = scl_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies For Training In Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Wandb To Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/gilangrilhami/Predicting-Micronutrients-using-Neural-Networks-and-Random-Forest-notebooks/runs/iarsoaiu\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.init()\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paramaters for training\n",
    "\n",
    "config.batch_size = 128\n",
    "config.epochs = 300\n",
    "config.learning_rate = 0.01\n",
    "config.dropout = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 layers neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 01:50:53.306798 140520423556928 deprecation_wrapper.py:119] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 01:50:54.988280 140520423556928 deprecation_wrapper.py:119] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the optimization method and cirterion\n",
    "\n",
    "adam = Adam(lr=config.learning_rate)\n",
    "model.compile(loss=mean_squared_error, optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 01:50:57.797777 140520423556928 deprecation_wrapper.py:119] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0727 01:50:57.803678 140520423556928 deprecation_wrapper.py:119] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0727 01:50:57.821817 140520423556928 deprecation_wrapper.py:119] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0727 01:50:57.830569 140520423556928 deprecation.py:506] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0727 01:50:57.831727 140520423556928 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0727 01:50:57.866065 140520423556928 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0727 01:50:58.008706 140520423556928 deprecation_wrapper.py:119] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7471 samples, validate on 1319 samples\n",
      "Epoch 1/300\n",
      "7471/7471 [==============================] - 1s 67us/step - loss: 7680176.9358 - acc: 0.2361 - val_loss: 979.0614 - val_acc: 0.2934\n",
      "Epoch 2/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 17714.7905 - acc: 0.2864 - val_loss: 977.6166 - val_acc: 0.2934\n",
      "Epoch 3/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 3462.1800 - acc: 0.2934 - val_loss: 976.9673 - val_acc: 0.2934\n",
      "Epoch 4/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 3462.1024 - acc: 0.2966 - val_loss: 974.3980 - val_acc: 0.2934\n",
      "Epoch 5/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 5302.1298 - acc: 0.3010 - val_loss: 975.1442 - val_acc: 0.2934\n",
      "Epoch 6/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 9686.8658 - acc: 0.2998 - val_loss: 973.5807 - val_acc: 0.2934\n",
      "Epoch 7/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 3515.2233 - acc: 0.3033 - val_loss: 971.9175 - val_acc: 0.2934\n",
      "Epoch 8/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 3270.8928 - acc: 0.3034 - val_loss: 969.5229 - val_acc: 0.2934\n",
      "Epoch 9/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2985.9313 - acc: 0.3033 - val_loss: 968.0038 - val_acc: 0.2934\n",
      "Epoch 10/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2934.0047 - acc: 0.3032 - val_loss: 966.3388 - val_acc: 0.2934\n",
      "Epoch 11/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2924.4441 - acc: 0.3045 - val_loss: 964.3771 - val_acc: 0.2934\n",
      "Epoch 12/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2943.2307 - acc: 0.3041 - val_loss: 962.4572 - val_acc: 0.2934\n",
      "Epoch 13/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 3334.5216 - acc: 0.3036 - val_loss: 960.5929 - val_acc: 0.2934\n",
      "Epoch 14/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2921.9360 - acc: 0.3041 - val_loss: 958.8500 - val_acc: 0.2934\n",
      "Epoch 15/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2932.7563 - acc: 0.3041 - val_loss: 957.1986 - val_acc: 0.2934\n",
      "Epoch 16/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2915.6639 - acc: 0.3034 - val_loss: 955.6114 - val_acc: 0.2934\n",
      "Epoch 17/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2936.1540 - acc: 0.3034 - val_loss: 954.2706 - val_acc: 0.2934\n",
      "Epoch 18/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 6242.3989 - acc: 0.3040 - val_loss: 952.8014 - val_acc: 0.2934\n",
      "Epoch 19/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2918.2574 - acc: 0.3034 - val_loss: 951.7383 - val_acc: 0.2934\n",
      "Epoch 20/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2918.3353 - acc: 0.3036 - val_loss: 950.1340 - val_acc: 0.2934\n",
      "Epoch 21/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2909.1773 - acc: 0.3038 - val_loss: 949.7520 - val_acc: 0.2934\n",
      "Epoch 22/300\n",
      "7471/7471 [==============================] - 0s 39us/step - loss: 2917.9139 - acc: 0.3038 - val_loss: 949.1742 - val_acc: 0.2934\n",
      "Epoch 23/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2910.8188 - acc: 0.3034 - val_loss: 948.5202 - val_acc: 0.2934\n",
      "Epoch 24/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 5651.5683 - acc: 0.3037 - val_loss: 947.9151 - val_acc: 0.2934\n",
      "Epoch 25/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2924.4608 - acc: 0.3040 - val_loss: 947.5149 - val_acc: 0.2934\n",
      "Epoch 26/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2904.5709 - acc: 0.3034 - val_loss: 947.1689 - val_acc: 0.2934\n",
      "Epoch 27/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2910.5736 - acc: 0.3040 - val_loss: 946.9083 - val_acc: 0.2934\n",
      "Epoch 28/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2903.7225 - acc: 0.3038 - val_loss: 946.6312 - val_acc: 0.2934\n",
      "Epoch 29/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2908.5558 - acc: 0.3038 - val_loss: 946.4496 - val_acc: 0.2934\n",
      "Epoch 30/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2907.6770 - acc: 0.3034 - val_loss: 946.3037 - val_acc: 0.2934\n",
      "Epoch 31/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2903.7543 - acc: 0.3034 - val_loss: 946.1442 - val_acc: 0.2934\n",
      "Epoch 32/300\n",
      "7471/7471 [==============================] - 0s 42us/step - loss: 2993.7645 - acc: 0.3028 - val_loss: 946.1146 - val_acc: 0.2934\n",
      "Epoch 33/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 732520909.8061 - acc: 0.3037 - val_loss: 946.0824 - val_acc: 0.2934\n",
      "Epoch 34/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2903.0486 - acc: 0.3038 - val_loss: 946.1491 - val_acc: 0.2934\n",
      "Epoch 35/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2903.0096 - acc: 0.3034 - val_loss: 946.0223 - val_acc: 0.2934\n",
      "Epoch 36/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2902.8894 - acc: 0.3041 - val_loss: 946.1331 - val_acc: 0.2934\n",
      "Epoch 37/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 3033.4365 - acc: 0.3038 - val_loss: 946.1386 - val_acc: 0.2934\n",
      "Epoch 38/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2910.1404 - acc: 0.3041 - val_loss: 946.1353 - val_acc: 0.2934\n",
      "Epoch 39/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2903.9226 - acc: 0.3041 - val_loss: 946.1319 - val_acc: 0.2934\n",
      "Epoch 40/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2909.7574 - acc: 0.3041 - val_loss: 946.1276 - val_acc: 0.2934\n",
      "Epoch 41/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2907.4039 - acc: 0.3042 - val_loss: 946.1235 - val_acc: 0.2934\n",
      "Epoch 42/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.6418 - acc: 0.3041 - val_loss: 946.1190 - val_acc: 0.2934\n",
      "Epoch 43/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2902.7503 - acc: 0.3037 - val_loss: 946.1006 - val_acc: 0.2934\n",
      "Epoch 44/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2925.1049 - acc: 0.3036 - val_loss: 946.1074 - val_acc: 0.2934\n",
      "Epoch 45/300\n",
      "7471/7471 [==============================] - 0s 38us/step - loss: 2982.1731 - acc: 0.3041 - val_loss: 946.1004 - val_acc: 0.2934\n",
      "Epoch 46/300\n",
      "7471/7471 [==============================] - 0s 50us/step - loss: 2904.0130 - acc: 0.3040 - val_loss: 946.0937 - val_acc: 0.2934\n",
      "Epoch 47/300\n",
      "7471/7471 [==============================] - 0s 51us/step - loss: 2927.8560 - acc: 0.3036 - val_loss: 946.0879 - val_acc: 0.2934\n",
      "Epoch 48/300\n",
      "7471/7471 [==============================] - 0s 39us/step - loss: 2910.3517 - acc: 0.3042 - val_loss: 946.0798 - val_acc: 0.2934\n",
      "Epoch 49/300\n",
      "7471/7471 [==============================] - 0s 41us/step - loss: 2902.4652 - acc: 0.3040 - val_loss: 946.0699 - val_acc: 0.2934\n",
      "Epoch 50/300\n",
      "7471/7471 [==============================] - 0s 37us/step - loss: 2911.9352 - acc: 0.3038 - val_loss: 946.0600 - val_acc: 0.2934\n",
      "Epoch 51/300\n",
      "7471/7471 [==============================] - 0s 51us/step - loss: 2902.7025 - acc: 0.3036 - val_loss: 946.0505 - val_acc: 0.2934\n",
      "Epoch 52/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2902.4227 - acc: 0.3036 - val_loss: 946.0408 - val_acc: 0.2934\n",
      "Epoch 53/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.4225 - acc: 0.3038 - val_loss: 946.0310 - val_acc: 0.2934\n",
      "Epoch 54/300\n",
      "7471/7471 [==============================] - 0s 47us/step - loss: 2904.3645 - acc: 0.3040 - val_loss: 946.0204 - val_acc: 0.2934\n",
      "Epoch 55/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2903.1075 - acc: 0.3037 - val_loss: 946.0096 - val_acc: 0.2934\n",
      "Epoch 56/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2907.0929 - acc: 0.3040 - val_loss: 945.9981 - val_acc: 0.2934\n",
      "Epoch 57/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 3000.1563 - acc: 0.3040 - val_loss: 945.9880 - val_acc: 0.2934\n",
      "Epoch 58/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2902.3093 - acc: 0.3038 - val_loss: 945.9788 - val_acc: 0.2934\n",
      "Epoch 59/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2905.5582 - acc: 0.3040 - val_loss: 945.9714 - val_acc: 0.2934\n",
      "Epoch 60/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.1882 - acc: 0.3040 - val_loss: 945.9644 - val_acc: 0.2934\n",
      "Epoch 61/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2902.1438 - acc: 0.3040 - val_loss: 945.9589 - val_acc: 0.2934\n",
      "Epoch 62/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2902.1439 - acc: 0.3040 - val_loss: 945.9546 - val_acc: 0.2934\n",
      "Epoch 63/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2920.4439 - acc: 0.3041 - val_loss: 945.9524 - val_acc: 0.2934\n",
      "Epoch 64/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 3355.6770 - acc: 0.3040 - val_loss: 945.9476 - val_acc: 0.2934\n",
      "Epoch 65/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.0543 - acc: 0.3038 - val_loss: 945.9479 - val_acc: 0.2934\n",
      "Epoch 66/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.0215 - acc: 0.3040 - val_loss: 945.9490 - val_acc: 0.2934\n",
      "Epoch 67/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2956.3498 - acc: 0.3038 - val_loss: 945.9534 - val_acc: 0.2934\n",
      "Epoch 68/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2903.0318 - acc: 0.3038 - val_loss: 945.9577 - val_acc: 0.2934\n",
      "Epoch 69/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2902.1254 - acc: 0.3040 - val_loss: 945.9618 - val_acc: 0.2934\n",
      "Epoch 70/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 3693.7376 - acc: 0.3040 - val_loss: 945.9699 - val_acc: 0.2934\n",
      "Epoch 71/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2910.8732 - acc: 0.3041 - val_loss: 945.9742 - val_acc: 0.2934\n",
      "Epoch 72/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2949.5589 - acc: 0.3041 - val_loss: 945.9842 - val_acc: 0.2934\n",
      "Epoch 73/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.0274 - acc: 0.3040 - val_loss: 945.9868 - val_acc: 0.2934\n",
      "Epoch 74/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2953.5594 - acc: 0.3037 - val_loss: 945.9921 - val_acc: 0.2934\n",
      "Epoch 75/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2908.0391 - acc: 0.3040 - val_loss: 946.0018 - val_acc: 0.2934\n",
      "Epoch 76/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 3085.7209 - acc: 0.3041 - val_loss: 946.0115 - val_acc: 0.2934\n",
      "Epoch 77/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9234 - acc: 0.3038 - val_loss: 946.0144 - val_acc: 0.2934\n",
      "Epoch 78/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 3032.5540 - acc: 0.3041 - val_loss: 946.0202 - val_acc: 0.2934\n",
      "Epoch 79/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2902.3471 - acc: 0.3040 - val_loss: 946.0290 - val_acc: 0.2934\n",
      "Epoch 80/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.8928 - acc: 0.3040 - val_loss: 946.0347 - val_acc: 0.2934\n",
      "Epoch 81/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.9696 - acc: 0.3038 - val_loss: 946.0407 - val_acc: 0.2934\n",
      "Epoch 82/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2903.8175 - acc: 0.3038 - val_loss: 946.0426 - val_acc: 0.2934\n",
      "Epoch 83/300\n",
      "7471/7471 [==============================] - 0s 40us/step - loss: 2901.9063 - acc: 0.3038 - val_loss: 946.0476 - val_acc: 0.2934\n",
      "Epoch 84/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2902.1105 - acc: 0.3038 - val_loss: 946.0444 - val_acc: 0.2934\n",
      "Epoch 85/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2905.1798 - acc: 0.3038 - val_loss: 946.0582 - val_acc: 0.2934\n",
      "Epoch 86/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9841 - acc: 0.3040 - val_loss: 946.0590 - val_acc: 0.2934\n",
      "Epoch 87/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8767 - acc: 0.3040 - val_loss: 946.0669 - val_acc: 0.2934\n",
      "Epoch 88/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 3324.6572 - acc: 0.3040 - val_loss: 946.0954 - val_acc: 0.2934\n",
      "Epoch 89/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.0781 - acc: 0.3037 - val_loss: 946.0734 - val_acc: 0.2934\n",
      "Epoch 90/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2902.0118 - acc: 0.3040 - val_loss: 946.0670 - val_acc: 0.2934\n",
      "Epoch 91/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2922.0355 - acc: 0.3040 - val_loss: 946.0735 - val_acc: 0.2934\n",
      "Epoch 92/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9654 - acc: 0.3040 - val_loss: 946.0581 - val_acc: 0.2934\n",
      "Epoch 93/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8892 - acc: 0.3040 - val_loss: 946.0627 - val_acc: 0.2934\n",
      "Epoch 94/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8934 - acc: 0.3040 - val_loss: 946.0682 - val_acc: 0.2934\n",
      "Epoch 95/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9426 - acc: 0.3040 - val_loss: 946.0704 - val_acc: 0.2934\n",
      "Epoch 96/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2905.7971 - acc: 0.3040 - val_loss: 946.0644 - val_acc: 0.2934\n",
      "Epoch 97/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8904 - acc: 0.3040 - val_loss: 946.0764 - val_acc: 0.2934\n",
      "Epoch 98/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2907.3561 - acc: 0.3040 - val_loss: 946.0794 - val_acc: 0.2934\n",
      "Epoch 99/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2905.7221 - acc: 0.3037 - val_loss: 946.0838 - val_acc: 0.2934\n",
      "Epoch 100/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9141 - acc: 0.3040 - val_loss: 946.0850 - val_acc: 0.2934\n",
      "Epoch 101/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8908 - acc: 0.3041 - val_loss: 946.0868 - val_acc: 0.2934\n",
      "Epoch 102/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9119 - acc: 0.3040 - val_loss: 946.0787 - val_acc: 0.2934\n",
      "Epoch 103/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9644 - acc: 0.3037 - val_loss: 946.0746 - val_acc: 0.2934\n",
      "Epoch 104/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8948 - acc: 0.3040 - val_loss: 946.0896 - val_acc: 0.2934\n",
      "Epoch 105/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 3210.2182 - acc: 0.3040 - val_loss: 946.1094 - val_acc: 0.2934\n",
      "Epoch 106/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9789 - acc: 0.3038 - val_loss: 946.0957 - val_acc: 0.2934\n",
      "Epoch 107/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2903.2484 - acc: 0.3040 - val_loss: 946.1025 - val_acc: 0.2934\n",
      "Epoch 108/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2902.0040 - acc: 0.3036 - val_loss: 946.1031 - val_acc: 0.2934\n",
      "Epoch 109/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8945 - acc: 0.3040 - val_loss: 946.1044 - val_acc: 0.2934\n",
      "Epoch 110/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8983 - acc: 0.3040 - val_loss: 946.0992 - val_acc: 0.2934\n",
      "Epoch 111/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8851 - acc: 0.3040 - val_loss: 946.0980 - val_acc: 0.2934\n",
      "Epoch 112/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8980 - acc: 0.3040 - val_loss: 946.0974 - val_acc: 0.2934\n",
      "Epoch 113/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9497 - acc: 0.3038 - val_loss: 946.0933 - val_acc: 0.2934\n",
      "Epoch 114/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9879 - acc: 0.3038 - val_loss: 946.1000 - val_acc: 0.2934\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.0054 - acc: 0.3040 - val_loss: 946.1021 - val_acc: 0.2934\n",
      "Epoch 116/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8804 - acc: 0.3040 - val_loss: 946.0995 - val_acc: 0.2934\n",
      "Epoch 117/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8836 - acc: 0.3040 - val_loss: 946.0612 - val_acc: 0.2934\n",
      "Epoch 118/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2902.2504 - acc: 0.3038 - val_loss: 946.0913 - val_acc: 0.2934\n",
      "Epoch 119/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.7940 - acc: 0.3040 - val_loss: 946.0872 - val_acc: 0.2934\n",
      "Epoch 120/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8913 - acc: 0.3040 - val_loss: 946.0846 - val_acc: 0.2934\n",
      "Epoch 121/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2902.2741 - acc: 0.3038 - val_loss: 946.1038 - val_acc: 0.2934\n",
      "Epoch 122/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2916.4926 - acc: 0.3040 - val_loss: 946.0992 - val_acc: 0.2934\n",
      "Epoch 123/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2901.8756 - acc: 0.3040 - val_loss: 946.1043 - val_acc: 0.2934\n",
      "Epoch 124/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8831 - acc: 0.3040 - val_loss: 946.1044 - val_acc: 0.2934\n",
      "Epoch 125/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.0491 - acc: 0.3040 - val_loss: 946.0870 - val_acc: 0.2934\n",
      "Epoch 126/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9546 - acc: 0.3038 - val_loss: 946.0918 - val_acc: 0.2934\n",
      "Epoch 127/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9183 - acc: 0.3038 - val_loss: 946.0962 - val_acc: 0.2934\n",
      "Epoch 128/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8860 - acc: 0.3040 - val_loss: 946.0975 - val_acc: 0.2934\n",
      "Epoch 129/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9521 - acc: 0.3038 - val_loss: 946.0800 - val_acc: 0.2934\n",
      "Epoch 130/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2902.9885 - acc: 0.3038 - val_loss: 946.0921 - val_acc: 0.2934\n",
      "Epoch 131/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 3066.4750 - acc: 0.3040 - val_loss: 946.1387 - val_acc: 0.2934\n",
      "Epoch 132/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 3015.3510 - acc: 0.3038 - val_loss: 946.1357 - val_acc: 0.2934\n",
      "Epoch 133/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2902.1990 - acc: 0.3038 - val_loss: 946.1075 - val_acc: 0.2934\n",
      "Epoch 134/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2902.1036 - acc: 0.3040 - val_loss: 946.1025 - val_acc: 0.2934\n",
      "Epoch 135/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9557 - acc: 0.3038 - val_loss: 946.0956 - val_acc: 0.2934\n",
      "Epoch 136/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9170 - acc: 0.3040 - val_loss: 946.0985 - val_acc: 0.2934\n",
      "Epoch 137/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9266 - acc: 0.3041 - val_loss: 946.0918 - val_acc: 0.2934\n",
      "Epoch 138/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8969 - acc: 0.3040 - val_loss: 946.0985 - val_acc: 0.2934\n",
      "Epoch 139/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8846 - acc: 0.3040 - val_loss: 946.0995 - val_acc: 0.2934\n",
      "Epoch 140/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9946 - acc: 0.3038 - val_loss: 946.0911 - val_acc: 0.2934\n",
      "Epoch 141/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2930.6589 - acc: 0.3040 - val_loss: 946.1039 - val_acc: 0.2934\n",
      "Epoch 142/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2902.0242 - acc: 0.3040 - val_loss: 946.0725 - val_acc: 0.2934\n",
      "Epoch 143/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8937 - acc: 0.3040 - val_loss: 946.0826 - val_acc: 0.2934\n",
      "Epoch 144/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8813 - acc: 0.3040 - val_loss: 946.0940 - val_acc: 0.2934\n",
      "Epoch 145/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8994 - acc: 0.3040 - val_loss: 946.0908 - val_acc: 0.2934\n",
      "Epoch 146/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8886 - acc: 0.3040 - val_loss: 946.1010 - val_acc: 0.2934\n",
      "Epoch 147/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8805 - acc: 0.3040 - val_loss: 946.0882 - val_acc: 0.2934\n",
      "Epoch 148/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2910.9287 - acc: 0.3040 - val_loss: 946.0927 - val_acc: 0.2934\n",
      "Epoch 149/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8856 - acc: 0.3040 - val_loss: 946.0985 - val_acc: 0.2934\n",
      "Epoch 150/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9275 - acc: 0.3040 - val_loss: 946.1083 - val_acc: 0.2934\n",
      "Epoch 151/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 3512.1658 - acc: 0.3040 - val_loss: 946.1305 - val_acc: 0.2934\n",
      "Epoch 152/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9378 - acc: 0.3038 - val_loss: 946.1064 - val_acc: 0.2934\n",
      "Epoch 153/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8908 - acc: 0.3040 - val_loss: 946.1093 - val_acc: 0.2934\n",
      "Epoch 154/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9015 - acc: 0.3038 - val_loss: 946.1182 - val_acc: 0.2934\n",
      "Epoch 155/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2901.8847 - acc: 0.3040 - val_loss: 946.1078 - val_acc: 0.2934\n",
      "Epoch 156/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8923 - acc: 0.3040 - val_loss: 946.1062 - val_acc: 0.2934\n",
      "Epoch 157/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8930 - acc: 0.3040 - val_loss: 946.1025 - val_acc: 0.2934\n",
      "Epoch 158/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8966 - acc: 0.3040 - val_loss: 946.0615 - val_acc: 0.2934\n",
      "Epoch 159/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8879 - acc: 0.3040 - val_loss: 946.1236 - val_acc: 0.2934\n",
      "Epoch 160/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8755 - acc: 0.3040 - val_loss: 946.1331 - val_acc: 0.2934\n",
      "Epoch 161/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.0161 - acc: 0.3040 - val_loss: 946.1325 - val_acc: 0.2934\n",
      "Epoch 162/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8754 - acc: 0.3040 - val_loss: 946.1301 - val_acc: 0.2934\n",
      "Epoch 163/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8900 - acc: 0.3040 - val_loss: 946.1101 - val_acc: 0.2934\n",
      "Epoch 164/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8908 - acc: 0.3040 - val_loss: 946.1250 - val_acc: 0.2934\n",
      "Epoch 165/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8867 - acc: 0.3040 - val_loss: 946.1208 - val_acc: 0.2934\n",
      "Epoch 166/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9388 - acc: 0.3040 - val_loss: 946.1143 - val_acc: 0.2934\n",
      "Epoch 167/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.1070 - val_acc: 0.2934\n",
      "Epoch 168/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9646 - acc: 0.3038 - val_loss: 946.1048 - val_acc: 0.2934\n",
      "Epoch 169/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8929 - acc: 0.3038 - val_loss: 946.1108 - val_acc: 0.2934\n",
      "Epoch 170/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8808 - acc: 0.3040 - val_loss: 946.1184 - val_acc: 0.2934\n",
      "Epoch 171/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8775 - acc: 0.3040 - val_loss: 946.1038 - val_acc: 0.2934\n",
      "Epoch 172/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8984 - acc: 0.3040 - val_loss: 946.1063 - val_acc: 0.2934\n",
      "Epoch 173/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8850 - acc: 0.3040 - val_loss: 946.1136 - val_acc: 0.2934\n",
      "Epoch 174/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8848 - acc: 0.3040 - val_loss: 946.0977 - val_acc: 0.2934\n",
      "Epoch 175/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8875 - acc: 0.3040 - val_loss: 946.1028 - val_acc: 0.2934\n",
      "Epoch 176/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2902.0171 - acc: 0.3040 - val_loss: 946.1123 - val_acc: 0.2934\n",
      "Epoch 177/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8814 - acc: 0.3040 - val_loss: 946.1079 - val_acc: 0.2934\n",
      "Epoch 178/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8904 - acc: 0.3040 - val_loss: 946.1041 - val_acc: 0.2934\n",
      "Epoch 179/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2901.8862 - acc: 0.3040 - val_loss: 946.1024 - val_acc: 0.2934\n",
      "Epoch 180/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2903.9191 - acc: 0.3040 - val_loss: 946.1016 - val_acc: 0.2934\n",
      "Epoch 181/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8972 - acc: 0.3040 - val_loss: 946.1014 - val_acc: 0.2934\n",
      "Epoch 182/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2946.7125 - acc: 0.3038 - val_loss: 946.0930 - val_acc: 0.2934\n",
      "Epoch 183/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.8844 - acc: 0.3040 - val_loss: 946.0993 - val_acc: 0.2934\n",
      "Epoch 184/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2901.9292 - acc: 0.3040 - val_loss: 946.0951 - val_acc: 0.2934\n",
      "Epoch 185/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9894 - acc: 0.3038 - val_loss: 946.0922 - val_acc: 0.2934\n",
      "Epoch 186/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.8864 - acc: 0.3040 - val_loss: 946.0920 - val_acc: 0.2934\n",
      "Epoch 187/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8943 - acc: 0.3040 - val_loss: 946.0951 - val_acc: 0.2934\n",
      "Epoch 188/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8880 - acc: 0.3040 - val_loss: 946.0795 - val_acc: 0.2934\n",
      "Epoch 189/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.1375 - acc: 0.3040 - val_loss: 946.0677 - val_acc: 0.2934\n",
      "Epoch 190/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9092 - acc: 0.3040 - val_loss: 946.1054 - val_acc: 0.2934\n",
      "Epoch 191/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8978 - acc: 0.3040 - val_loss: 946.0981 - val_acc: 0.2934\n",
      "Epoch 192/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8851 - acc: 0.3040 - val_loss: 946.0913 - val_acc: 0.2934\n",
      "Epoch 193/300\n",
      "7471/7471 [==============================] - 0s 37us/step - loss: 2901.8990 - acc: 0.3040 - val_loss: 946.0825 - val_acc: 0.2934\n",
      "Epoch 194/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8951 - acc: 0.3040 - val_loss: 946.1031 - val_acc: 0.2934\n",
      "Epoch 195/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8783 - acc: 0.3040 - val_loss: 946.0912 - val_acc: 0.2934\n",
      "Epoch 196/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8760 - acc: 0.3040 - val_loss: 946.1053 - val_acc: 0.2934\n",
      "Epoch 197/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9019 - acc: 0.3040 - val_loss: 946.0848 - val_acc: 0.2934\n",
      "Epoch 198/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8911 - acc: 0.3040 - val_loss: 946.1002 - val_acc: 0.2934\n",
      "Epoch 199/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8843 - acc: 0.3040 - val_loss: 946.0883 - val_acc: 0.2934\n",
      "Epoch 200/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8904 - acc: 0.3040 - val_loss: 946.0911 - val_acc: 0.2934\n",
      "Epoch 201/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9676 - acc: 0.3040 - val_loss: 946.0897 - val_acc: 0.2934\n",
      "Epoch 202/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8870 - acc: 0.3040 - val_loss: 946.0975 - val_acc: 0.2934\n",
      "Epoch 203/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8865 - acc: 0.3040 - val_loss: 946.0863 - val_acc: 0.2934\n",
      "Epoch 204/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8901 - acc: 0.3040 - val_loss: 946.0742 - val_acc: 0.2934\n",
      "Epoch 205/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8878 - acc: 0.3040 - val_loss: 946.0850 - val_acc: 0.2934\n",
      "Epoch 206/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8821 - acc: 0.3040 - val_loss: 946.0847 - val_acc: 0.2934\n",
      "Epoch 207/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8915 - acc: 0.3040 - val_loss: 946.0929 - val_acc: 0.2934\n",
      "Epoch 208/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.8972 - acc: 0.3040 - val_loss: 946.0505 - val_acc: 0.2934\n",
      "Epoch 209/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.8891 - acc: 0.3040 - val_loss: 946.0840 - val_acc: 0.2934\n",
      "Epoch 210/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.8813 - acc: 0.3040 - val_loss: 946.0853 - val_acc: 0.2934\n",
      "Epoch 211/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9256 - acc: 0.3038 - val_loss: 946.0926 - val_acc: 0.2934\n",
      "Epoch 212/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8810 - acc: 0.3040 - val_loss: 946.0809 - val_acc: 0.2934\n",
      "Epoch 213/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.8944 - acc: 0.3040 - val_loss: 946.0933 - val_acc: 0.2934\n",
      "Epoch 214/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8960 - acc: 0.3040 - val_loss: 946.0866 - val_acc: 0.2934\n",
      "Epoch 215/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2901.8794 - acc: 0.3040 - val_loss: 946.0959 - val_acc: 0.2934\n",
      "Epoch 216/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2950.3567 - acc: 0.3038 - val_loss: 946.0894 - val_acc: 0.2934\n",
      "Epoch 217/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8831 - acc: 0.3038 - val_loss: 946.0881 - val_acc: 0.2934\n",
      "Epoch 218/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2903.1420 - acc: 0.3040 - val_loss: 946.0833 - val_acc: 0.2934\n",
      "Epoch 219/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8842 - acc: 0.3040 - val_loss: 946.0948 - val_acc: 0.2934\n",
      "Epoch 220/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8866 - acc: 0.3040 - val_loss: 946.0966 - val_acc: 0.2934\n",
      "Epoch 221/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.5633 - acc: 0.3040 - val_loss: 946.0895 - val_acc: 0.2934\n",
      "Epoch 222/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.0913 - val_acc: 0.2934\n",
      "Epoch 223/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9135 - acc: 0.3040 - val_loss: 946.1032 - val_acc: 0.2934\n",
      "Epoch 224/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9289 - acc: 0.3040 - val_loss: 946.0991 - val_acc: 0.2934\n",
      "Epoch 225/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2902.6586 - acc: 0.3040 - val_loss: 946.1061 - val_acc: 0.2934\n",
      "Epoch 226/300\n",
      "7471/7471 [==============================] - 0s 38us/step - loss: 2901.8910 - acc: 0.3040 - val_loss: 946.1016 - val_acc: 0.2934\n",
      "Epoch 227/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8856 - acc: 0.3040 - val_loss: 946.1166 - val_acc: 0.2934\n",
      "Epoch 228/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8799 - acc: 0.3040 - val_loss: 946.1091 - val_acc: 0.2934\n",
      "Epoch 229/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 0s 35us/step - loss: 2901.8894 - acc: 0.3040 - val_loss: 946.1115 - val_acc: 0.2934\n",
      "Epoch 230/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.9670 - acc: 0.3038 - val_loss: 946.0980 - val_acc: 0.2934\n",
      "Epoch 231/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9003 - acc: 0.3040 - val_loss: 946.1072 - val_acc: 0.2934\n",
      "Epoch 232/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9816 - acc: 0.3040 - val_loss: 946.1073 - val_acc: 0.2934\n",
      "Epoch 233/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8825 - acc: 0.3040 - val_loss: 946.1091 - val_acc: 0.2934\n",
      "Epoch 234/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8935 - acc: 0.3040 - val_loss: 946.1049 - val_acc: 0.2934\n",
      "Epoch 235/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8826 - acc: 0.3040 - val_loss: 946.1183 - val_acc: 0.2934\n",
      "Epoch 236/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.9102 - acc: 0.3040 - val_loss: 946.1048 - val_acc: 0.2934\n",
      "Epoch 237/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9472 - acc: 0.3038 - val_loss: 946.1050 - val_acc: 0.2934\n",
      "Epoch 238/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9534 - acc: 0.3038 - val_loss: 946.1036 - val_acc: 0.2934\n",
      "Epoch 239/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8845 - acc: 0.3040 - val_loss: 946.0939 - val_acc: 0.2934\n",
      "Epoch 240/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9457 - acc: 0.3040 - val_loss: 946.1000 - val_acc: 0.2934\n",
      "Epoch 241/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8838 - acc: 0.3040 - val_loss: 946.1082 - val_acc: 0.2934\n",
      "Epoch 242/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.9005 - acc: 0.3040 - val_loss: 946.1012 - val_acc: 0.2934\n",
      "Epoch 243/300\n",
      "7471/7471 [==============================] - 0s 38us/step - loss: 2901.8810 - acc: 0.3040 - val_loss: 946.0970 - val_acc: 0.2934\n",
      "Epoch 244/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8925 - acc: 0.3040 - val_loss: 946.0951 - val_acc: 0.2934\n",
      "Epoch 245/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9697 - acc: 0.3037 - val_loss: 946.0982 - val_acc: 0.2934\n",
      "Epoch 246/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2902.3352 - acc: 0.3041 - val_loss: 946.1058 - val_acc: 0.2934\n",
      "Epoch 247/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8977 - acc: 0.3038 - val_loss: 946.0932 - val_acc: 0.2934\n",
      "Epoch 248/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9035 - acc: 0.3040 - val_loss: 946.0879 - val_acc: 0.2934\n",
      "Epoch 249/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2900.4742 - acc: 0.3040 - val_loss: 946.0991 - val_acc: 0.2934\n",
      "Epoch 250/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 3602.4686 - acc: 0.3038 - val_loss: 946.0946 - val_acc: 0.2934\n",
      "Epoch 251/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8894 - acc: 0.3040 - val_loss: 946.0878 - val_acc: 0.2934\n",
      "Epoch 252/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9950 - acc: 0.3037 - val_loss: 946.0850 - val_acc: 0.2934\n",
      "Epoch 253/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2902.2681 - acc: 0.3040 - val_loss: 946.0889 - val_acc: 0.2934\n",
      "Epoch 254/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.0046 - acc: 0.3041 - val_loss: 946.0925 - val_acc: 0.2934\n",
      "Epoch 255/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9227 - acc: 0.3041 - val_loss: 946.0978 - val_acc: 0.2934\n",
      "Epoch 256/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2912.8972 - acc: 0.3040 - val_loss: 946.0983 - val_acc: 0.2934\n",
      "Epoch 257/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9009 - acc: 0.3040 - val_loss: 946.1019 - val_acc: 0.2934\n",
      "Epoch 258/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8914 - acc: 0.3040 - val_loss: 946.0935 - val_acc: 0.2934\n",
      "Epoch 259/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8793 - acc: 0.3040 - val_loss: 946.0955 - val_acc: 0.2934\n",
      "Epoch 260/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 6222.3717 - acc: 0.3040 - val_loss: 946.1245 - val_acc: 0.2934\n",
      "Epoch 261/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8858 - acc: 0.3040 - val_loss: 946.1209 - val_acc: 0.2934\n",
      "Epoch 262/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9444 - acc: 0.3041 - val_loss: 946.1221 - val_acc: 0.2934\n",
      "Epoch 263/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8798 - acc: 0.3040 - val_loss: 946.1158 - val_acc: 0.2934\n",
      "Epoch 264/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8788 - acc: 0.3040 - val_loss: 946.1229 - val_acc: 0.2934\n",
      "Epoch 265/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8750 - acc: 0.3040 - val_loss: 946.1213 - val_acc: 0.2934\n",
      "Epoch 266/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 3947.1118 - acc: 0.3040 - val_loss: 946.1109 - val_acc: 0.2934\n",
      "Epoch 267/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2901.8862 - acc: 0.3038 - val_loss: 946.1140 - val_acc: 0.2934\n",
      "Epoch 268/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2902.0315 - acc: 0.3038 - val_loss: 946.1226 - val_acc: 0.2934\n",
      "Epoch 269/300\n",
      "7471/7471 [==============================] - 0s 47us/step - loss: 2901.8976 - acc: 0.3041 - val_loss: 946.1164 - val_acc: 0.2934\n",
      "Epoch 270/300\n",
      "7471/7471 [==============================] - 0s 50us/step - loss: 2901.9051 - acc: 0.3040 - val_loss: 946.1164 - val_acc: 0.2934\n",
      "Epoch 271/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2902.3414 - acc: 0.3037 - val_loss: 946.1063 - val_acc: 0.2934\n",
      "Epoch 272/300\n",
      "7471/7471 [==============================] - 0s 47us/step - loss: 2901.8995 - acc: 0.3040 - val_loss: 946.1208 - val_acc: 0.2934\n",
      "Epoch 273/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.8814 - acc: 0.3040 - val_loss: 946.1049 - val_acc: 0.2934\n",
      "Epoch 274/300\n",
      "7471/7471 [==============================] - 0s 38us/step - loss: 2901.8755 - acc: 0.3040 - val_loss: 946.1142 - val_acc: 0.2934\n",
      "Epoch 275/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.8889 - acc: 0.3038 - val_loss: 946.1083 - val_acc: 0.2934\n",
      "Epoch 276/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.8779 - acc: 0.3040 - val_loss: 946.1106 - val_acc: 0.2934\n",
      "Epoch 277/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.8792 - acc: 0.3040 - val_loss: 946.1140 - val_acc: 0.2934\n",
      "Epoch 278/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.1068 - val_acc: 0.2934\n",
      "Epoch 279/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2901.8928 - acc: 0.3040 - val_loss: 946.0922 - val_acc: 0.2934\n",
      "Epoch 280/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8774 - acc: 0.3040 - val_loss: 946.1109 - val_acc: 0.2934\n",
      "Epoch 281/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.0284 - acc: 0.3040 - val_loss: 946.0805 - val_acc: 0.2934\n",
      "Epoch 282/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2909.8923 - acc: 0.3038 - val_loss: 946.1291 - val_acc: 0.2934\n",
      "Epoch 283/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8773 - acc: 0.3040 - val_loss: 946.1363 - val_acc: 0.2934\n",
      "Epoch 284/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8741 - acc: 0.3038 - val_loss: 946.1349 - val_acc: 0.2934\n",
      "Epoch 285/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2952.1724 - acc: 0.3040 - val_loss: 946.1305 - val_acc: 0.2934\n",
      "Epoch 286/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.1268 - val_acc: 0.2934\n",
      "Epoch 287/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9691 - acc: 0.3040 - val_loss: 946.1218 - val_acc: 0.2934\n",
      "Epoch 288/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8893 - acc: 0.3040 - val_loss: 946.1228 - val_acc: 0.2934\n",
      "Epoch 289/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8884 - acc: 0.3040 - val_loss: 946.1129 - val_acc: 0.2934\n",
      "Epoch 290/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8840 - acc: 0.3040 - val_loss: 946.1210 - val_acc: 0.2934\n",
      "Epoch 291/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8795 - acc: 0.3040 - val_loss: 946.1250 - val_acc: 0.2934\n",
      "Epoch 292/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8811 - acc: 0.3040 - val_loss: 946.1119 - val_acc: 0.2934\n",
      "Epoch 293/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8818 - acc: 0.3040 - val_loss: 946.1129 - val_acc: 0.2934\n",
      "Epoch 294/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9941 - acc: 0.3040 - val_loss: 946.1028 - val_acc: 0.2934\n",
      "Epoch 295/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.6424 - acc: 0.3040 - val_loss: 946.0874 - val_acc: 0.2934\n",
      "Epoch 296/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8906 - acc: 0.3040 - val_loss: 946.0943 - val_acc: 0.2934\n",
      "Epoch 297/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8822 - acc: 0.3040 - val_loss: 946.1061 - val_acc: 0.2934\n",
      "Epoch 298/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8895 - acc: 0.3040 - val_loss: 946.0970 - val_acc: 0.2934\n",
      "Epoch 299/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9006 - acc: 0.3040 - val_loss: 946.0988 - val_acc: 0.2934\n",
      "Epoch 300/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.9387 - acc: 0.3041 - val_loss: 946.0868 - val_acc: 0.2934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd150c8630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_x_train, input_y_train, \n",
    "          epochs=config.epochs, validation_data=(input_x_test, input_y_test), \n",
    "          callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 layers neural network with less hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/gilangrilhami/Predicting-Micronutrients-using-Neural-Networks-and-Random-Forest-notebooks/runs/hlaok36j\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 01:54:39.116574 140520423556928 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0727 01:54:39.148196 140520423556928 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7471 samples, validate on 1319 samples\n",
      "Epoch 1/300\n",
      "7471/7471 [==============================] - 1s 69us/step - loss: 1363913.1867 - acc: 0.2397 - val_loss: 978.3026 - val_acc: 0.2934\n",
      "Epoch 2/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 4190.8955 - acc: 0.2850 - val_loss: 977.3681 - val_acc: 0.2934\n",
      "Epoch 3/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 17179949809.1264 - acc: 0.2399 - val_loss: 977.8725 - val_acc: 0.2146\n",
      "Epoch 4/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 3345.6782 - acc: 0.2101 - val_loss: 977.8575 - val_acc: 0.2146\n",
      "Epoch 5/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 3965.7001 - acc: 0.2144 - val_loss: 977.8610 - val_acc: 0.2146\n",
      "Epoch 6/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 6833.8979 - acc: 0.2142 - val_loss: 977.8547 - val_acc: 0.2146\n",
      "Epoch 7/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 3143.5718 - acc: 0.2131 - val_loss: 977.8403 - val_acc: 0.2146\n",
      "Epoch 8/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 135725490.6396 - acc: 0.2138 - val_loss: 977.8616 - val_acc: 0.2267\n",
      "Epoch 9/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2951.5479 - acc: 0.2214 - val_loss: 977.2624 - val_acc: 0.2949\n",
      "Epoch 10/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 37080.2168 - acc: 0.2518 - val_loss: 977.8691 - val_acc: 0.2934\n",
      "Epoch 11/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2960.2162 - acc: 0.2993 - val_loss: 977.8446 - val_acc: 0.2934\n",
      "Epoch 12/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2942.9295 - acc: 0.2971 - val_loss: 977.8170 - val_acc: 0.2934\n",
      "Epoch 13/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2957.3140 - acc: 0.2981 - val_loss: 977.7856 - val_acc: 0.2934\n",
      "Epoch 14/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2972.7767 - acc: 0.2988 - val_loss: 977.7501 - val_acc: 0.2934\n",
      "Epoch 15/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2955.8554 - acc: 0.3009 - val_loss: 977.7109 - val_acc: 0.2934\n",
      "Epoch 16/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2941.4139 - acc: 0.3024 - val_loss: 977.6648 - val_acc: 0.2934\n",
      "Epoch 17/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 18506.1659 - acc: 0.3006 - val_loss: 977.6078 - val_acc: 0.2934\n",
      "Epoch 18/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2966.0987 - acc: 0.3022 - val_loss: 977.5517 - val_acc: 0.2934\n",
      "Epoch 19/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 51294.6077 - acc: 0.2997 - val_loss: 977.4974 - val_acc: 0.2934\n",
      "Epoch 20/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2942.7484 - acc: 0.3018 - val_loss: 977.4256 - val_acc: 0.2934\n",
      "Epoch 21/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 3135.1557 - acc: 0.3016 - val_loss: 977.3438 - val_acc: 0.2934\n",
      "Epoch 22/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2948.1382 - acc: 0.3018 - val_loss: 977.2524 - val_acc: 0.2934\n",
      "Epoch 23/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2940.4544 - acc: 0.3020 - val_loss: 977.1518 - val_acc: 0.2934\n",
      "Epoch 24/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 3170.1936 - acc: 0.3017 - val_loss: 977.0367 - val_acc: 0.2934\n",
      "Epoch 25/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2939.0301 - acc: 0.3020 - val_loss: 976.9089 - val_acc: 0.2934\n",
      "Epoch 26/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 379365717.0870 - acc: 0.2941 - val_loss: 977.6037 - val_acc: 0.2146\n",
      "Epoch 27/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2941.2914 - acc: 0.2100 - val_loss: 977.6706 - val_acc: 0.2146\n",
      "Epoch 28/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2948.6765 - acc: 0.2095 - val_loss: 977.5868 - val_acc: 0.2146\n",
      "Epoch 29/300\n",
      "7471/7471 [==============================] - 0s 23us/step - loss: 54823.9583 - acc: 0.2108 - val_loss: 977.5075 - val_acc: 0.2146\n",
      "Epoch 30/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2938.8036 - acc: 0.2096 - val_loss: 977.4023 - val_acc: 0.2146\n",
      "Epoch 31/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2943.8743 - acc: 0.2100 - val_loss: 977.2821 - val_acc: 0.2146\n",
      "Epoch 32/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2938.5620 - acc: 0.2111 - val_loss: 977.1484 - val_acc: 0.2146\n",
      "Epoch 33/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2937.9083 - acc: 0.2109 - val_loss: 976.9987 - val_acc: 0.2146\n",
      "Epoch 34/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2938.0791 - acc: 0.2103 - val_loss: 976.8302 - val_acc: 0.2146\n",
      "Epoch 35/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2938.4094 - acc: 0.2108 - val_loss: 976.6436 - val_acc: 0.2146\n",
      "Epoch 36/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2937.1770 - acc: 0.2099 - val_loss: 976.4342 - val_acc: 0.2146\n",
      "Epoch 37/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2936.9462 - acc: 0.2100 - val_loss: 976.2017 - val_acc: 0.2146\n",
      "Epoch 38/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2936.7423 - acc: 0.2097 - val_loss: 975.9416 - val_acc: 0.2146\n",
      "Epoch 39/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 5838.2643 - acc: 0.2104 - val_loss: 975.6532 - val_acc: 0.2146\n",
      "Epoch 40/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2935.9903 - acc: 0.2120 - val_loss: 975.3367 - val_acc: 0.2146\n",
      "Epoch 41/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2935.7789 - acc: 0.2115 - val_loss: 974.9774 - val_acc: 0.2146\n",
      "Epoch 42/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 4028.9523 - acc: 0.2373 - val_loss: 974.5862 - val_acc: 0.2934\n",
      "Epoch 43/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2944.2145 - acc: 0.3025 - val_loss: 974.1475 - val_acc: 0.2934\n",
      "Epoch 44/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2938.7588 - acc: 0.3034 - val_loss: 973.6557 - val_acc: 0.2934\n",
      "Epoch 45/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2933.7872 - acc: 0.3033 - val_loss: 973.1197 - val_acc: 0.2934\n",
      "Epoch 46/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2977.0084 - acc: 0.3029 - val_loss: 972.5237 - val_acc: 0.2934\n",
      "Epoch 47/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2932.5119 - acc: 0.3033 - val_loss: 971.8697 - val_acc: 0.2934\n",
      "Epoch 48/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2931.8584 - acc: 0.3033 - val_loss: 971.1679 - val_acc: 0.2934\n",
      "Epoch 49/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2931.1803 - acc: 0.3036 - val_loss: 970.3862 - val_acc: 0.2934\n",
      "Epoch 50/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2930.1429 - acc: 0.3038 - val_loss: 969.5469 - val_acc: 0.2934\n",
      "Epoch 51/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2929.1770 - acc: 0.3038 - val_loss: 968.6483 - val_acc: 0.2934\n",
      "Epoch 52/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2928.1830 - acc: 0.3041 - val_loss: 967.6566 - val_acc: 0.2934\n",
      "Epoch 53/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2927.0386 - acc: 0.3040 - val_loss: 966.6221 - val_acc: 0.2934\n",
      "Epoch 54/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2925.8804 - acc: 0.3037 - val_loss: 965.5497 - val_acc: 0.2934\n",
      "Epoch 55/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2928.8985 - acc: 0.3034 - val_loss: 964.3738 - val_acc: 0.2934\n",
      "Epoch 56/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2923.5025 - acc: 0.3034 - val_loss: 963.1410 - val_acc: 0.2934\n",
      "Epoch 57/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 3143.2293 - acc: 0.3040 - val_loss: 961.8586 - val_acc: 0.2934\n",
      "Epoch 58/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2920.5890 - acc: 0.3040 - val_loss: 960.5723 - val_acc: 0.2934\n",
      "Epoch 59/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2919.1290 - acc: 0.3038 - val_loss: 959.2716 - val_acc: 0.2934\n",
      "Epoch 60/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2918.0476 - acc: 0.3037 - val_loss: 958.0101 - val_acc: 0.2934\n",
      "Epoch 61/300\n",
      "7471/7471 [==============================] - 0s 23us/step - loss: 2916.2794 - acc: 0.3040 - val_loss: 956.7097 - val_acc: 0.2934\n",
      "Epoch 62/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2914.8333 - acc: 0.3038 - val_loss: 955.4361 - val_acc: 0.2934\n",
      "Epoch 63/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2917.9545 - acc: 0.3040 - val_loss: 954.2406 - val_acc: 0.2934\n",
      "Epoch 64/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2912.1299 - acc: 0.3040 - val_loss: 953.1355 - val_acc: 0.2934\n",
      "Epoch 65/300\n",
      "7471/7471 [==============================] - 0s 37us/step - loss: 2911.4495 - acc: 0.3040 - val_loss: 952.1112 - val_acc: 0.2934\n",
      "Epoch 66/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2909.7980 - acc: 0.3040 - val_loss: 951.1234 - val_acc: 0.2934\n",
      "Epoch 67/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2908.6755 - acc: 0.3038 - val_loss: 950.2729 - val_acc: 0.2934\n",
      "Epoch 68/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2907.7482 - acc: 0.3038 - val_loss: 949.5116 - val_acc: 0.2934\n",
      "Epoch 69/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2906.9279 - acc: 0.3040 - val_loss: 948.8523 - val_acc: 0.2934\n",
      "Epoch 70/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2906.3589 - acc: 0.3036 - val_loss: 948.2818 - val_acc: 0.2934\n",
      "Epoch 71/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2905.6406 - acc: 0.3037 - val_loss: 947.8182 - val_acc: 0.2934\n",
      "Epoch 72/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2905.6544 - acc: 0.3038 - val_loss: 947.3897 - val_acc: 0.2934\n",
      "Epoch 73/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2904.4023 - acc: 0.3038 - val_loss: 947.0749 - val_acc: 0.2934\n",
      "Epoch 74/300\n",
      "7471/7471 [==============================] - 0s 40us/step - loss: 2903.9490 - acc: 0.3038 - val_loss: 946.7926 - val_acc: 0.2934\n",
      "Epoch 75/300\n",
      "7471/7471 [==============================] - 0s 41us/step - loss: 3158.1703 - acc: 0.3041 - val_loss: 946.5956 - val_acc: 0.2934\n",
      "Epoch 76/300\n",
      "7471/7471 [==============================] - 0s 38us/step - loss: 2905.3593 - acc: 0.3036 - val_loss: 946.4191 - val_acc: 0.2934\n",
      "Epoch 77/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2904.5513 - acc: 0.3040 - val_loss: 946.2809 - val_acc: 0.2934\n",
      "Epoch 78/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2903.1233 - acc: 0.3040 - val_loss: 946.1910 - val_acc: 0.2934\n",
      "Epoch 79/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2911.2293 - acc: 0.3040 - val_loss: 946.0792 - val_acc: 0.2934\n",
      "Epoch 80/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2903.6053 - acc: 0.3036 - val_loss: 946.0273 - val_acc: 0.2934\n",
      "Epoch 81/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2902.7406 - acc: 0.3041 - val_loss: 945.9850 - val_acc: 0.2934\n",
      "Epoch 82/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2910.5101 - acc: 0.3042 - val_loss: 945.9591 - val_acc: 0.2934\n",
      "Epoch 83/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2908.1818 - acc: 0.3038 - val_loss: 945.9348 - val_acc: 0.2934\n",
      "Epoch 84/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.8854 - acc: 0.3037 - val_loss: 945.9310 - val_acc: 0.2934\n",
      "Epoch 85/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2910.4046 - acc: 0.3041 - val_loss: 945.9168 - val_acc: 0.2934\n",
      "Epoch 86/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.2224 - acc: 0.3040 - val_loss: 945.9144 - val_acc: 0.2934\n",
      "Epoch 87/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2904.0547 - acc: 0.3040 - val_loss: 945.9169 - val_acc: 0.2934\n",
      "Epoch 88/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2902.0252 - acc: 0.3040 - val_loss: 945.9269 - val_acc: 0.2934\n",
      "Epoch 89/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2911.0787 - acc: 0.3041 - val_loss: 945.9394 - val_acc: 0.2934\n",
      "Epoch 90/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2902.3833 - acc: 0.3038 - val_loss: 945.9423 - val_acc: 0.2934\n",
      "Epoch 91/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2902.8275 - acc: 0.3038 - val_loss: 945.9560 - val_acc: 0.2934\n",
      "Epoch 92/300\n",
      "7471/7471 [==============================] - 0s 23us/step - loss: 2902.0937 - acc: 0.3040 - val_loss: 945.9612 - val_acc: 0.2934\n",
      "Epoch 93/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 3744.7197 - acc: 0.3038 - val_loss: 946.0316 - val_acc: 0.2934\n",
      "Epoch 94/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2902.0395 - acc: 0.3040 - val_loss: 945.9955 - val_acc: 0.2934\n",
      "Epoch 95/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2903.2054 - acc: 0.3040 - val_loss: 945.9845 - val_acc: 0.2934\n",
      "Epoch 96/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9426 - acc: 0.3040 - val_loss: 945.9977 - val_acc: 0.2934\n",
      "Epoch 97/300\n",
      "7471/7471 [==============================] - 0s 23us/step - loss: 9939.2786 - acc: 0.3041 - val_loss: 946.0606 - val_acc: 0.2934\n",
      "Epoch 98/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9524 - acc: 0.3038 - val_loss: 946.0556 - val_acc: 0.2934\n",
      "Epoch 99/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2901.7607 - acc: 0.3040 - val_loss: 946.0384 - val_acc: 0.2934\n",
      "Epoch 100/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2901.9653 - acc: 0.3040 - val_loss: 946.0490 - val_acc: 0.2934\n",
      "Epoch 101/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2901.9928 - acc: 0.3040 - val_loss: 946.0510 - val_acc: 0.2934\n",
      "Epoch 102/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8864 - acc: 0.3040 - val_loss: 946.0473 - val_acc: 0.2934\n",
      "Epoch 103/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 2901.8858 - acc: 0.3040 - val_loss: 946.0589 - val_acc: 0.2934\n",
      "Epoch 104/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9028 - acc: 0.3040 - val_loss: 946.0483 - val_acc: 0.2934\n",
      "Epoch 105/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2901.8794 - acc: 0.3040 - val_loss: 946.0609 - val_acc: 0.2934\n",
      "Epoch 106/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8976 - acc: 0.3040 - val_loss: 946.0530 - val_acc: 0.2934\n",
      "Epoch 107/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9287 - acc: 0.3038 - val_loss: 946.0755 - val_acc: 0.2934\n",
      "Epoch 108/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8928 - acc: 0.3038 - val_loss: 946.0782 - val_acc: 0.2934\n",
      "Epoch 109/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9077 - acc: 0.3040 - val_loss: 946.0703 - val_acc: 0.2934\n",
      "Epoch 110/300\n",
      "7471/7471 [==============================] - 0s 45us/step - loss: 2901.9057 - acc: 0.3040 - val_loss: 946.0443 - val_acc: 0.2934\n",
      "Epoch 111/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.9233 - acc: 0.3040 - val_loss: 946.0736 - val_acc: 0.2934\n",
      "Epoch 112/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2902.1327 - acc: 0.3038 - val_loss: 946.0717 - val_acc: 0.2934\n",
      "Epoch 113/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8846 - acc: 0.3040 - val_loss: 946.0882 - val_acc: 0.2934\n",
      "Epoch 114/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.0787 - val_acc: 0.2934\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8913 - acc: 0.3040 - val_loss: 946.0820 - val_acc: 0.2934\n",
      "Epoch 116/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8847 - acc: 0.3040 - val_loss: 946.0736 - val_acc: 0.2934\n",
      "Epoch 117/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8898 - acc: 0.3040 - val_loss: 946.0764 - val_acc: 0.2934\n",
      "Epoch 118/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9048 - acc: 0.3040 - val_loss: 946.0838 - val_acc: 0.2934\n",
      "Epoch 119/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8898 - acc: 0.3040 - val_loss: 946.0796 - val_acc: 0.2934\n",
      "Epoch 120/300\n",
      "7471/7471 [==============================] - 0s 24us/step - loss: 3210.1942 - acc: 0.3040 - val_loss: 946.0847 - val_acc: 0.2934\n",
      "Epoch 121/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9189 - acc: 0.3040 - val_loss: 946.0845 - val_acc: 0.2934\n",
      "Epoch 122/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8839 - acc: 0.3040 - val_loss: 946.0830 - val_acc: 0.2934\n",
      "Epoch 123/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2902.2183 - acc: 0.3040 - val_loss: 946.0849 - val_acc: 0.2934\n",
      "Epoch 124/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8873 - acc: 0.3040 - val_loss: 946.0714 - val_acc: 0.2934\n",
      "Epoch 125/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2901.8891 - acc: 0.3040 - val_loss: 946.0978 - val_acc: 0.2934\n",
      "Epoch 126/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8806 - acc: 0.3040 - val_loss: 946.0905 - val_acc: 0.2934\n",
      "Epoch 127/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8881 - acc: 0.3040 - val_loss: 946.0963 - val_acc: 0.2934\n",
      "Epoch 128/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8856 - acc: 0.3038 - val_loss: 946.1036 - val_acc: 0.2934\n",
      "Epoch 129/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2995.8519 - acc: 0.3040 - val_loss: 946.1294 - val_acc: 0.2934\n",
      "Epoch 130/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2902.3373 - acc: 0.3038 - val_loss: 946.1042 - val_acc: 0.2934\n",
      "Epoch 131/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.9006 - acc: 0.3040 - val_loss: 946.0996 - val_acc: 0.2934\n",
      "Epoch 132/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.9067 - acc: 0.3040 - val_loss: 946.1005 - val_acc: 0.2934\n",
      "Epoch 133/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2901.9222 - acc: 0.3040 - val_loss: 946.0992 - val_acc: 0.2934\n",
      "Epoch 134/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2901.8800 - acc: 0.3040 - val_loss: 946.1068 - val_acc: 0.2934\n",
      "Epoch 135/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2901.9060 - acc: 0.3040 - val_loss: 946.0975 - val_acc: 0.2934\n",
      "Epoch 136/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2902.2289 - acc: 0.3037 - val_loss: 946.1058 - val_acc: 0.2934\n",
      "Epoch 137/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2921.4929 - acc: 0.3038 - val_loss: 946.1030 - val_acc: 0.2934\n",
      "Epoch 138/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2904.2967 - acc: 0.3040 - val_loss: 946.0996 - val_acc: 0.2934\n",
      "Epoch 139/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.9185 - acc: 0.3040 - val_loss: 946.0956 - val_acc: 0.2934\n",
      "Epoch 140/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2901.8864 - acc: 0.3040 - val_loss: 946.1114 - val_acc: 0.2934\n",
      "Epoch 141/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2919.7439 - acc: 0.3041 - val_loss: 946.1007 - val_acc: 0.2934\n",
      "Epoch 142/300\n",
      "7471/7471 [==============================] - 0s 45us/step - loss: 2901.8849 - acc: 0.3040 - val_loss: 946.1024 - val_acc: 0.2934\n",
      "Epoch 143/300\n",
      "7471/7471 [==============================] - 0s 39us/step - loss: 2901.8926 - acc: 0.3040 - val_loss: 946.0966 - val_acc: 0.2934\n",
      "Epoch 144/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.8849 - acc: 0.3040 - val_loss: 946.0934 - val_acc: 0.2934\n",
      "Epoch 145/300\n",
      "7471/7471 [==============================] - 0s 37us/step - loss: 2901.9918 - acc: 0.3038 - val_loss: 946.0912 - val_acc: 0.2934\n",
      "Epoch 146/300\n",
      "7471/7471 [==============================] - 0s 38us/step - loss: 2901.8812 - acc: 0.3040 - val_loss: 946.0870 - val_acc: 0.2934\n",
      "Epoch 147/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2901.8904 - acc: 0.3040 - val_loss: 946.1044 - val_acc: 0.2934\n",
      "Epoch 148/300\n",
      "7471/7471 [==============================] - 0s 38us/step - loss: 2901.8938 - acc: 0.3040 - val_loss: 946.1024 - val_acc: 0.2934\n",
      "Epoch 149/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2905.7257 - acc: 0.3041 - val_loss: 946.0989 - val_acc: 0.2934\n",
      "Epoch 150/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.9744 - acc: 0.3040 - val_loss: 946.1038 - val_acc: 0.2934\n",
      "Epoch 151/300\n",
      "7471/7471 [==============================] - 0s 35us/step - loss: 2901.8871 - acc: 0.3040 - val_loss: 946.0926 - val_acc: 0.2934\n",
      "Epoch 152/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.8869 - acc: 0.3040 - val_loss: 946.0957 - val_acc: 0.2934\n",
      "Epoch 153/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2901.8888 - acc: 0.3040 - val_loss: 946.0897 - val_acc: 0.2934\n",
      "Epoch 154/300\n",
      "7471/7471 [==============================] - 0s 33us/step - loss: 2901.8836 - acc: 0.3040 - val_loss: 946.0926 - val_acc: 0.2934\n",
      "Epoch 155/300\n",
      "7471/7471 [==============================] - 0s 34us/step - loss: 2901.8815 - acc: 0.3038 - val_loss: 946.0860 - val_acc: 0.2934\n",
      "Epoch 156/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.9962 - acc: 0.3040 - val_loss: 946.0944 - val_acc: 0.2934\n",
      "Epoch 157/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.9823 - acc: 0.3040 - val_loss: 946.0964 - val_acc: 0.2934\n",
      "Epoch 158/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2901.9108 - acc: 0.3041 - val_loss: 946.0974 - val_acc: 0.2934\n",
      "Epoch 159/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2901.9429 - acc: 0.3040 - val_loss: 946.0950 - val_acc: 0.2934\n",
      "Epoch 160/300\n",
      "7471/7471 [==============================] - 0s 32us/step - loss: 2901.9251 - acc: 0.3040 - val_loss: 946.0970 - val_acc: 0.2934\n",
      "Epoch 161/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2904.4175 - acc: 0.3038 - val_loss: 946.0956 - val_acc: 0.2934\n",
      "Epoch 162/300\n",
      "7471/7471 [==============================] - 0s 39us/step - loss: 2901.8835 - acc: 0.3040 - val_loss: 946.1009 - val_acc: 0.2934\n",
      "Epoch 163/300\n",
      "7471/7471 [==============================] - 0s 36us/step - loss: 2901.8802 - acc: 0.3040 - val_loss: 946.0953 - val_acc: 0.2934\n",
      "Epoch 164/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.9049 - acc: 0.3038 - val_loss: 946.0948 - val_acc: 0.2934\n",
      "Epoch 165/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8877 - acc: 0.3038 - val_loss: 946.0890 - val_acc: 0.2934\n",
      "Epoch 166/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2902.9232 - acc: 0.3038 - val_loss: 946.0896 - val_acc: 0.2934\n",
      "Epoch 167/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2902.3468 - acc: 0.3040 - val_loss: 946.0812 - val_acc: 0.2934\n",
      "Epoch 168/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2902.5197 - acc: 0.3038 - val_loss: 946.0862 - val_acc: 0.2934\n",
      "Epoch 169/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8844 - acc: 0.3040 - val_loss: 946.0933 - val_acc: 0.2934\n",
      "Epoch 170/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8949 - acc: 0.3040 - val_loss: 946.0846 - val_acc: 0.2934\n",
      "Epoch 171/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8888 - acc: 0.3040 - val_loss: 946.0894 - val_acc: 0.2934\n",
      "Epoch 172/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8893 - acc: 0.3040 - val_loss: 946.0760 - val_acc: 0.2934\n",
      "Epoch 173/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9058 - acc: 0.3040 - val_loss: 946.0783 - val_acc: 0.2934\n",
      "Epoch 174/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8919 - acc: 0.3040 - val_loss: 946.0866 - val_acc: 0.2934\n",
      "Epoch 175/300\n",
      "7471/7471 [==============================] - 0s 31us/step - loss: 2901.9027 - acc: 0.3040 - val_loss: 946.0816 - val_acc: 0.2934\n",
      "Epoch 176/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9001 - acc: 0.3040 - val_loss: 946.0939 - val_acc: 0.2934\n",
      "Epoch 177/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8920 - acc: 0.3040 - val_loss: 946.0837 - val_acc: 0.2934\n",
      "Epoch 178/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8879 - acc: 0.3040 - val_loss: 946.0898 - val_acc: 0.2934\n",
      "Epoch 179/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8811 - acc: 0.3040 - val_loss: 946.1050 - val_acc: 0.2934\n",
      "Epoch 180/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8890 - acc: 0.3040 - val_loss: 946.0967 - val_acc: 0.2934\n",
      "Epoch 181/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8911 - acc: 0.3040 - val_loss: 946.0874 - val_acc: 0.2934\n",
      "Epoch 182/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8882 - acc: 0.3040 - val_loss: 946.0796 - val_acc: 0.2934\n",
      "Epoch 183/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8827 - acc: 0.3038 - val_loss: 946.0913 - val_acc: 0.2934\n",
      "Epoch 184/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8782 - acc: 0.3037 - val_loss: 946.0984 - val_acc: 0.2934\n",
      "Epoch 185/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.0447 - acc: 0.3040 - val_loss: 946.0935 - val_acc: 0.2934\n",
      "Epoch 186/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8960 - acc: 0.3040 - val_loss: 946.0756 - val_acc: 0.2934\n",
      "Epoch 187/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2974.8690 - acc: 0.3041 - val_loss: 946.1122 - val_acc: 0.2934\n",
      "Epoch 188/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8912 - acc: 0.3040 - val_loss: 946.0989 - val_acc: 0.2934\n",
      "Epoch 189/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8825 - acc: 0.3040 - val_loss: 946.1000 - val_acc: 0.2934\n",
      "Epoch 190/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2902.4970 - acc: 0.3040 - val_loss: 946.0968 - val_acc: 0.2934\n",
      "Epoch 191/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8873 - acc: 0.3040 - val_loss: 946.0909 - val_acc: 0.2934\n",
      "Epoch 192/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8856 - acc: 0.3040 - val_loss: 946.0978 - val_acc: 0.2934\n",
      "Epoch 193/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8950 - acc: 0.3040 - val_loss: 946.0904 - val_acc: 0.2934\n",
      "Epoch 194/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8890 - acc: 0.3040 - val_loss: 946.1146 - val_acc: 0.2934\n",
      "Epoch 195/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8829 - acc: 0.3040 - val_loss: 946.1052 - val_acc: 0.2934\n",
      "Epoch 196/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8832 - acc: 0.3040 - val_loss: 946.1049 - val_acc: 0.2934\n",
      "Epoch 197/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8837 - acc: 0.3040 - val_loss: 946.0879 - val_acc: 0.2934\n",
      "Epoch 198/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8847 - acc: 0.3040 - val_loss: 946.0982 - val_acc: 0.2934\n",
      "Epoch 199/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8838 - acc: 0.3040 - val_loss: 946.1063 - val_acc: 0.2934\n",
      "Epoch 200/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8876 - acc: 0.3040 - val_loss: 946.0903 - val_acc: 0.2934\n",
      "Epoch 201/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8832 - acc: 0.3040 - val_loss: 946.0920 - val_acc: 0.2934\n",
      "Epoch 202/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8791 - acc: 0.3040 - val_loss: 946.0868 - val_acc: 0.2934\n",
      "Epoch 203/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8939 - acc: 0.3040 - val_loss: 946.1081 - val_acc: 0.2934\n",
      "Epoch 204/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8871 - acc: 0.3040 - val_loss: 946.0957 - val_acc: 0.2934\n",
      "Epoch 205/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8738 - acc: 0.3040 - val_loss: 946.0887 - val_acc: 0.2934\n",
      "Epoch 206/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8977 - acc: 0.3040 - val_loss: 946.0843 - val_acc: 0.2934\n",
      "Epoch 207/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8852 - acc: 0.3040 - val_loss: 946.0832 - val_acc: 0.2934\n",
      "Epoch 208/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.9051 - acc: 0.3040 - val_loss: 946.0867 - val_acc: 0.2934\n",
      "Epoch 209/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.9160 - acc: 0.3040 - val_loss: 946.0933 - val_acc: 0.2934\n",
      "Epoch 210/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.9123 - acc: 0.3038 - val_loss: 946.0899 - val_acc: 0.2934\n",
      "Epoch 211/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8837 - acc: 0.3040 - val_loss: 946.0762 - val_acc: 0.2934\n",
      "Epoch 212/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8803 - acc: 0.3040 - val_loss: 946.0885 - val_acc: 0.2934\n",
      "Epoch 213/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8968 - acc: 0.3040 - val_loss: 946.0893 - val_acc: 0.2934\n",
      "Epoch 214/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8879 - acc: 0.3040 - val_loss: 946.0597 - val_acc: 0.2934\n",
      "Epoch 215/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8875 - acc: 0.3040 - val_loss: 946.0884 - val_acc: 0.2934\n",
      "Epoch 216/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8814 - acc: 0.3040 - val_loss: 946.1015 - val_acc: 0.2934\n",
      "Epoch 217/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8888 - acc: 0.3040 - val_loss: 946.1109 - val_acc: 0.2934\n",
      "Epoch 218/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8883 - acc: 0.3040 - val_loss: 946.0988 - val_acc: 0.2934\n",
      "Epoch 219/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8920 - acc: 0.3040 - val_loss: 946.0898 - val_acc: 0.2934\n",
      "Epoch 220/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8804 - acc: 0.3040 - val_loss: 946.0933 - val_acc: 0.2934\n",
      "Epoch 221/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8860 - acc: 0.3040 - val_loss: 946.0884 - val_acc: 0.2934\n",
      "Epoch 222/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8856 - acc: 0.3040 - val_loss: 946.0806 - val_acc: 0.2934\n",
      "Epoch 223/300\n",
      "7471/7471 [==============================] - 0s 30us/step - loss: 2901.8881 - acc: 0.3040 - val_loss: 946.0837 - val_acc: 0.2934\n",
      "Epoch 224/300\n",
      "7471/7471 [==============================] - 0s 29us/step - loss: 2901.8848 - acc: 0.3040 - val_loss: 946.0852 - val_acc: 0.2934\n",
      "Epoch 225/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8823 - acc: 0.3040 - val_loss: 946.0851 - val_acc: 0.2934\n",
      "Epoch 226/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8838 - acc: 0.3040 - val_loss: 946.0848 - val_acc: 0.2934\n",
      "Epoch 227/300\n",
      "7471/7471 [==============================] - 0s 28us/step - loss: 2901.8867 - acc: 0.3040 - val_loss: 946.0843 - val_acc: 0.2934\n",
      "Epoch 228/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8813 - acc: 0.3040 - val_loss: 946.0918 - val_acc: 0.2934\n",
      "Epoch 229/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8875 - acc: 0.3040 - val_loss: 946.0922 - val_acc: 0.2934\n",
      "Epoch 230/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2902.5569 - acc: 0.3038 - val_loss: 946.0911 - val_acc: 0.2934\n",
      "Epoch 231/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9024 - acc: 0.3040 - val_loss: 946.0900 - val_acc: 0.2934\n",
      "Epoch 232/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8927 - acc: 0.3040 - val_loss: 946.0930 - val_acc: 0.2934\n",
      "Epoch 233/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8849 - acc: 0.3040 - val_loss: 946.0692 - val_acc: 0.2934\n",
      "Epoch 234/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8823 - acc: 0.3040 - val_loss: 946.0874 - val_acc: 0.2934\n",
      "Epoch 235/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.9067 - acc: 0.3040 - val_loss: 946.0884 - val_acc: 0.2934\n",
      "Epoch 236/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8873 - acc: 0.3040 - val_loss: 946.0870 - val_acc: 0.2934\n",
      "Epoch 237/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8811 - acc: 0.3040 - val_loss: 946.0970 - val_acc: 0.2934\n",
      "Epoch 238/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8938 - acc: 0.3040 - val_loss: 946.0939 - val_acc: 0.2934\n",
      "Epoch 239/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8896 - acc: 0.3040 - val_loss: 946.0990 - val_acc: 0.2934\n",
      "Epoch 240/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8805 - acc: 0.3040 - val_loss: 946.0951 - val_acc: 0.2934\n",
      "Epoch 241/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.9388 - acc: 0.3040 - val_loss: 946.0960 - val_acc: 0.2934\n",
      "Epoch 242/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8863 - acc: 0.3040 - val_loss: 946.0930 - val_acc: 0.2934\n",
      "Epoch 243/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9031 - acc: 0.3040 - val_loss: 946.0953 - val_acc: 0.2934\n",
      "Epoch 244/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8895 - acc: 0.3040 - val_loss: 946.0933 - val_acc: 0.2934\n",
      "Epoch 245/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8870 - acc: 0.3040 - val_loss: 946.0937 - val_acc: 0.2934\n",
      "Epoch 246/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8901 - acc: 0.3040 - val_loss: 946.0956 - val_acc: 0.2934\n",
      "Epoch 247/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8807 - acc: 0.3040 - val_loss: 946.1010 - val_acc: 0.2934\n",
      "Epoch 248/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8895 - acc: 0.3040 - val_loss: 946.0969 - val_acc: 0.2934\n",
      "Epoch 249/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8873 - acc: 0.3040 - val_loss: 946.0882 - val_acc: 0.2934\n",
      "Epoch 250/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8812 - acc: 0.3040 - val_loss: 946.0950 - val_acc: 0.2934\n",
      "Epoch 251/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8865 - acc: 0.3040 - val_loss: 946.0876 - val_acc: 0.2934\n",
      "Epoch 252/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8803 - acc: 0.3040 - val_loss: 946.0896 - val_acc: 0.2934\n",
      "Epoch 253/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8847 - acc: 0.3040 - val_loss: 946.0878 - val_acc: 0.2934\n",
      "Epoch 254/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8893 - acc: 0.3040 - val_loss: 946.0847 - val_acc: 0.2934\n",
      "Epoch 255/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8831 - acc: 0.3040 - val_loss: 946.0907 - val_acc: 0.2934\n",
      "Epoch 256/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8773 - acc: 0.3040 - val_loss: 946.0889 - val_acc: 0.2934\n",
      "Epoch 257/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8884 - acc: 0.3040 - val_loss: 946.0835 - val_acc: 0.2934\n",
      "Epoch 258/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8926 - acc: 0.3040 - val_loss: 946.0861 - val_acc: 0.2934\n",
      "Epoch 259/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8877 - acc: 0.3040 - val_loss: 946.0962 - val_acc: 0.2934\n",
      "Epoch 260/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8985 - acc: 0.3040 - val_loss: 946.0944 - val_acc: 0.2934\n",
      "Epoch 261/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8979 - acc: 0.3040 - val_loss: 946.0804 - val_acc: 0.2934\n",
      "Epoch 262/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8814 - acc: 0.3040 - val_loss: 946.1058 - val_acc: 0.2934\n",
      "Epoch 263/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8814 - acc: 0.3040 - val_loss: 946.1055 - val_acc: 0.2934\n",
      "Epoch 264/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8803 - acc: 0.3040 - val_loss: 946.1038 - val_acc: 0.2934\n",
      "Epoch 265/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8852 - acc: 0.3040 - val_loss: 946.0902 - val_acc: 0.2934\n",
      "Epoch 266/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8905 - acc: 0.3040 - val_loss: 946.0978 - val_acc: 0.2934\n",
      "Epoch 267/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8789 - acc: 0.3040 - val_loss: 946.0852 - val_acc: 0.2934\n",
      "Epoch 268/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8854 - acc: 0.3040 - val_loss: 946.0856 - val_acc: 0.2934\n",
      "Epoch 269/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8863 - acc: 0.3040 - val_loss: 946.0955 - val_acc: 0.2934\n",
      "Epoch 270/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8804 - acc: 0.3040 - val_loss: 946.0925 - val_acc: 0.2934\n",
      "Epoch 271/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8897 - acc: 0.3040 - val_loss: 946.0901 - val_acc: 0.2934\n",
      "Epoch 272/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8784 - acc: 0.3040 - val_loss: 946.0959 - val_acc: 0.2934\n",
      "Epoch 273/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8777 - acc: 0.3040 - val_loss: 946.0939 - val_acc: 0.2934\n",
      "Epoch 274/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8886 - acc: 0.3040 - val_loss: 946.0873 - val_acc: 0.2934\n",
      "Epoch 275/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8868 - acc: 0.3040 - val_loss: 946.0877 - val_acc: 0.2934\n",
      "Epoch 276/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8927 - acc: 0.3040 - val_loss: 946.0915 - val_acc: 0.2934\n",
      "Epoch 277/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8818 - acc: 0.3040 - val_loss: 946.0815 - val_acc: 0.2934\n",
      "Epoch 278/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8906 - acc: 0.3040 - val_loss: 946.0906 - val_acc: 0.2934\n",
      "Epoch 279/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8860 - acc: 0.3040 - val_loss: 946.0929 - val_acc: 0.2934\n",
      "Epoch 280/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.9010 - acc: 0.3040 - val_loss: 946.0697 - val_acc: 0.2934\n",
      "Epoch 281/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8966 - acc: 0.3040 - val_loss: 946.1022 - val_acc: 0.2934\n",
      "Epoch 282/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8810 - acc: 0.3040 - val_loss: 946.0999 - val_acc: 0.2934\n",
      "Epoch 283/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8985 - acc: 0.3040 - val_loss: 946.0957 - val_acc: 0.2934\n",
      "Epoch 284/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8900 - acc: 0.3040 - val_loss: 946.0944 - val_acc: 0.2934\n",
      "Epoch 285/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8933 - acc: 0.3040 - val_loss: 946.0948 - val_acc: 0.2934\n",
      "Epoch 286/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8830 - acc: 0.3040 - val_loss: 946.0989 - val_acc: 0.2934\n",
      "Epoch 287/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8884 - acc: 0.3040 - val_loss: 946.0957 - val_acc: 0.2934\n",
      "Epoch 288/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8915 - acc: 0.3040 - val_loss: 946.0973 - val_acc: 0.2934\n",
      "Epoch 289/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8989 - acc: 0.3040 - val_loss: 946.0969 - val_acc: 0.2934\n",
      "Epoch 290/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8880 - acc: 0.3040 - val_loss: 946.0850 - val_acc: 0.2934\n",
      "Epoch 291/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8808 - acc: 0.3040 - val_loss: 946.0881 - val_acc: 0.2934\n",
      "Epoch 292/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8805 - acc: 0.3040 - val_loss: 946.0920 - val_acc: 0.2934\n",
      "Epoch 293/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8844 - acc: 0.3040 - val_loss: 946.0980 - val_acc: 0.2934\n",
      "Epoch 294/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8822 - acc: 0.3040 - val_loss: 946.0864 - val_acc: 0.2934\n",
      "Epoch 295/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8849 - acc: 0.3040 - val_loss: 946.0989 - val_acc: 0.2934\n",
      "Epoch 296/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8890 - acc: 0.3040 - val_loss: 946.1046 - val_acc: 0.2934\n",
      "Epoch 297/300\n",
      "7471/7471 [==============================] - 0s 27us/step - loss: 2901.8832 - acc: 0.3040 - val_loss: 946.0968 - val_acc: 0.2934\n",
      "Epoch 298/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8898 - acc: 0.3040 - val_loss: 946.0969 - val_acc: 0.2934\n",
      "Epoch 299/300\n",
      "7471/7471 [==============================] - 0s 25us/step - loss: 2901.8864 - acc: 0.3040 - val_loss: 946.0919 - val_acc: 0.2934\n",
      "Epoch 300/300\n",
      "7471/7471 [==============================] - 0s 26us/step - loss: 2901.8884 - acc: 0.3040 - val_loss: 946.1013 - val_acc: 0.2934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd142b67b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "config = wandb.config\n",
    "\n",
    "config.batch_size = 128\n",
    "config.epochs = 300\n",
    "config.learning_rate = 0.01\n",
    "config.dropout = 0.8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(6))\n",
    "\n",
    "adam = Adam(lr=config.learning_rate)\n",
    "model.compile(loss=mean_squared_error, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(input_x_train, input_y_train, \n",
    "          epochs=config.epochs, validation_data=(input_x_test, input_y_test), \n",
    "          callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 layers neural network with more hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/gilangrilhami/Predicting-Micronutrients-using-Neural-Networks-and-Random-Forest-notebooks/runs/66qazwp2\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 01:57:31.228710 140520423556928 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7471 samples, validate on 1319 samples\n",
      "Epoch 1/300\n",
      "7471/7471 [==============================] - 3s 359us/step - loss: 107757469821.0848 - acc: 0.1949 - val_loss: 995.0447 - val_acc: 0.1850\n",
      "Epoch 2/300\n",
      "7471/7471 [==============================] - 2s 330us/step - loss: 5830.6161 - acc: 0.1721 - val_loss: 979.5536 - val_acc: 0.1448\n",
      "Epoch 3/300\n",
      "7471/7471 [==============================] - 3s 338us/step - loss: 10028.5349 - acc: 0.1628 - val_loss: 991.5660 - val_acc: 0.2138\n",
      "Epoch 4/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 3045.9441 - acc: 0.1545 - val_loss: 977.7269 - val_acc: 0.1964\n",
      "Epoch 5/300\n",
      "7471/7471 [==============================] - 2s 289us/step - loss: 1315823150.1727 - acc: 0.1583 - val_loss: 979.7827 - val_acc: 0.1471\n",
      "Epoch 6/300\n",
      "7471/7471 [==============================] - 3s 358us/step - loss: 3090.5625 - acc: 0.1488 - val_loss: 977.3919 - val_acc: 0.1524\n",
      "Epoch 7/300\n",
      "7471/7471 [==============================] - 3s 364us/step - loss: 3022.4857 - acc: 0.1470 - val_loss: 979.7960 - val_acc: 0.1440\n",
      "Epoch 8/300\n",
      "7471/7471 [==============================] - 2s 282us/step - loss: 3774.6723 - acc: 0.1452 - val_loss: 979.7888 - val_acc: 0.1433\n",
      "Epoch 9/300\n",
      "7471/7471 [==============================] - 2s 283us/step - loss: 3776.8643 - acc: 0.1443 - val_loss: 979.6751 - val_acc: 0.1440\n",
      "Epoch 10/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2974.0051 - acc: 0.1421 - val_loss: 979.3784 - val_acc: 0.1425\n",
      "Epoch 11/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2940.4606 - acc: 0.1427 - val_loss: 979.5235 - val_acc: 0.1440\n",
      "Epoch 12/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2940.7020 - acc: 0.1430 - val_loss: 979.5578 - val_acc: 0.1440\n",
      "Epoch 13/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2979.0860 - acc: 0.1431 - val_loss: 979.6492 - val_acc: 0.1433\n",
      "Epoch 14/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2940.2890 - acc: 0.1428 - val_loss: 979.6099 - val_acc: 0.1433\n",
      "Epoch 15/300\n",
      "7471/7471 [==============================] - 2s 284us/step - loss: 2950.5533 - acc: 0.1413 - val_loss: 979.5652 - val_acc: 0.1433\n",
      "Epoch 16/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2955.2897 - acc: 0.1420 - val_loss: 979.5140 - val_acc: 0.1433\n",
      "Epoch 17/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2941.8259 - acc: 0.1420 - val_loss: 979.4572 - val_acc: 0.1433\n",
      "Epoch 18/300\n",
      "7471/7471 [==============================] - 2s 324us/step - loss: 2940.3040 - acc: 0.1420 - val_loss: 979.3964 - val_acc: 0.1433\n",
      "Epoch 19/300\n",
      "7471/7471 [==============================] - 2s 320us/step - loss: 2939.9923 - acc: 0.1421 - val_loss: 979.3213 - val_acc: 0.1433\n",
      "Epoch 20/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2946.3663 - acc: 0.1417 - val_loss: 979.2388 - val_acc: 0.1433\n",
      "Epoch 21/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 3073.6443 - acc: 0.1442 - val_loss: 979.1490 - val_acc: 0.2934\n",
      "Epoch 22/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2939.6885 - acc: 0.3034 - val_loss: 979.0447 - val_acc: 0.2934\n",
      "Epoch 23/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 3165.2513 - acc: 0.3032 - val_loss: 978.9302 - val_acc: 0.2934\n",
      "Epoch 24/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2943.6325 - acc: 0.3028 - val_loss: 978.8023 - val_acc: 0.2934\n",
      "Epoch 25/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2947.5301 - acc: 0.3028 - val_loss: 978.6586 - val_acc: 0.2934\n",
      "Epoch 26/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 3013.7126 - acc: 0.3028 - val_loss: 978.4996 - val_acc: 0.2934\n",
      "Epoch 27/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2963.9824 - acc: 0.3040 - val_loss: 978.3180 - val_acc: 0.2934\n",
      "Epoch 28/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2940.6900 - acc: 0.3037 - val_loss: 978.1175 - val_acc: 0.2934\n",
      "Epoch 29/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2938.2309 - acc: 0.3030 - val_loss: 977.8961 - val_acc: 0.2934\n",
      "Epoch 30/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2941.2738 - acc: 0.3040 - val_loss: 977.6438 - val_acc: 0.2934\n",
      "Epoch 31/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2938.0915 - acc: 0.3038 - val_loss: 977.3659 - val_acc: 0.2934\n",
      "Epoch 32/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 10569.3820 - acc: 0.3036 - val_loss: 977.0911 - val_acc: 0.2934\n",
      "Epoch 33/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2937.3376 - acc: 0.3036 - val_loss: 976.7224 - val_acc: 0.2934\n",
      "Epoch 34/300\n",
      "7471/7471 [==============================] - 2s 306us/step - loss: 2936.9789 - acc: 0.3037 - val_loss: 976.3409 - val_acc: 0.2934\n",
      "Epoch 35/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2936.5057 - acc: 0.3037 - val_loss: 975.9184 - val_acc: 0.2934\n",
      "Epoch 36/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2936.2024 - acc: 0.3033 - val_loss: 975.4401 - val_acc: 0.2934\n",
      "Epoch 37/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2964.2978 - acc: 0.3038 - val_loss: 974.9266 - val_acc: 0.2934\n",
      "Epoch 38/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2938.9655 - acc: 0.3037 - val_loss: 974.3538 - val_acc: 0.2934\n",
      "Epoch 39/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2949.6986 - acc: 0.3038 - val_loss: 973.7005 - val_acc: 0.2934\n",
      "Epoch 40/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2936.4849 - acc: 0.3042 - val_loss: 973.0730 - val_acc: 0.2934\n",
      "Epoch 41/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2932.8165 - acc: 0.3037 - val_loss: 972.2677 - val_acc: 0.2934\n",
      "Epoch 42/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 3008.8459 - acc: 0.3041 - val_loss: 971.4378 - val_acc: 0.2934\n",
      "Epoch 43/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2937.3786 - acc: 0.3040 - val_loss: 970.5419 - val_acc: 0.2934\n",
      "Epoch 44/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2930.1620 - acc: 0.3040 - val_loss: 969.5940 - val_acc: 0.2934\n",
      "Epoch 45/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2929.1674 - acc: 0.3041 - val_loss: 968.5306 - val_acc: 0.2934\n",
      "Epoch 46/300\n",
      "7471/7471 [==============================] - 2s 291us/step - loss: 2927.8429 - acc: 0.3040 - val_loss: 967.4273 - val_acc: 0.2934\n",
      "Epoch 47/300\n",
      "7471/7471 [==============================] - 2s 312us/step - loss: 2926.5443 - acc: 0.3040 - val_loss: 966.2261 - val_acc: 0.2934\n",
      "Epoch 48/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2954.4895 - acc: 0.3038 - val_loss: 965.0043 - val_acc: 0.2934\n",
      "Epoch 49/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2924.1572 - acc: 0.3041 - val_loss: 963.7229 - val_acc: 0.2934\n",
      "Epoch 50/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2925.6042 - acc: 0.3040 - val_loss: 962.3897 - val_acc: 0.2934\n",
      "Epoch 51/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2921.3862 - acc: 0.3040 - val_loss: 961.0572 - val_acc: 0.2934\n",
      "Epoch 52/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2919.5501 - acc: 0.3037 - val_loss: 959.6956 - val_acc: 0.2934\n",
      "Epoch 53/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2918.0711 - acc: 0.3040 - val_loss: 958.3338 - val_acc: 0.2934\n",
      "Epoch 54/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2916.8468 - acc: 0.3042 - val_loss: 956.9612 - val_acc: 0.2934\n",
      "Epoch 55/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2919.6750 - acc: 0.3037 - val_loss: 955.6300 - val_acc: 0.2934\n",
      "Epoch 56/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2913.6504 - acc: 0.3040 - val_loss: 954.3816 - val_acc: 0.2934\n",
      "Epoch 57/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2916.4891 - acc: 0.3040 - val_loss: 953.2434 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2910.9475 - acc: 0.3040 - val_loss: 952.1542 - val_acc: 0.2934\n",
      "Epoch 59/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2909.7590 - acc: 0.3040 - val_loss: 951.1529 - val_acc: 0.2934\n",
      "Epoch 60/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2908.6968 - acc: 0.3040 - val_loss: 950.3352 - val_acc: 0.2934\n",
      "Epoch 61/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2907.7370 - acc: 0.3040 - val_loss: 949.5688 - val_acc: 0.2934\n",
      "Epoch 62/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2906.8417 - acc: 0.3040 - val_loss: 948.8476 - val_acc: 0.2934\n",
      "Epoch 63/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2917.7410 - acc: 0.3040 - val_loss: 948.3570 - val_acc: 0.2934\n",
      "Epoch 64/300\n",
      "7471/7471 [==============================] - 2s 312us/step - loss: 2905.3800 - acc: 0.3040 - val_loss: 947.7649 - val_acc: 0.2934\n",
      "Epoch 65/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2904.8101 - acc: 0.3040 - val_loss: 947.3514 - val_acc: 0.2934\n",
      "Epoch 66/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2904.9483 - acc: 0.3040 - val_loss: 947.0336 - val_acc: 0.2934\n",
      "Epoch 67/300\n",
      "7471/7471 [==============================] - 2s 312us/step - loss: 2907.4972 - acc: 0.3042 - val_loss: 946.7708 - val_acc: 0.2934\n",
      "Epoch 68/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2903.5779 - acc: 0.3038 - val_loss: 946.5631 - val_acc: 0.2934\n",
      "Epoch 69/300\n",
      "7471/7471 [==============================] - 2s 305us/step - loss: 2903.3025 - acc: 0.3038 - val_loss: 946.3795 - val_acc: 0.2934\n",
      "Epoch 70/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2903.0394 - acc: 0.3040 - val_loss: 946.2471 - val_acc: 0.2934\n",
      "Epoch 71/300\n",
      "7471/7471 [==============================] - 2s 315us/step - loss: 2928.3788 - acc: 0.3040 - val_loss: 946.1538 - val_acc: 0.2934\n",
      "Epoch 72/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2903.2293 - acc: 0.3038 - val_loss: 946.0694 - val_acc: 0.2934\n",
      "Epoch 73/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 6904.5925 - acc: 0.3040 - val_loss: 946.0725 - val_acc: 0.2934\n",
      "Epoch 74/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2907.9640 - acc: 0.3040 - val_loss: 946.0192 - val_acc: 0.2934\n",
      "Epoch 75/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2902.3668 - acc: 0.3040 - val_loss: 945.9687 - val_acc: 0.2934\n",
      "Epoch 76/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2902.2818 - acc: 0.3040 - val_loss: 945.9370 - val_acc: 0.2934\n",
      "Epoch 77/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2902.1977 - acc: 0.3040 - val_loss: 945.9211 - val_acc: 0.2934\n",
      "Epoch 78/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2902.1989 - acc: 0.3040 - val_loss: 945.9172 - val_acc: 0.2934\n",
      "Epoch 79/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2902.0947 - acc: 0.3040 - val_loss: 945.9132 - val_acc: 0.2934\n",
      "Epoch 80/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2924.3668 - acc: 0.3041 - val_loss: 945.9257 - val_acc: 0.2934\n",
      "Epoch 81/300\n",
      "7471/7471 [==============================] - 2s 291us/step - loss: 2902.1230 - acc: 0.3040 - val_loss: 945.9258 - val_acc: 0.2934\n",
      "Epoch 82/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2902.0050 - acc: 0.3040 - val_loss: 945.9368 - val_acc: 0.2934\n",
      "Epoch 83/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.9740 - acc: 0.3040 - val_loss: 945.9434 - val_acc: 0.2934\n",
      "Epoch 84/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2901.9587 - acc: 0.3040 - val_loss: 945.9574 - val_acc: 0.2934\n",
      "Epoch 85/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2926.1984 - acc: 0.3041 - val_loss: 945.9823 - val_acc: 0.2934\n",
      "Epoch 86/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2901.9419 - acc: 0.3040 - val_loss: 945.9739 - val_acc: 0.2934\n",
      "Epoch 87/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.9358 - acc: 0.3040 - val_loss: 945.9750 - val_acc: 0.2934\n",
      "Epoch 88/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2902.8094 - acc: 0.3040 - val_loss: 945.9849 - val_acc: 0.2934\n",
      "Epoch 89/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.9148 - acc: 0.3040 - val_loss: 945.9902 - val_acc: 0.2934\n",
      "Epoch 90/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.9156 - acc: 0.3040 - val_loss: 946.0035 - val_acc: 0.2934\n",
      "Epoch 91/300\n",
      "7471/7471 [==============================] - 2s 306us/step - loss: 2902.2223 - acc: 0.3037 - val_loss: 946.0085 - val_acc: 0.2934\n",
      "Epoch 92/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2902.0808 - acc: 0.3040 - val_loss: 946.0183 - val_acc: 0.2934\n",
      "Epoch 93/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8952 - acc: 0.3040 - val_loss: 946.0185 - val_acc: 0.2934\n",
      "Epoch 94/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.9106 - acc: 0.3040 - val_loss: 946.0205 - val_acc: 0.2934\n",
      "Epoch 95/300\n",
      "7471/7471 [==============================] - 2s 289us/step - loss: 2902.6388 - acc: 0.3038 - val_loss: 946.0279 - val_acc: 0.2934\n",
      "Epoch 96/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.9076 - acc: 0.3040 - val_loss: 946.0200 - val_acc: 0.2934\n",
      "Epoch 97/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8929 - acc: 0.3040 - val_loss: 946.0363 - val_acc: 0.2934\n",
      "Epoch 98/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2906.4742 - acc: 0.3037 - val_loss: 946.0423 - val_acc: 0.2934\n",
      "Epoch 99/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.8822 - acc: 0.3040 - val_loss: 946.0531 - val_acc: 0.2934\n",
      "Epoch 100/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 3035.2065 - acc: 0.3040 - val_loss: 946.0515 - val_acc: 0.2934\n",
      "Epoch 101/300\n",
      "7471/7471 [==============================] - 2s 307us/step - loss: 2902.5385 - acc: 0.3038 - val_loss: 946.0524 - val_acc: 0.2934\n",
      "Epoch 102/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.8985 - acc: 0.3040 - val_loss: 946.0601 - val_acc: 0.2934\n",
      "Epoch 103/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8868 - acc: 0.3041 - val_loss: 946.0631 - val_acc: 0.2934\n",
      "Epoch 104/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2902.9777 - acc: 0.3040 - val_loss: 946.0743 - val_acc: 0.2934\n",
      "Epoch 105/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2906.2082 - acc: 0.3040 - val_loss: 946.0692 - val_acc: 0.2934\n",
      "Epoch 106/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8926 - acc: 0.3040 - val_loss: 946.0747 - val_acc: 0.2934\n",
      "Epoch 107/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8901 - acc: 0.3040 - val_loss: 946.0745 - val_acc: 0.2934\n",
      "Epoch 108/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2901.8897 - acc: 0.3040 - val_loss: 946.0738 - val_acc: 0.2934\n",
      "Epoch 109/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.8883 - acc: 0.3040 - val_loss: 946.0799 - val_acc: 0.2934\n",
      "Epoch 110/300\n",
      "7471/7471 [==============================] - 2s 307us/step - loss: 2901.8938 - acc: 0.3040 - val_loss: 946.0810 - val_acc: 0.2934\n",
      "Epoch 111/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 3104.8960 - acc: 0.3041 - val_loss: 946.1211 - val_acc: 0.2934\n",
      "Epoch 112/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2902.5547 - acc: 0.3040 - val_loss: 946.0974 - val_acc: 0.2934\n",
      "Epoch 113/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8803 - acc: 0.3040 - val_loss: 946.0978 - val_acc: 0.2934\n",
      "Epoch 114/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.8875 - acc: 0.3040 - val_loss: 946.0899 - val_acc: 0.2934\n",
      "Epoch 115/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8951 - acc: 0.3040 - val_loss: 946.0903 - val_acc: 0.2934\n",
      "Epoch 116/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.8937 - acc: 0.3040 - val_loss: 946.1027 - val_acc: 0.2934\n",
      "Epoch 117/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2903.1161 - acc: 0.3037 - val_loss: 946.0885 - val_acc: 0.2934\n",
      "Epoch 118/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8909 - acc: 0.3040 - val_loss: 946.0900 - val_acc: 0.2934\n",
      "Epoch 119/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2901.8831 - acc: 0.3040 - val_loss: 946.0884 - val_acc: 0.2934\n",
      "Epoch 120/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.8872 - acc: 0.3040 - val_loss: 946.0698 - val_acc: 0.2934\n",
      "Epoch 121/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2901.8794 - acc: 0.3040 - val_loss: 946.0826 - val_acc: 0.2934\n",
      "Epoch 122/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2902.0557 - acc: 0.3038 - val_loss: 946.0841 - val_acc: 0.2934\n",
      "Epoch 123/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.9454 - acc: 0.3038 - val_loss: 946.1005 - val_acc: 0.2934\n",
      "Epoch 124/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2904.0837 - acc: 0.3038 - val_loss: 946.0928 - val_acc: 0.2934\n",
      "Epoch 125/300\n",
      "7471/7471 [==============================] - 2s 287us/step - loss: 2902.3298 - acc: 0.3038 - val_loss: 946.0857 - val_acc: 0.2934\n",
      "Epoch 126/300\n",
      "7471/7471 [==============================] - 2s 317us/step - loss: 2901.8815 - acc: 0.3040 - val_loss: 946.1001 - val_acc: 0.2934\n",
      "Epoch 127/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2921.8763 - acc: 0.3040 - val_loss: 946.1008 - val_acc: 0.2934\n",
      "Epoch 128/300\n",
      "7471/7471 [==============================] - 2s 287us/step - loss: 2905.3907 - acc: 0.3040 - val_loss: 946.0917 - val_acc: 0.2934\n",
      "Epoch 129/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8794 - acc: 0.3040 - val_loss: 946.0954 - val_acc: 0.2934\n",
      "Epoch 130/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2901.8838 - acc: 0.3040 - val_loss: 946.0953 - val_acc: 0.2934\n",
      "Epoch 131/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2956.0863 - acc: 0.3040 - val_loss: 946.0901 - val_acc: 0.2934\n",
      "Epoch 132/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8919 - acc: 0.3040 - val_loss: 946.0928 - val_acc: 0.2934\n",
      "Epoch 133/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.9883 - acc: 0.3040 - val_loss: 946.0959 - val_acc: 0.2934\n",
      "Epoch 134/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2902.0276 - acc: 0.3040 - val_loss: 946.0820 - val_acc: 0.2934\n",
      "Epoch 135/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8774 - acc: 0.3040 - val_loss: 946.0915 - val_acc: 0.2934\n",
      "Epoch 136/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8845 - acc: 0.3040 - val_loss: 946.0895 - val_acc: 0.2934\n",
      "Epoch 137/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8864 - acc: 0.3040 - val_loss: 946.0695 - val_acc: 0.2934\n",
      "Epoch 138/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2902.7777 - acc: 0.3041 - val_loss: 946.0861 - val_acc: 0.2934\n",
      "Epoch 139/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8768 - acc: 0.3040 - val_loss: 946.1033 - val_acc: 0.2934\n",
      "Epoch 140/300\n",
      "7471/7471 [==============================] - 2s 289us/step - loss: 2901.8802 - acc: 0.3040 - val_loss: 946.1095 - val_acc: 0.2934\n",
      "Epoch 141/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2909.5682 - acc: 0.3040 - val_loss: 946.1018 - val_acc: 0.2934\n",
      "Epoch 142/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8993 - acc: 0.3040 - val_loss: 946.1037 - val_acc: 0.2934\n",
      "Epoch 143/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8935 - acc: 0.3040 - val_loss: 946.0990 - val_acc: 0.2934\n",
      "Epoch 144/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2902.1502 - acc: 0.3038 - val_loss: 946.0969 - val_acc: 0.2934\n",
      "Epoch 145/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2902.5812 - acc: 0.3040 - val_loss: 946.1035 - val_acc: 0.2934\n",
      "Epoch 146/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8892 - acc: 0.3040 - val_loss: 946.1099 - val_acc: 0.2934\n",
      "Epoch 147/300\n",
      "7471/7471 [==============================] - 2s 306us/step - loss: 2901.8748 - acc: 0.3040 - val_loss: 946.1050 - val_acc: 0.2934\n",
      "Epoch 148/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8807 - acc: 0.3040 - val_loss: 946.0976 - val_acc: 0.2934\n",
      "Epoch 149/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8935 - acc: 0.3040 - val_loss: 946.0886 - val_acc: 0.2934\n",
      "Epoch 150/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8895 - acc: 0.3040 - val_loss: 946.0961 - val_acc: 0.2934\n",
      "Epoch 151/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2909.8815 - acc: 0.3040 - val_loss: 946.0853 - val_acc: 0.2934\n",
      "Epoch 152/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.8802 - acc: 0.3041 - val_loss: 946.0868 - val_acc: 0.2934\n",
      "Epoch 153/300\n",
      "7471/7471 [==============================] - 2s 305us/step - loss: 2901.8862 - acc: 0.3040 - val_loss: 946.0782 - val_acc: 0.2934\n",
      "Epoch 154/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8856 - acc: 0.3040 - val_loss: 946.0912 - val_acc: 0.2934\n",
      "Epoch 155/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2901.8891 - acc: 0.3038 - val_loss: 946.0919 - val_acc: 0.2934\n",
      "Epoch 156/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2901.8870 - acc: 0.3040 - val_loss: 946.0934 - val_acc: 0.2934\n",
      "Epoch 157/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2902.4159 - acc: 0.3040 - val_loss: 946.0794 - val_acc: 0.2934\n",
      "Epoch 158/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2902.0051 - acc: 0.3040 - val_loss: 946.0896 - val_acc: 0.2934\n",
      "Epoch 159/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2901.9379 - acc: 0.3040 - val_loss: 946.0847 - val_acc: 0.2934\n",
      "Epoch 160/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8860 - acc: 0.3040 - val_loss: 946.0903 - val_acc: 0.2934\n",
      "Epoch 161/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2901.8909 - acc: 0.3040 - val_loss: 946.0755 - val_acc: 0.2934\n",
      "Epoch 162/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2901.9060 - acc: 0.3040 - val_loss: 946.0988 - val_acc: 0.2934\n",
      "Epoch 163/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8892 - acc: 0.3040 - val_loss: 946.1073 - val_acc: 0.2934\n",
      "Epoch 164/300\n",
      "7471/7471 [==============================] - 2s 289us/step - loss: 2905.0733 - acc: 0.3040 - val_loss: 946.0918 - val_acc: 0.2934\n",
      "Epoch 165/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8826 - acc: 0.3040 - val_loss: 946.0852 - val_acc: 0.2934\n",
      "Epoch 166/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2917.2181 - acc: 0.3040 - val_loss: 946.0891 - val_acc: 0.2934\n",
      "Epoch 167/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2901.8834 - acc: 0.3040 - val_loss: 946.0897 - val_acc: 0.2934\n",
      "Epoch 168/300\n",
      "7471/7471 [==============================] - 2s 306us/step - loss: 2901.8882 - acc: 0.3041 - val_loss: 946.0974 - val_acc: 0.2934\n",
      "Epoch 169/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2904.9934 - acc: 0.3038 - val_loss: 946.1039 - val_acc: 0.2934\n",
      "Epoch 170/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2901.8859 - acc: 0.3040 - val_loss: 946.0873 - val_acc: 0.2934\n",
      "Epoch 171/300\n",
      "7471/7471 [==============================] - 2s 307us/step - loss: 2901.8914 - acc: 0.3040 - val_loss: 946.0884 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/300\n",
      "7471/7471 [==============================] - 2s 288us/step - loss: 2901.8895 - acc: 0.3040 - val_loss: 946.0974 - val_acc: 0.2934\n",
      "Epoch 173/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 3123.8819 - acc: 0.3040 - val_loss: 946.0883 - val_acc: 0.2934\n",
      "Epoch 174/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.9037 - acc: 0.3040 - val_loss: 946.0750 - val_acc: 0.2934\n",
      "Epoch 175/300\n",
      "7471/7471 [==============================] - 2s 291us/step - loss: 2901.9168 - acc: 0.3040 - val_loss: 946.0882 - val_acc: 0.2934\n",
      "Epoch 176/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8879 - acc: 0.3040 - val_loss: 946.1069 - val_acc: 0.2934\n",
      "Epoch 177/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2941.2389 - acc: 0.3038 - val_loss: 946.1078 - val_acc: 0.2934\n",
      "Epoch 178/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8914 - acc: 0.3040 - val_loss: 946.0957 - val_acc: 0.2934\n",
      "Epoch 179/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 3310.1286 - acc: 0.3041 - val_loss: 946.1503 - val_acc: 0.2934\n",
      "Epoch 180/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.9148 - acc: 0.3040 - val_loss: 946.1178 - val_acc: 0.2934\n",
      "Epoch 181/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8829 - acc: 0.3040 - val_loss: 946.0984 - val_acc: 0.2934\n",
      "Epoch 182/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8833 - acc: 0.3040 - val_loss: 946.0983 - val_acc: 0.2934\n",
      "Epoch 183/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.8746 - acc: 0.3040 - val_loss: 946.0924 - val_acc: 0.2934\n",
      "Epoch 184/300\n",
      "7471/7471 [==============================] - 2s 291us/step - loss: 2901.8911 - acc: 0.3040 - val_loss: 946.0740 - val_acc: 0.2934\n",
      "Epoch 185/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2905.8834 - acc: 0.3040 - val_loss: 946.0924 - val_acc: 0.2934\n",
      "Epoch 186/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2901.8862 - acc: 0.3040 - val_loss: 946.0924 - val_acc: 0.2934\n",
      "Epoch 187/300\n",
      "7471/7471 [==============================] - 2s 305us/step - loss: 2901.8828 - acc: 0.3040 - val_loss: 946.0972 - val_acc: 0.2934\n",
      "Epoch 188/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8827 - acc: 0.3040 - val_loss: 946.1010 - val_acc: 0.2934\n",
      "Epoch 189/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2902.1681 - acc: 0.3038 - val_loss: 946.0963 - val_acc: 0.2934\n",
      "Epoch 190/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 3343.0660 - acc: 0.3040 - val_loss: 946.0828 - val_acc: 0.2934\n",
      "Epoch 191/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8822 - acc: 0.3040 - val_loss: 946.0885 - val_acc: 0.2934\n",
      "Epoch 192/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8888 - acc: 0.3040 - val_loss: 946.0934 - val_acc: 0.2934\n",
      "Epoch 193/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2902.8270 - acc: 0.3040 - val_loss: 946.0925 - val_acc: 0.2934\n",
      "Epoch 194/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2902.4199 - acc: 0.3038 - val_loss: 946.0879 - val_acc: 0.2934\n",
      "Epoch 195/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2901.8952 - acc: 0.3040 - val_loss: 946.0816 - val_acc: 0.2934\n",
      "Epoch 196/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 3340.1418 - acc: 0.3038 - val_loss: 946.0848 - val_acc: 0.2934\n",
      "Epoch 197/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8887 - acc: 0.3040 - val_loss: 946.0739 - val_acc: 0.2934\n",
      "Epoch 198/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8796 - acc: 0.3040 - val_loss: 946.0808 - val_acc: 0.2934\n",
      "Epoch 199/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2901.8856 - acc: 0.3040 - val_loss: 946.0795 - val_acc: 0.2934\n",
      "Epoch 200/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8878 - acc: 0.3040 - val_loss: 946.0903 - val_acc: 0.2934\n",
      "Epoch 201/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8854 - acc: 0.3040 - val_loss: 946.0934 - val_acc: 0.2934\n",
      "Epoch 202/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.0931 - val_acc: 0.2934\n",
      "Epoch 203/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8867 - acc: 0.3040 - val_loss: 946.0974 - val_acc: 0.2934\n",
      "Epoch 204/300\n",
      "7471/7471 [==============================] - 2s 307us/step - loss: 2901.8854 - acc: 0.3040 - val_loss: 946.0894 - val_acc: 0.2934\n",
      "Epoch 205/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2901.8895 - acc: 0.3040 - val_loss: 946.0921 - val_acc: 0.2934\n",
      "Epoch 206/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 3628.8253 - acc: 0.3041 - val_loss: 946.1327 - val_acc: 0.2934\n",
      "Epoch 207/300\n",
      "7471/7471 [==============================] - 2s 286us/step - loss: 2901.8888 - acc: 0.3041 - val_loss: 946.1018 - val_acc: 0.2934\n",
      "Epoch 208/300\n",
      "7471/7471 [==============================] - 2s 305us/step - loss: 2902.3827 - acc: 0.3038 - val_loss: 946.1046 - val_acc: 0.2934\n",
      "Epoch 209/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8870 - acc: 0.3040 - val_loss: 946.0915 - val_acc: 0.2934\n",
      "Epoch 210/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2952.7340 - acc: 0.3040 - val_loss: 946.0952 - val_acc: 0.2934\n",
      "Epoch 211/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8862 - acc: 0.3040 - val_loss: 946.0980 - val_acc: 0.2934\n",
      "Epoch 212/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8915 - acc: 0.3040 - val_loss: 946.0963 - val_acc: 0.2934\n",
      "Epoch 213/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8859 - acc: 0.3040 - val_loss: 946.0895 - val_acc: 0.2934\n",
      "Epoch 214/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2910.4417 - acc: 0.3040 - val_loss: 946.0990 - val_acc: 0.2934\n",
      "Epoch 215/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8947 - acc: 0.3040 - val_loss: 946.1007 - val_acc: 0.2934\n",
      "Epoch 216/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2924.6968 - acc: 0.3041 - val_loss: 946.1074 - val_acc: 0.2934\n",
      "Epoch 217/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2901.8979 - acc: 0.3040 - val_loss: 946.0841 - val_acc: 0.2934\n",
      "Epoch 218/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8919 - acc: 0.3040 - val_loss: 946.0938 - val_acc: 0.2934\n",
      "Epoch 219/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8815 - acc: 0.3040 - val_loss: 946.1009 - val_acc: 0.2934\n",
      "Epoch 220/300\n",
      "7471/7471 [==============================] - 2s 289us/step - loss: 2901.8972 - acc: 0.3038 - val_loss: 946.0863 - val_acc: 0.2934\n",
      "Epoch 221/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.8775 - acc: 0.3040 - val_loss: 946.0953 - val_acc: 0.2934\n",
      "Epoch 222/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2902.0574 - acc: 0.3040 - val_loss: 946.0894 - val_acc: 0.2934\n",
      "Epoch 223/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.8779 - acc: 0.3040 - val_loss: 946.0980 - val_acc: 0.2934\n",
      "Epoch 224/300\n",
      "7471/7471 [==============================] - 2s 304us/step - loss: 2901.8901 - acc: 0.3040 - val_loss: 946.0953 - val_acc: 0.2934\n",
      "Epoch 225/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2901.8874 - acc: 0.3040 - val_loss: 946.1107 - val_acc: 0.2934\n",
      "Epoch 226/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2901.8780 - acc: 0.3040 - val_loss: 946.1025 - val_acc: 0.2934\n",
      "Epoch 227/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2901.8825 - acc: 0.3040 - val_loss: 946.0693 - val_acc: 0.2934\n",
      "Epoch 228/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 3058.7744 - acc: 0.3038 - val_loss: 946.1224 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2901.8854 - acc: 0.3040 - val_loss: 946.1113 - val_acc: 0.2934\n",
      "Epoch 230/300\n",
      "7471/7471 [==============================] - 2s 291us/step - loss: 2901.8792 - acc: 0.3040 - val_loss: 946.1099 - val_acc: 0.2934\n",
      "Epoch 231/300\n",
      "7471/7471 [==============================] - 2s 288us/step - loss: 2901.8793 - acc: 0.3040 - val_loss: 946.1173 - val_acc: 0.2934\n",
      "Epoch 232/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8911 - acc: 0.3040 - val_loss: 946.1169 - val_acc: 0.2934\n",
      "Epoch 233/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2901.8854 - acc: 0.3040 - val_loss: 946.1106 - val_acc: 0.2934\n",
      "Epoch 234/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8742 - acc: 0.3040 - val_loss: 946.1140 - val_acc: 0.2934\n",
      "Epoch 235/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2916.4768 - acc: 0.3040 - val_loss: 946.1212 - val_acc: 0.2934\n",
      "Epoch 236/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8876 - acc: 0.3040 - val_loss: 946.1188 - val_acc: 0.2934\n",
      "Epoch 237/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8767 - acc: 0.3040 - val_loss: 946.1063 - val_acc: 0.2934\n",
      "Epoch 238/300\n",
      "7471/7471 [==============================] - 2s 291us/step - loss: 2901.8898 - acc: 0.3040 - val_loss: 946.1141 - val_acc: 0.2934\n",
      "Epoch 239/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.9471 - acc: 0.3041 - val_loss: 946.0960 - val_acc: 0.2934\n",
      "Epoch 240/300\n",
      "7471/7471 [==============================] - 2s 286us/step - loss: 2901.8865 - acc: 0.3040 - val_loss: 946.0989 - val_acc: 0.2934\n",
      "Epoch 241/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.9958 - acc: 0.3040 - val_loss: 946.1065 - val_acc: 0.2934\n",
      "Epoch 242/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.8949 - acc: 0.3040 - val_loss: 946.1095 - val_acc: 0.2934\n",
      "Epoch 243/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8830 - acc: 0.3040 - val_loss: 946.1021 - val_acc: 0.2934\n",
      "Epoch 244/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8801 - acc: 0.3040 - val_loss: 946.1009 - val_acc: 0.2934\n",
      "Epoch 245/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8881 - acc: 0.3040 - val_loss: 946.1116 - val_acc: 0.2934\n",
      "Epoch 246/300\n",
      "7471/7471 [==============================] - 2s 291us/step - loss: 2901.8885 - acc: 0.3040 - val_loss: 946.1124 - val_acc: 0.2934\n",
      "Epoch 247/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2903.6299 - acc: 0.3038 - val_loss: 946.0953 - val_acc: 0.2934\n",
      "Epoch 248/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.8867 - acc: 0.3040 - val_loss: 946.1091 - val_acc: 0.2934\n",
      "Epoch 249/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8917 - acc: 0.3038 - val_loss: 946.1009 - val_acc: 0.2934\n",
      "Epoch 250/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8864 - acc: 0.3040 - val_loss: 946.0978 - val_acc: 0.2934\n",
      "Epoch 251/300\n",
      "7471/7471 [==============================] - 2s 287us/step - loss: 2901.8938 - acc: 0.3040 - val_loss: 946.0911 - val_acc: 0.2934\n",
      "Epoch 252/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8789 - acc: 0.3040 - val_loss: 946.1005 - val_acc: 0.2934\n",
      "Epoch 253/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2901.8905 - acc: 0.3040 - val_loss: 946.0933 - val_acc: 0.2934\n",
      "Epoch 254/300\n",
      "7471/7471 [==============================] - 2s 285us/step - loss: 2901.8803 - acc: 0.3040 - val_loss: 946.0967 - val_acc: 0.2934\n",
      "Epoch 255/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.8811 - acc: 0.3040 - val_loss: 946.0846 - val_acc: 0.2934\n",
      "Epoch 256/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2901.8826 - acc: 0.3040 - val_loss: 946.0911 - val_acc: 0.2934\n",
      "Epoch 257/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8827 - acc: 0.3040 - val_loss: 946.0920 - val_acc: 0.2934\n",
      "Epoch 258/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2901.9090 - acc: 0.3040 - val_loss: 946.0810 - val_acc: 0.2934\n",
      "Epoch 259/300\n",
      "7471/7471 [==============================] - 2s 290us/step - loss: 2901.8870 - acc: 0.3040 - val_loss: 946.0864 - val_acc: 0.2934\n",
      "Epoch 260/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2912.9322 - acc: 0.3038 - val_loss: 946.0886 - val_acc: 0.2934\n",
      "Epoch 261/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 3588.5666 - acc: 0.3040 - val_loss: 946.1052 - val_acc: 0.2934\n",
      "Epoch 262/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.9107 - acc: 0.3040 - val_loss: 946.0728 - val_acc: 0.2934\n",
      "Epoch 263/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2905.8778 - acc: 0.3040 - val_loss: 946.0794 - val_acc: 0.2934\n",
      "Epoch 264/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.8984 - acc: 0.3040 - val_loss: 946.0880 - val_acc: 0.2934\n",
      "Epoch 265/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8823 - acc: 0.3040 - val_loss: 946.0830 - val_acc: 0.2934\n",
      "Epoch 266/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.8854 - acc: 0.3040 - val_loss: 946.0867 - val_acc: 0.2934\n",
      "Epoch 267/300\n",
      "7471/7471 [==============================] - 2s 289us/step - loss: 2901.8875 - acc: 0.3040 - val_loss: 946.0746 - val_acc: 0.2934\n",
      "Epoch 268/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8748 - acc: 0.3040 - val_loss: 946.0857 - val_acc: 0.2934\n",
      "Epoch 269/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8833 - acc: 0.3040 - val_loss: 946.0924 - val_acc: 0.2934\n",
      "Epoch 270/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8867 - acc: 0.3040 - val_loss: 946.0875 - val_acc: 0.2934\n",
      "Epoch 271/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.8884 - acc: 0.3040 - val_loss: 946.0877 - val_acc: 0.2934\n",
      "Epoch 272/300\n",
      "7471/7471 [==============================] - 2s 291us/step - loss: 2901.8900 - acc: 0.3040 - val_loss: 946.0779 - val_acc: 0.2934\n",
      "Epoch 273/300\n",
      "7471/7471 [==============================] - 2s 313us/step - loss: 2901.8810 - acc: 0.3040 - val_loss: 946.0808 - val_acc: 0.2934\n",
      "Epoch 274/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8979 - acc: 0.3040 - val_loss: 946.0550 - val_acc: 0.2934\n",
      "Epoch 275/300\n",
      "7471/7471 [==============================] - 2s 307us/step - loss: 2901.8919 - acc: 0.3040 - val_loss: 946.0851 - val_acc: 0.2934\n",
      "Epoch 276/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.9538 - acc: 0.3040 - val_loss: 946.0811 - val_acc: 0.2934\n",
      "Epoch 277/300\n",
      "7471/7471 [==============================] - 2s 300us/step - loss: 2901.8809 - acc: 0.3040 - val_loss: 946.0899 - val_acc: 0.2934\n",
      "Epoch 278/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2903.4631 - acc: 0.3038 - val_loss: 946.0834 - val_acc: 0.2934\n",
      "Epoch 279/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8763 - acc: 0.3040 - val_loss: 946.0864 - val_acc: 0.2934\n",
      "Epoch 280/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8882 - acc: 0.3040 - val_loss: 946.0889 - val_acc: 0.2934\n",
      "Epoch 281/300\n",
      "7471/7471 [==============================] - 2s 289us/step - loss: 2901.8816 - acc: 0.3040 - val_loss: 946.0827 - val_acc: 0.2934\n",
      "Epoch 282/300\n",
      "7471/7471 [==============================] - 2s 296us/step - loss: 2901.8948 - acc: 0.3040 - val_loss: 946.0841 - val_acc: 0.2934\n",
      "Epoch 283/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2904.4889 - acc: 0.3041 - val_loss: 946.0933 - val_acc: 0.2934\n",
      "Epoch 284/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8838 - acc: 0.3040 - val_loss: 946.0838 - val_acc: 0.2934\n",
      "Epoch 285/300\n",
      "7471/7471 [==============================] - 2s 297us/step - loss: 2901.8788 - acc: 0.3040 - val_loss: 946.0830 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/300\n",
      "7471/7471 [==============================] - 2s 292us/step - loss: 2901.8870 - acc: 0.3040 - val_loss: 946.0771 - val_acc: 0.2934\n",
      "Epoch 287/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8857 - acc: 0.3040 - val_loss: 946.0875 - val_acc: 0.2934\n",
      "Epoch 288/300\n",
      "7471/7471 [==============================] - 2s 307us/step - loss: 2901.8826 - acc: 0.3040 - val_loss: 946.0701 - val_acc: 0.2934\n",
      "Epoch 289/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8840 - acc: 0.3040 - val_loss: 946.1073 - val_acc: 0.2934\n",
      "Epoch 290/300\n",
      "7471/7471 [==============================] - 2s 301us/step - loss: 2901.8881 - acc: 0.3040 - val_loss: 946.0911 - val_acc: 0.2934\n",
      "Epoch 291/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8868 - acc: 0.3040 - val_loss: 946.0879 - val_acc: 0.2934\n",
      "Epoch 292/300\n",
      "7471/7471 [==============================] - 2s 305us/step - loss: 2901.8843 - acc: 0.3040 - val_loss: 946.0918 - val_acc: 0.2934\n",
      "Epoch 293/300\n",
      "7471/7471 [==============================] - 2s 293us/step - loss: 2901.8876 - acc: 0.3040 - val_loss: 946.0825 - val_acc: 0.2934\n",
      "Epoch 294/300\n",
      "7471/7471 [==============================] - 2s 295us/step - loss: 2901.8868 - acc: 0.3040 - val_loss: 946.0908 - val_acc: 0.2934\n",
      "Epoch 295/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2901.8941 - acc: 0.3040 - val_loss: 946.0928 - val_acc: 0.2934\n",
      "Epoch 296/300\n",
      "7471/7471 [==============================] - 2s 298us/step - loss: 2927.4174 - acc: 0.3040 - val_loss: 946.0981 - val_acc: 0.2934\n",
      "Epoch 297/300\n",
      "7471/7471 [==============================] - 2s 299us/step - loss: 2901.8808 - acc: 0.3040 - val_loss: 946.0889 - val_acc: 0.2934\n",
      "Epoch 298/300\n",
      "7471/7471 [==============================] - 2s 302us/step - loss: 2901.8935 - acc: 0.3040 - val_loss: 946.0811 - val_acc: 0.2934\n",
      "Epoch 299/300\n",
      "7471/7471 [==============================] - 2s 303us/step - loss: 2940.0883 - acc: 0.3041 - val_loss: 946.0972 - val_acc: 0.2934\n",
      "Epoch 300/300\n",
      "7471/7471 [==============================] - 2s 294us/step - loss: 2901.9025 - acc: 0.3040 - val_loss: 946.0803 - val_acc: 0.2934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd4a9b6860>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "config = wandb.config\n",
    "\n",
    "config.batch_size = 128\n",
    "config.epochs = 300\n",
    "config.learning_rate = 0.01\n",
    "config.dropout = 0.8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(6))\n",
    "\n",
    "adam = Adam(lr=config.learning_rate)\n",
    "model.compile(loss=mean_squared_error, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(input_x_train, input_y_train, \n",
    "          epochs=config.epochs, validation_data=(input_x_test, input_y_test), \n",
    "          callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 layers neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/gilangrilhami/Predicting-Micronutrients-using-Neural-Networks-and-Random-Forest-notebooks/runs/91j3wna8\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7471 samples, validate on 1319 samples\n",
      "Epoch 1/300\n",
      "7471/7471 [==============================] - 4s 586us/step - loss: 242918697215.6516 - acc: 0.1944 - val_loss: 979.7029 - val_acc: 0.2146\n",
      "Epoch 2/300\n",
      "7471/7471 [==============================] - 3s 434us/step - loss: 105332.9616 - acc: 0.2060 - val_loss: 979.7001 - val_acc: 0.2146\n",
      "Epoch 3/300\n",
      "7471/7471 [==============================] - 3s 428us/step - loss: 5253.9283 - acc: 0.2075 - val_loss: 979.6968 - val_acc: 0.2146\n",
      "Epoch 4/300\n",
      "7471/7471 [==============================] - 3s 434us/step - loss: 5624.9228 - acc: 0.2104 - val_loss: 979.6926 - val_acc: 0.2146\n",
      "Epoch 5/300\n",
      "7471/7471 [==============================] - 3s 437us/step - loss: 3230.0049 - acc: 0.2100 - val_loss: 979.6873 - val_acc: 0.2146\n",
      "Epoch 6/300\n",
      "7471/7471 [==============================] - 3s 422us/step - loss: 5059.8429 - acc: 0.2099 - val_loss: 979.6814 - val_acc: 0.2146\n",
      "Epoch 7/300\n",
      "7471/7471 [==============================] - 3s 422us/step - loss: 4460813.3405 - acc: 0.2107 - val_loss: 979.6873 - val_acc: 0.2146\n",
      "Epoch 8/300\n",
      "7471/7471 [==============================] - 3s 412us/step - loss: 3041.0798 - acc: 0.2105 - val_loss: 979.6788 - val_acc: 0.2146\n",
      "Epoch 9/300\n",
      "7471/7471 [==============================] - 3s 411us/step - loss: 3095.0939 - acc: 0.2105 - val_loss: 979.6692 - val_acc: 0.2146\n",
      "Epoch 10/300\n",
      "7471/7471 [==============================] - 3s 421us/step - loss: 3225.7001 - acc: 0.2105 - val_loss: 979.6582 - val_acc: 0.2146\n",
      "Epoch 11/300\n",
      "7471/7471 [==============================] - 3s 433us/step - loss: 2948.8827 - acc: 0.2108 - val_loss: 979.6456 - val_acc: 0.2146\n",
      "Epoch 12/300\n",
      "7471/7471 [==============================] - 3s 414us/step - loss: 1051210.3629 - acc: 0.2099 - val_loss: 979.6331 - val_acc: 0.2146\n",
      "Epoch 13/300\n",
      "7471/7471 [==============================] - 3s 389us/step - loss: 3064.9286 - acc: 0.2103 - val_loss: 979.6169 - val_acc: 0.2146\n",
      "Epoch 14/300\n",
      "7471/7471 [==============================] - 3s 390us/step - loss: 2945.2911 - acc: 0.2109 - val_loss: 979.5988 - val_acc: 0.2146\n",
      "Epoch 15/300\n",
      "7471/7471 [==============================] - 4s 502us/step - loss: 2949.0082 - acc: 0.2107 - val_loss: 979.5779 - val_acc: 0.2146\n",
      "Epoch 16/300\n",
      "7471/7471 [==============================] - 4s 478us/step - loss: 3030.2567 - acc: 0.2103 - val_loss: 979.5546 - val_acc: 0.2146\n",
      "Epoch 17/300\n",
      "7471/7471 [==============================] - 3s 453us/step - loss: 2940.3114 - acc: 0.2103 - val_loss: 979.5287 - val_acc: 0.2146\n",
      "Epoch 18/300\n",
      "7471/7471 [==============================] - 3s 422us/step - loss: 17590.0999 - acc: 0.2100 - val_loss: 979.5001 - val_acc: 0.2146\n",
      "Epoch 19/300\n",
      "7471/7471 [==============================] - 3s 459us/step - loss: 2940.8627 - acc: 0.2104 - val_loss: 979.4675 - val_acc: 0.2146\n",
      "Epoch 20/300\n",
      "7471/7471 [==============================] - 4s 474us/step - loss: 3071.5122 - acc: 0.2103 - val_loss: 979.4303 - val_acc: 0.2146\n",
      "Epoch 21/300\n",
      "7471/7471 [==============================] - 3s 448us/step - loss: 15153.2384 - acc: 0.2105 - val_loss: 979.3913 - val_acc: 0.2146\n",
      "Epoch 22/300\n",
      "7471/7471 [==============================] - 3s 407us/step - loss: 2991.4860 - acc: 0.2105 - val_loss: 979.3452 - val_acc: 0.2146\n",
      "Epoch 23/300\n",
      "7471/7471 [==============================] - 4s 473us/step - loss: 2940.1748 - acc: 0.2103 - val_loss: 979.2922 - val_acc: 0.2146\n",
      "Epoch 24/300\n",
      "7471/7471 [==============================] - 3s 426us/step - loss: 2939.8992 - acc: 0.2107 - val_loss: 979.2333 - val_acc: 0.2146\n",
      "Epoch 25/300\n",
      "7471/7471 [==============================] - 3s 390us/step - loss: 4267.7901 - acc: 0.2100 - val_loss: 979.1677 - val_acc: 0.2146\n",
      "Epoch 26/300\n",
      "7471/7471 [==============================] - 3s 412us/step - loss: 2947.4460 - acc: 0.2103 - val_loss: 979.0944 - val_acc: 0.2146\n",
      "Epoch 27/300\n",
      "7471/7471 [==============================] - 3s 425us/step - loss: 2953.1299 - acc: 0.2245 - val_loss: 979.0088 - val_acc: 0.2934\n",
      "Epoch 28/300\n",
      "7471/7471 [==============================] - 3s 467us/step - loss: 2940.6289 - acc: 0.3040 - val_loss: 978.9164 - val_acc: 0.2934\n",
      "Epoch 29/300\n",
      "7471/7471 [==============================] - 3s 417us/step - loss: 2939.5448 - acc: 0.3040 - val_loss: 978.8116 - val_acc: 0.2934\n",
      "Epoch 30/300\n",
      "7471/7471 [==============================] - 4s 524us/step - loss: 2945.2623 - acc: 0.3040 - val_loss: 978.6937 - val_acc: 0.2934\n",
      "Epoch 31/300\n",
      "7471/7471 [==============================] - 3s 442us/step - loss: 2939.2161 - acc: 0.3040 - val_loss: 978.5634 - val_acc: 0.2934\n",
      "Epoch 32/300\n",
      "7471/7471 [==============================] - 3s 411us/step - loss: 2939.1194 - acc: 0.3034 - val_loss: 978.4143 - val_acc: 0.2934\n",
      "Epoch 33/300\n",
      "7471/7471 [==============================] - 3s 468us/step - loss: 3035.9638 - acc: 0.3038 - val_loss: 978.2496 - val_acc: 0.2934\n",
      "Epoch 34/300\n",
      "7471/7471 [==============================] - 3s 407us/step - loss: 2950.0171 - acc: 0.3038 - val_loss: 978.0636 - val_acc: 0.2934\n",
      "Epoch 35/300\n",
      "7471/7471 [==============================] - 3s 419us/step - loss: 2939.5171 - acc: 0.3038 - val_loss: 977.8575 - val_acc: 0.2934\n",
      "Epoch 36/300\n",
      "7471/7471 [==============================] - 3s 431us/step - loss: 2938.3192 - acc: 0.3038 - val_loss: 977.6275 - val_acc: 0.2934\n",
      "Epoch 37/300\n",
      "7471/7471 [==============================] - 3s 439us/step - loss: 2938.0202 - acc: 0.3040 - val_loss: 977.3741 - val_acc: 0.2934\n",
      "Epoch 38/300\n",
      "7471/7471 [==============================] - 3s 414us/step - loss: 2937.7269 - acc: 0.3040 - val_loss: 977.0858 - val_acc: 0.2934\n",
      "Epoch 39/300\n",
      "7471/7471 [==============================] - 3s 415us/step - loss: 2937.4030 - acc: 0.3040 - val_loss: 976.7703 - val_acc: 0.2934\n",
      "Epoch 40/300\n",
      "7471/7471 [==============================] - 3s 418us/step - loss: 2937.0444 - acc: 0.3040 - val_loss: 976.4137 - val_acc: 0.2934\n",
      "Epoch 41/300\n",
      "7471/7471 [==============================] - 3s 415us/step - loss: 18583.1242 - acc: 0.3040 - val_loss: 975.9391 - val_acc: 0.2934\n",
      "Epoch 42/300\n",
      "7471/7471 [==============================] - 3s 411us/step - loss: 2936.6461 - acc: 0.3040 - val_loss: 975.5023 - val_acc: 0.2934\n",
      "Epoch 43/300\n",
      "7471/7471 [==============================] - 3s 415us/step - loss: 2935.6254 - acc: 0.3040 - val_loss: 975.0258 - val_acc: 0.2934\n",
      "Epoch 44/300\n",
      "7471/7471 [==============================] - 3s 429us/step - loss: 2935.1471 - acc: 0.3038 - val_loss: 974.4967 - val_acc: 0.2934\n",
      "Epoch 45/300\n",
      "7471/7471 [==============================] - 3s 404us/step - loss: 3149.3379 - acc: 0.3037 - val_loss: 973.9143 - val_acc: 0.2934\n",
      "Epoch 46/300\n",
      "7471/7471 [==============================] - 3s 421us/step - loss: 2933.9106 - acc: 0.3040 - val_loss: 973.2653 - val_acc: 0.2934\n",
      "Epoch 47/300\n",
      "7471/7471 [==============================] - 3s 406us/step - loss: 2935.1060 - acc: 0.3040 - val_loss: 972.5646 - val_acc: 0.2934\n",
      "Epoch 48/300\n",
      "7471/7471 [==============================] - 3s 409us/step - loss: 2936.0741 - acc: 0.3041 - val_loss: 971.8128 - val_acc: 0.2934\n",
      "Epoch 49/300\n",
      "7471/7471 [==============================] - 3s 419us/step - loss: 2931.4747 - acc: 0.3038 - val_loss: 970.9595 - val_acc: 0.2934\n",
      "Epoch 50/300\n",
      "7471/7471 [==============================] - 3s 405us/step - loss: 2930.5268 - acc: 0.3040 - val_loss: 970.0565 - val_acc: 0.2934\n",
      "Epoch 51/300\n",
      "7471/7471 [==============================] - 3s 400us/step - loss: 2929.5107 - acc: 0.3040 - val_loss: 969.0833 - val_acc: 0.2934\n",
      "Epoch 52/300\n",
      "7471/7471 [==============================] - 3s 401us/step - loss: 2928.4213 - acc: 0.3040 - val_loss: 968.0275 - val_acc: 0.2934\n",
      "Epoch 53/300\n",
      "7471/7471 [==============================] - 3s 399us/step - loss: 2927.2449 - acc: 0.3040 - val_loss: 966.9301 - val_acc: 0.2934\n",
      "Epoch 54/300\n",
      "7471/7471 [==============================] - 3s 412us/step - loss: 2926.2011 - acc: 0.3041 - val_loss: 965.7337 - val_acc: 0.2934\n",
      "Epoch 55/300\n",
      "7471/7471 [==============================] - 3s 407us/step - loss: 2948.5498 - acc: 0.3040 - val_loss: 964.4911 - val_acc: 0.2934\n",
      "Epoch 56/300\n",
      "7471/7471 [==============================] - 3s 425us/step - loss: 3170.2141 - acc: 0.3038 - val_loss: 963.2083 - val_acc: 0.2934\n",
      "Epoch 57/300\n",
      "7471/7471 [==============================] - 3s 420us/step - loss: 2921.9000 - acc: 0.3040 - val_loss: 961.9015 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "7471/7471 [==============================] - 3s 429us/step - loss: 2920.7871 - acc: 0.3040 - val_loss: 960.5589 - val_acc: 0.2934\n",
      "Epoch 59/300\n",
      "7471/7471 [==============================] - 3s 435us/step - loss: 2978.0929 - acc: 0.3040 - val_loss: 959.1820 - val_acc: 0.2934\n",
      "Epoch 60/300\n",
      "7471/7471 [==============================] - 3s 434us/step - loss: 2919.1923 - acc: 0.3038 - val_loss: 957.7865 - val_acc: 0.2934\n",
      "Epoch 61/300\n",
      "7471/7471 [==============================] - 3s 439us/step - loss: 2922.4113 - acc: 0.3040 - val_loss: 956.4562 - val_acc: 0.2934\n",
      "Epoch 62/300\n",
      "7471/7471 [==============================] - 3s 396us/step - loss: 2914.6753 - acc: 0.3040 - val_loss: 955.2096 - val_acc: 0.2934\n",
      "Epoch 63/300\n",
      "7471/7471 [==============================] - 3s 416us/step - loss: 2913.0695 - acc: 0.3040 - val_loss: 953.9941 - val_acc: 0.2934\n",
      "Epoch 64/300\n",
      "7471/7471 [==============================] - 3s 421us/step - loss: 2911.7474 - acc: 0.3040 - val_loss: 952.8778 - val_acc: 0.2934\n",
      "Epoch 65/300\n",
      "7471/7471 [==============================] - 3s 402us/step - loss: 2910.5075 - acc: 0.3040 - val_loss: 951.8185 - val_acc: 0.2934\n",
      "Epoch 66/300\n",
      "7471/7471 [==============================] - 3s 380us/step - loss: 2909.3405 - acc: 0.3040 - val_loss: 950.8768 - val_acc: 0.2934\n",
      "Epoch 67/300\n",
      "7471/7471 [==============================] - 3s 383us/step - loss: 2941.2648 - acc: 0.3041 - val_loss: 950.0164 - val_acc: 0.2934\n",
      "Epoch 68/300\n",
      "7471/7471 [==============================] - 3s 382us/step - loss: 2908.2151 - acc: 0.3040 - val_loss: 949.2618 - val_acc: 0.2934\n",
      "Epoch 69/300\n",
      "7471/7471 [==============================] - 3s 392us/step - loss: 2906.5174 - acc: 0.3040 - val_loss: 948.6482 - val_acc: 0.2934\n",
      "Epoch 70/300\n",
      "7471/7471 [==============================] - 3s 387us/step - loss: 2905.7962 - acc: 0.3040 - val_loss: 948.1162 - val_acc: 0.2934\n",
      "Epoch 71/300\n",
      "7471/7471 [==============================] - 3s 393us/step - loss: 8475.9360 - acc: 0.3040 - val_loss: 947.5648 - val_acc: 0.2934\n",
      "Epoch 72/300\n",
      "7471/7471 [==============================] - 3s 381us/step - loss: 2904.5759 - acc: 0.3040 - val_loss: 947.2099 - val_acc: 0.2934\n",
      "Epoch 73/300\n",
      "7471/7471 [==============================] - 3s 384us/step - loss: 2904.1391 - acc: 0.3040 - val_loss: 946.9214 - val_acc: 0.2934\n",
      "Epoch 74/300\n",
      "7471/7471 [==============================] - 3s 408us/step - loss: 2903.7686 - acc: 0.3040 - val_loss: 946.6817 - val_acc: 0.2934\n",
      "Epoch 75/300\n",
      "7471/7471 [==============================] - 3s 419us/step - loss: 2903.4827 - acc: 0.3040 - val_loss: 946.4886 - val_acc: 0.2934\n",
      "Epoch 76/300\n",
      "7471/7471 [==============================] - 3s 410us/step - loss: 2903.1739 - acc: 0.3040 - val_loss: 946.3408 - val_acc: 0.2934\n",
      "Epoch 77/300\n",
      "7471/7471 [==============================] - 3s 416us/step - loss: 2902.9500 - acc: 0.3040 - val_loss: 946.2091 - val_acc: 0.2934\n",
      "Epoch 78/300\n",
      "7471/7471 [==============================] - 3s 416us/step - loss: 2902.7680 - acc: 0.3040 - val_loss: 946.1154 - val_acc: 0.2934\n",
      "Epoch 79/300\n",
      "7471/7471 [==============================] - 3s 403us/step - loss: 2989.6374 - acc: 0.3038 - val_loss: 946.0373 - val_acc: 0.2934\n",
      "Epoch 80/300\n",
      "7471/7471 [==============================] - 3s 418us/step - loss: 2902.4731 - acc: 0.3040 - val_loss: 945.9956 - val_acc: 0.2934\n",
      "Epoch 81/300\n",
      "7471/7471 [==============================] - 3s 411us/step - loss: 2902.3720 - acc: 0.3040 - val_loss: 945.9617 - val_acc: 0.2934\n",
      "Epoch 82/300\n",
      "7471/7471 [==============================] - 3s 412us/step - loss: 2902.3984 - acc: 0.3040 - val_loss: 945.9497 - val_acc: 0.2934\n",
      "Epoch 83/300\n",
      "7471/7471 [==============================] - 3s 413us/step - loss: 2902.3952 - acc: 0.3038 - val_loss: 945.9223 - val_acc: 0.2934\n",
      "Epoch 84/300\n",
      "7471/7471 [==============================] - 3s 416us/step - loss: 2902.1590 - acc: 0.3040 - val_loss: 945.9183 - val_acc: 0.2934\n",
      "Epoch 85/300\n",
      "7471/7471 [==============================] - 3s 419us/step - loss: 2902.1084 - acc: 0.3040 - val_loss: 945.9210 - val_acc: 0.2934\n",
      "Epoch 86/300\n",
      "7471/7471 [==============================] - 3s 405us/step - loss: 2942.1112 - acc: 0.3038 - val_loss: 945.9257 - val_acc: 0.2934\n",
      "Epoch 87/300\n",
      "7471/7471 [==============================] - 3s 401us/step - loss: 2902.0323 - acc: 0.3040 - val_loss: 945.9250 - val_acc: 0.2934\n",
      "Epoch 88/300\n",
      "7471/7471 [==============================] - 3s 406us/step - loss: 2902.1125 - acc: 0.3040 - val_loss: 945.9313 - val_acc: 0.2934\n",
      "Epoch 89/300\n",
      "7471/7471 [==============================] - 3s 411us/step - loss: 2901.9926 - acc: 0.3040 - val_loss: 945.9403 - val_acc: 0.2934\n",
      "Epoch 90/300\n",
      "7471/7471 [==============================] - 3s 395us/step - loss: 2937.6588 - acc: 0.3038 - val_loss: 945.9573 - val_acc: 0.2934\n",
      "Epoch 91/300\n",
      "7471/7471 [==============================] - 3s 425us/step - loss: 2902.4102 - acc: 0.3040 - val_loss: 945.9516 - val_acc: 0.2934\n",
      "Epoch 92/300\n",
      "7471/7471 [==============================] - 3s 414us/step - loss: 2910.6082 - acc: 0.3038 - val_loss: 945.9597 - val_acc: 0.2934\n",
      "Epoch 93/300\n",
      "7471/7471 [==============================] - 3s 416us/step - loss: 2901.9321 - acc: 0.3040 - val_loss: 945.9655 - val_acc: 0.2934\n",
      "Epoch 94/300\n",
      "7471/7471 [==============================] - 4s 483us/step - loss: 2902.1334 - acc: 0.3040 - val_loss: 945.9728 - val_acc: 0.2934\n",
      "Epoch 95/300\n",
      "7471/7471 [==============================] - 3s 464us/step - loss: 2901.9235 - acc: 0.3040 - val_loss: 945.9896 - val_acc: 0.2934\n",
      "Epoch 96/300\n",
      "7471/7471 [==============================] - 3s 437us/step - loss: 2901.9361 - acc: 0.3040 - val_loss: 946.0003 - val_acc: 0.2934\n",
      "Epoch 97/300\n",
      "7471/7471 [==============================] - 3s 428us/step - loss: 2901.9000 - acc: 0.3040 - val_loss: 946.0014 - val_acc: 0.2934\n",
      "Epoch 98/300\n",
      "7471/7471 [==============================] - 4s 473us/step - loss: 2901.9028 - acc: 0.3040 - val_loss: 946.0036 - val_acc: 0.2934\n",
      "Epoch 99/300\n",
      "7471/7471 [==============================] - 3s 413us/step - loss: 2901.8976 - acc: 0.3040 - val_loss: 946.0200 - val_acc: 0.2934\n",
      "Epoch 100/300\n",
      "7471/7471 [==============================] - 3s 432us/step - loss: 2939.4759 - acc: 0.3038 - val_loss: 946.0254 - val_acc: 0.2934\n",
      "Epoch 101/300\n",
      "7471/7471 [==============================] - 3s 423us/step - loss: 2901.8960 - acc: 0.3040 - val_loss: 946.0305 - val_acc: 0.2934\n",
      "Epoch 102/300\n",
      "7471/7471 [==============================] - 3s 464us/step - loss: 2901.8983 - acc: 0.3040 - val_loss: 946.0122 - val_acc: 0.2934\n",
      "Epoch 103/300\n",
      "7471/7471 [==============================] - 3s 408us/step - loss: 2901.8847 - acc: 0.3040 - val_loss: 946.0425 - val_acc: 0.2934\n",
      "Epoch 104/300\n",
      "7471/7471 [==============================] - 3s 433us/step - loss: 2901.8896 - acc: 0.3040 - val_loss: 946.0409 - val_acc: 0.2934\n",
      "Epoch 105/300\n",
      "7471/7471 [==============================] - 3s 441us/step - loss: 2901.8977 - acc: 0.3040 - val_loss: 946.0569 - val_acc: 0.2934\n",
      "Epoch 106/300\n",
      "7471/7471 [==============================] - 3s 424us/step - loss: 2901.9174 - acc: 0.3040 - val_loss: 946.0464 - val_acc: 0.2934\n",
      "Epoch 107/300\n",
      "7471/7471 [==============================] - 3s 407us/step - loss: 2901.8823 - acc: 0.3040 - val_loss: 946.0585 - val_acc: 0.2934\n",
      "Epoch 108/300\n",
      "7471/7471 [==============================] - 3s 432us/step - loss: 2901.8884 - acc: 0.3040 - val_loss: 946.0606 - val_acc: 0.2934\n",
      "Epoch 109/300\n",
      "7471/7471 [==============================] - 3s 423us/step - loss: 2901.8938 - acc: 0.3040 - val_loss: 946.0611 - val_acc: 0.2934\n",
      "Epoch 110/300\n",
      "7471/7471 [==============================] - 3s 457us/step - loss: 2901.8896 - acc: 0.3040 - val_loss: 946.0589 - val_acc: 0.2934\n",
      "Epoch 111/300\n",
      "7471/7471 [==============================] - 3s 421us/step - loss: 2901.8854 - acc: 0.3040 - val_loss: 946.0742 - val_acc: 0.2934\n",
      "Epoch 112/300\n",
      "7471/7471 [==============================] - 3s 425us/step - loss: 2901.8860 - acc: 0.3040 - val_loss: 946.0771 - val_acc: 0.2934\n",
      "Epoch 113/300\n",
      "7471/7471 [==============================] - 3s 424us/step - loss: 2901.8822 - acc: 0.3040 - val_loss: 946.0790 - val_acc: 0.2934\n",
      "Epoch 114/300\n",
      "7471/7471 [==============================] - 3s 402us/step - loss: 2901.8805 - acc: 0.3040 - val_loss: 946.0877 - val_acc: 0.2934\n",
      "Epoch 115/300\n",
      "7471/7471 [==============================] - 3s 412us/step - loss: 2901.8906 - acc: 0.3040 - val_loss: 946.0794 - val_acc: 0.2934\n",
      "Epoch 116/300\n",
      "7471/7471 [==============================] - 3s 415us/step - loss: 2901.9014 - acc: 0.3040 - val_loss: 946.0885 - val_acc: 0.2934\n",
      "Epoch 117/300\n",
      "7471/7471 [==============================] - 3s 413us/step - loss: 3027.5007 - acc: 0.3041 - val_loss: 946.1067 - val_acc: 0.2934\n",
      "Epoch 118/300\n",
      "7471/7471 [==============================] - 3s 414us/step - loss: 2901.8890 - acc: 0.3040 - val_loss: 946.0973 - val_acc: 0.2934\n",
      "Epoch 119/300\n",
      "7471/7471 [==============================] - 3s 409us/step - loss: 2901.8859 - acc: 0.3040 - val_loss: 946.0741 - val_acc: 0.2934\n",
      "Epoch 120/300\n",
      "7471/7471 [==============================] - 3s 417us/step - loss: 2901.8833 - acc: 0.3040 - val_loss: 946.0852 - val_acc: 0.2934\n",
      "Epoch 121/300\n",
      "7471/7471 [==============================] - 3s 443us/step - loss: 2901.8872 - acc: 0.3040 - val_loss: 946.0822 - val_acc: 0.2934\n",
      "Epoch 122/300\n",
      "7471/7471 [==============================] - 3s 411us/step - loss: 2901.8916 - acc: 0.3040 - val_loss: 946.0830 - val_acc: 0.2934\n",
      "Epoch 123/300\n",
      "7471/7471 [==============================] - 4s 471us/step - loss: 2901.8860 - acc: 0.3040 - val_loss: 946.0789 - val_acc: 0.2934\n",
      "Epoch 124/300\n",
      "7471/7471 [==============================] - 3s 428us/step - loss: 2901.9011 - acc: 0.3040 - val_loss: 946.0884 - val_acc: 0.2934\n",
      "Epoch 125/300\n",
      "7471/7471 [==============================] - 3s 429us/step - loss: 2901.8897 - acc: 0.3040 - val_loss: 946.0911 - val_acc: 0.2934\n",
      "Epoch 126/300\n",
      "7471/7471 [==============================] - 3s 447us/step - loss: 2901.8736 - acc: 0.3040 - val_loss: 946.0828 - val_acc: 0.2934\n",
      "Epoch 127/300\n",
      "7471/7471 [==============================] - 4s 500us/step - loss: 2901.8765 - acc: 0.3040 - val_loss: 946.0883 - val_acc: 0.2934\n",
      "Epoch 128/300\n",
      "7471/7471 [==============================] - 3s 420us/step - loss: 2901.8808 - acc: 0.3040 - val_loss: 946.0913 - val_acc: 0.2934\n",
      "Epoch 129/300\n",
      "7471/7471 [==============================] - 3s 424us/step - loss: 2901.8831 - acc: 0.3040 - val_loss: 946.0851 - val_acc: 0.2934\n",
      "Epoch 130/300\n",
      "7471/7471 [==============================] - 3s 426us/step - loss: 2901.8946 - acc: 0.3040 - val_loss: 946.0831 - val_acc: 0.2934\n",
      "Epoch 131/300\n",
      "7471/7471 [==============================] - 3s 435us/step - loss: 2901.8790 - acc: 0.3040 - val_loss: 946.0903 - val_acc: 0.2934\n",
      "Epoch 132/300\n",
      "7471/7471 [==============================] - 3s 459us/step - loss: 2901.8933 - acc: 0.3040 - val_loss: 946.0936 - val_acc: 0.2934\n",
      "Epoch 133/300\n",
      "7471/7471 [==============================] - 4s 545us/step - loss: 2901.8913 - acc: 0.3040 - val_loss: 946.0927 - val_acc: 0.2934\n",
      "Epoch 134/300\n",
      "7471/7471 [==============================] - 3s 457us/step - loss: 2901.8970 - acc: 0.3040 - val_loss: 946.0804 - val_acc: 0.2934\n",
      "Epoch 135/300\n",
      "7471/7471 [==============================] - 3s 431us/step - loss: 2963.6111 - acc: 0.3038 - val_loss: 946.0881 - val_acc: 0.2934\n",
      "Epoch 136/300\n",
      "7471/7471 [==============================] - 3s 449us/step - loss: 2901.8935 - acc: 0.3040 - val_loss: 946.0864 - val_acc: 0.2934\n",
      "Epoch 137/300\n",
      "7471/7471 [==============================] - 3s 439us/step - loss: 2901.8932 - acc: 0.3041 - val_loss: 946.0892 - val_acc: 0.2934\n",
      "Epoch 138/300\n",
      "7471/7471 [==============================] - 3s 443us/step - loss: 2901.8836 - acc: 0.3040 - val_loss: 946.0918 - val_acc: 0.2934\n",
      "Epoch 139/300\n",
      "7471/7471 [==============================] - 3s 441us/step - loss: 2901.8797 - acc: 0.3040 - val_loss: 946.0903 - val_acc: 0.2934\n",
      "Epoch 140/300\n",
      "7471/7471 [==============================] - 3s 467us/step - loss: 2901.8949 - acc: 0.3040 - val_loss: 946.0687 - val_acc: 0.2934\n",
      "Epoch 141/300\n",
      "7471/7471 [==============================] - 3s 450us/step - loss: 2901.8857 - acc: 0.3040 - val_loss: 946.0756 - val_acc: 0.2934\n",
      "Epoch 142/300\n",
      "7471/7471 [==============================] - 3s 457us/step - loss: 2901.9014 - acc: 0.3040 - val_loss: 946.0708 - val_acc: 0.2934\n",
      "Epoch 143/300\n",
      "7471/7471 [==============================] - 3s 456us/step - loss: 2901.8878 - acc: 0.3040 - val_loss: 946.0850 - val_acc: 0.2934\n",
      "Epoch 144/300\n",
      "7471/7471 [==============================] - 3s 465us/step - loss: 2901.8893 - acc: 0.3040 - val_loss: 946.0851 - val_acc: 0.2934\n",
      "Epoch 145/300\n",
      "7471/7471 [==============================] - 3s 448us/step - loss: 2901.8886 - acc: 0.3040 - val_loss: 946.0913 - val_acc: 0.2934\n",
      "Epoch 146/300\n",
      "7471/7471 [==============================] - 3s 439us/step - loss: 2901.8893 - acc: 0.3040 - val_loss: 946.0886 - val_acc: 0.2934\n",
      "Epoch 147/300\n",
      "7471/7471 [==============================] - 3s 440us/step - loss: 2901.8855 - acc: 0.3040 - val_loss: 946.0948 - val_acc: 0.2934\n",
      "Epoch 148/300\n",
      "7471/7471 [==============================] - 3s 440us/step - loss: 2901.8824 - acc: 0.3040 - val_loss: 946.0756 - val_acc: 0.2934\n",
      "Epoch 149/300\n",
      "7471/7471 [==============================] - 4s 496us/step - loss: 2903.1914 - acc: 0.3038 - val_loss: 946.0844 - val_acc: 0.2934\n",
      "Epoch 150/300\n",
      "7471/7471 [==============================] - 4s 600us/step - loss: 2901.8844 - acc: 0.3040 - val_loss: 946.0831 - val_acc: 0.2934\n",
      "Epoch 151/300\n",
      "7471/7471 [==============================] - 4s 515us/step - loss: 2901.8817 - acc: 0.3040 - val_loss: 946.0837 - val_acc: 0.2934\n",
      "Epoch 152/300\n",
      "7471/7471 [==============================] - 4s 562us/step - loss: 2901.8885 - acc: 0.3040 - val_loss: 946.0786 - val_acc: 0.2934\n",
      "Epoch 153/300\n",
      "7471/7471 [==============================] - 4s 509us/step - loss: 2901.8850 - acc: 0.3040 - val_loss: 946.0762 - val_acc: 0.2934\n",
      "Epoch 154/300\n",
      "7471/7471 [==============================] - 4s 567us/step - loss: 2901.8968 - acc: 0.3040 - val_loss: 946.0796 - val_acc: 0.2934\n",
      "Epoch 155/300\n",
      "7471/7471 [==============================] - 4s 529us/step - loss: 2901.8918 - acc: 0.3040 - val_loss: 946.0801 - val_acc: 0.2934\n",
      "Epoch 156/300\n",
      "7471/7471 [==============================] - 4s 526us/step - loss: 2901.8850 - acc: 0.3040 - val_loss: 946.0884 - val_acc: 0.2934\n",
      "Epoch 157/300\n",
      "7471/7471 [==============================] - 4s 528us/step - loss: 2901.8845 - acc: 0.3040 - val_loss: 946.0987 - val_acc: 0.2934\n",
      "Epoch 158/300\n",
      "7471/7471 [==============================] - 4s 533us/step - loss: 2901.8955 - acc: 0.3040 - val_loss: 946.0587 - val_acc: 0.2934\n",
      "Epoch 159/300\n",
      "7471/7471 [==============================] - 4s 530us/step - loss: 2901.8782 - acc: 0.3040 - val_loss: 946.0858 - val_acc: 0.2934\n",
      "Epoch 160/300\n",
      "7471/7471 [==============================] - 4s 526us/step - loss: 2901.8758 - acc: 0.3040 - val_loss: 946.0852 - val_acc: 0.2934\n",
      "Epoch 161/300\n",
      "7471/7471 [==============================] - 4s 505us/step - loss: 2901.8896 - acc: 0.3040 - val_loss: 946.0867 - val_acc: 0.2934\n",
      "Epoch 162/300\n",
      "7471/7471 [==============================] - 4s 501us/step - loss: 2901.8953 - acc: 0.3040 - val_loss: 946.0863 - val_acc: 0.2934\n",
      "Epoch 163/300\n",
      "7471/7471 [==============================] - 4s 505us/step - loss: 2901.8842 - acc: 0.3040 - val_loss: 946.0910 - val_acc: 0.2934\n",
      "Epoch 164/300\n",
      "7471/7471 [==============================] - 4s 502us/step - loss: 2901.8830 - acc: 0.3040 - val_loss: 946.0844 - val_acc: 0.2934\n",
      "Epoch 165/300\n",
      "7471/7471 [==============================] - 4s 507us/step - loss: 2901.8887 - acc: 0.3040 - val_loss: 946.0651 - val_acc: 0.2934\n",
      "Epoch 166/300\n",
      "7471/7471 [==============================] - 4s 501us/step - loss: 4366.1624 - acc: 0.3040 - val_loss: 946.0887 - val_acc: 0.2934\n",
      "Epoch 167/300\n",
      "7471/7471 [==============================] - 4s 508us/step - loss: 2924.8800 - acc: 0.3040 - val_loss: 946.0835 - val_acc: 0.2934\n",
      "Epoch 168/300\n",
      "7471/7471 [==============================] - 4s 494us/step - loss: 2904.8312 - acc: 0.3038 - val_loss: 946.0794 - val_acc: 0.2934\n",
      "Epoch 169/300\n",
      "7471/7471 [==============================] - 4s 500us/step - loss: 2901.8876 - acc: 0.3040 - val_loss: 946.0790 - val_acc: 0.2934\n",
      "Epoch 170/300\n",
      "7471/7471 [==============================] - 4s 517us/step - loss: 2910.6295 - acc: 0.3038 - val_loss: 946.0836 - val_acc: 0.2934\n",
      "Epoch 171/300\n",
      "7471/7471 [==============================] - 4s 539us/step - loss: 2901.8850 - acc: 0.3040 - val_loss: 946.0731 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/300\n",
      "7471/7471 [==============================] - 4s 539us/step - loss: 2901.9173 - acc: 0.3040 - val_loss: 946.0811 - val_acc: 0.2934\n",
      "Epoch 173/300\n",
      "7471/7471 [==============================] - 4s 538us/step - loss: 2901.8862 - acc: 0.3040 - val_loss: 946.0784 - val_acc: 0.2934\n",
      "Epoch 174/300\n",
      "7471/7471 [==============================] - 4s 526us/step - loss: 2901.8858 - acc: 0.3040 - val_loss: 946.0868 - val_acc: 0.2934\n",
      "Epoch 175/300\n",
      "7471/7471 [==============================] - 4s 531us/step - loss: 2901.8954 - acc: 0.3040 - val_loss: 946.0887 - val_acc: 0.2934\n",
      "Epoch 176/300\n",
      "7471/7471 [==============================] - 4s 469us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.0872 - val_acc: 0.2934\n",
      "Epoch 177/300\n",
      "7471/7471 [==============================] - 3s 450us/step - loss: 2901.8839 - acc: 0.3040 - val_loss: 946.0887 - val_acc: 0.2934\n",
      "Epoch 178/300\n",
      "7471/7471 [==============================] - 3s 453us/step - loss: 3178.3464 - acc: 0.3040 - val_loss: 946.1224 - val_acc: 0.2934\n",
      "Epoch 179/300\n",
      "7471/7471 [==============================] - 3s 456us/step - loss: 2901.8870 - acc: 0.3040 - val_loss: 946.1014 - val_acc: 0.2934\n",
      "Epoch 180/300\n",
      "7471/7471 [==============================] - 4s 473us/step - loss: 2901.8922 - acc: 0.3040 - val_loss: 946.0762 - val_acc: 0.2934\n",
      "Epoch 181/300\n",
      "7471/7471 [==============================] - 3s 409us/step - loss: 2901.8824 - acc: 0.3040 - val_loss: 946.0860 - val_acc: 0.2934\n",
      "Epoch 182/300\n",
      "7471/7471 [==============================] - 3s 410us/step - loss: 2901.8875 - acc: 0.3040 - val_loss: 946.0868 - val_acc: 0.2934\n",
      "Epoch 183/300\n",
      "7471/7471 [==============================] - 3s 432us/step - loss: 2901.8815 - acc: 0.3040 - val_loss: 946.0878 - val_acc: 0.2934\n",
      "Epoch 184/300\n",
      "7471/7471 [==============================] - 3s 418us/step - loss: 2902.8860 - acc: 0.3038 - val_loss: 946.0924 - val_acc: 0.2934\n",
      "Epoch 185/300\n",
      "7471/7471 [==============================] - 3s 399us/step - loss: 2901.8834 - acc: 0.3040 - val_loss: 946.0907 - val_acc: 0.2934\n",
      "Epoch 186/300\n",
      "7471/7471 [==============================] - 3s 410us/step - loss: 2901.9027 - acc: 0.3040 - val_loss: 946.0862 - val_acc: 0.2934\n",
      "Epoch 187/300\n",
      "7471/7471 [==============================] - 3s 415us/step - loss: 2901.8862 - acc: 0.3040 - val_loss: 946.1019 - val_acc: 0.2934\n",
      "Epoch 188/300\n",
      "7471/7471 [==============================] - 3s 425us/step - loss: 2901.8863 - acc: 0.3040 - val_loss: 946.0850 - val_acc: 0.2934\n",
      "Epoch 189/300\n",
      "7471/7471 [==============================] - 3s 407us/step - loss: 2901.8706 - acc: 0.3040 - val_loss: 946.0950 - val_acc: 0.2934\n",
      "Epoch 190/300\n",
      "7471/7471 [==============================] - 3s 410us/step - loss: 2901.8815 - acc: 0.3040 - val_loss: 946.0884 - val_acc: 0.2934\n",
      "Epoch 191/300\n",
      "7471/7471 [==============================] - 3s 397us/step - loss: 2901.8902 - acc: 0.3040 - val_loss: 946.0903 - val_acc: 0.2934\n",
      "Epoch 192/300\n",
      "7471/7471 [==============================] - 3s 418us/step - loss: 2901.8851 - acc: 0.3040 - val_loss: 946.0941 - val_acc: 0.2934\n",
      "Epoch 193/300\n",
      "7471/7471 [==============================] - 4s 473us/step - loss: 2901.8877 - acc: 0.3040 - val_loss: 946.0973 - val_acc: 0.2934\n",
      "Epoch 194/300\n",
      "7471/7471 [==============================] - 4s 518us/step - loss: 2901.8806 - acc: 0.3040 - val_loss: 946.0964 - val_acc: 0.2934\n",
      "Epoch 195/300\n",
      "7471/7471 [==============================] - 4s 539us/step - loss: 2901.8885 - acc: 0.3040 - val_loss: 946.0863 - val_acc: 0.2934\n",
      "Epoch 196/300\n",
      "7471/7471 [==============================] - 4s 572us/step - loss: 2901.8920 - acc: 0.3040 - val_loss: 946.0954 - val_acc: 0.2934\n",
      "Epoch 197/300\n",
      "7471/7471 [==============================] - 4s 510us/step - loss: 2901.8868 - acc: 0.3040 - val_loss: 946.0929 - val_acc: 0.2934\n",
      "Epoch 198/300\n",
      "7471/7471 [==============================] - 4s 536us/step - loss: 2901.8790 - acc: 0.3040 - val_loss: 946.0838 - val_acc: 0.2934\n",
      "Epoch 199/300\n",
      "7471/7471 [==============================] - 4s 547us/step - loss: 2901.8846 - acc: 0.3040 - val_loss: 946.0901 - val_acc: 0.2934\n",
      "Epoch 200/300\n",
      "7471/7471 [==============================] - 4s 566us/step - loss: 2901.8979 - acc: 0.3040 - val_loss: 946.0793 - val_acc: 0.2934\n",
      "Epoch 201/300\n",
      "7471/7471 [==============================] - 4s 536us/step - loss: 2901.8898 - acc: 0.3040 - val_loss: 946.0838 - val_acc: 0.2934\n",
      "Epoch 202/300\n",
      "7471/7471 [==============================] - 4s 521us/step - loss: 2901.8931 - acc: 0.3040 - val_loss: 946.0879 - val_acc: 0.2934\n",
      "Epoch 203/300\n",
      "7471/7471 [==============================] - 4s 527us/step - loss: 2902.2402 - acc: 0.3040 - val_loss: 946.0777 - val_acc: 0.2934\n",
      "Epoch 204/300\n",
      "7471/7471 [==============================] - 4s 530us/step - loss: 2901.8890 - acc: 0.3040 - val_loss: 946.0886 - val_acc: 0.2934\n",
      "Epoch 205/300\n",
      "7471/7471 [==============================] - 4s 530us/step - loss: 2901.8923 - acc: 0.3040 - val_loss: 946.0847 - val_acc: 0.2934\n",
      "Epoch 206/300\n",
      "7471/7471 [==============================] - 4s 542us/step - loss: 2901.8908 - acc: 0.3040 - val_loss: 946.0833 - val_acc: 0.2934\n",
      "Epoch 207/300\n",
      "7471/7471 [==============================] - 4s 515us/step - loss: 2901.8833 - acc: 0.3040 - val_loss: 946.0929 - val_acc: 0.2934\n",
      "Epoch 208/300\n",
      "7471/7471 [==============================] - 4s 513us/step - loss: 2901.8918 - acc: 0.3040 - val_loss: 946.0870 - val_acc: 0.2934\n",
      "Epoch 209/300\n",
      "7471/7471 [==============================] - 4s 527us/step - loss: 3023.0519 - acc: 0.3041 - val_loss: 946.0920 - val_acc: 0.2934\n",
      "Epoch 210/300\n",
      "7471/7471 [==============================] - 4s 515us/step - loss: 2901.8857 - acc: 0.3040 - val_loss: 946.0856 - val_acc: 0.2934\n",
      "Epoch 211/300\n",
      "7471/7471 [==============================] - 4s 499us/step - loss: 2901.8958 - acc: 0.3040 - val_loss: 946.0925 - val_acc: 0.2934\n",
      "Epoch 212/300\n",
      "7471/7471 [==============================] - 3s 454us/step - loss: 2901.8818 - acc: 0.3040 - val_loss: 946.0956 - val_acc: 0.2934\n",
      "Epoch 213/300\n",
      "7471/7471 [==============================] - 4s 544us/step - loss: 2901.8850 - acc: 0.3040 - val_loss: 946.0860 - val_acc: 0.2934\n",
      "Epoch 214/300\n",
      "7471/7471 [==============================] - 4s 538us/step - loss: 2901.8824 - acc: 0.3040 - val_loss: 946.0922 - val_acc: 0.2934\n",
      "Epoch 215/300\n",
      "7471/7471 [==============================] - 4s 542us/step - loss: 2902.1707 - acc: 0.3040 - val_loss: 946.0819 - val_acc: 0.2934\n",
      "Epoch 216/300\n",
      "7471/7471 [==============================] - 4s 554us/step - loss: 2901.8865 - acc: 0.3040 - val_loss: 946.0905 - val_acc: 0.2934\n",
      "Epoch 217/300\n",
      "7471/7471 [==============================] - 4s 553us/step - loss: 2901.9676 - acc: 0.3040 - val_loss: 946.0826 - val_acc: 0.2934\n",
      "Epoch 218/300\n",
      "7471/7471 [==============================] - 3s 446us/step - loss: 2901.8781 - acc: 0.3040 - val_loss: 946.0918 - val_acc: 0.2934\n",
      "Epoch 219/300\n",
      "7471/7471 [==============================] - 3s 449us/step - loss: 2901.8860 - acc: 0.3040 - val_loss: 946.0858 - val_acc: 0.2934\n",
      "Epoch 220/300\n",
      "7471/7471 [==============================] - 3s 448us/step - loss: 2901.8849 - acc: 0.3040 - val_loss: 946.1001 - val_acc: 0.2934\n",
      "Epoch 221/300\n",
      "7471/7471 [==============================] - 4s 480us/step - loss: 2901.8945 - acc: 0.3040 - val_loss: 946.0756 - val_acc: 0.2934\n",
      "Epoch 222/300\n",
      "7471/7471 [==============================] - 3s 449us/step - loss: 2918.4411 - acc: 0.3040 - val_loss: 946.0871 - val_acc: 0.2934\n",
      "Epoch 223/300\n",
      "7471/7471 [==============================] - 3s 438us/step - loss: 2901.8762 - acc: 0.3040 - val_loss: 946.0907 - val_acc: 0.2934\n",
      "Epoch 224/300\n",
      "7471/7471 [==============================] - 3s 430us/step - loss: 2901.8901 - acc: 0.3040 - val_loss: 946.1011 - val_acc: 0.2934\n",
      "Epoch 225/300\n",
      "7471/7471 [==============================] - 3s 428us/step - loss: 2901.8827 - acc: 0.3040 - val_loss: 946.0895 - val_acc: 0.2934\n",
      "Epoch 226/300\n",
      "7471/7471 [==============================] - 3s 428us/step - loss: 2901.8852 - acc: 0.3040 - val_loss: 946.0854 - val_acc: 0.2934\n",
      "Epoch 227/300\n",
      "7471/7471 [==============================] - 3s 406us/step - loss: 2901.8896 - acc: 0.3040 - val_loss: 946.0925 - val_acc: 0.2934\n",
      "Epoch 228/300\n",
      "7471/7471 [==============================] - 3s 463us/step - loss: 2901.8927 - acc: 0.3040 - val_loss: 946.0792 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/300\n",
      "7471/7471 [==============================] - 3s 433us/step - loss: 2901.8798 - acc: 0.3040 - val_loss: 946.0888 - val_acc: 0.2934\n",
      "Epoch 230/300\n",
      "7471/7471 [==============================] - 3s 448us/step - loss: 2901.8907 - acc: 0.3040 - val_loss: 946.1043 - val_acc: 0.2934\n",
      "Epoch 231/300\n",
      "7471/7471 [==============================] - 3s 434us/step - loss: 2901.8835 - acc: 0.3040 - val_loss: 946.0818 - val_acc: 0.2934\n",
      "Epoch 232/300\n",
      "7471/7471 [==============================] - 3s 433us/step - loss: 2901.8890 - acc: 0.3040 - val_loss: 946.0902 - val_acc: 0.2934\n",
      "Epoch 233/300\n",
      "7471/7471 [==============================] - 3s 463us/step - loss: 2901.8833 - acc: 0.3040 - val_loss: 946.0921 - val_acc: 0.2934\n",
      "Epoch 234/300\n",
      "7471/7471 [==============================] - 3s 460us/step - loss: 2901.8876 - acc: 0.3040 - val_loss: 946.0976 - val_acc: 0.2934\n",
      "Epoch 235/300\n",
      "7471/7471 [==============================] - 3s 453us/step - loss: 2901.8866 - acc: 0.3040 - val_loss: 946.0997 - val_acc: 0.2934\n",
      "Epoch 236/300\n",
      "7471/7471 [==============================] - 3s 456us/step - loss: 2901.8909 - acc: 0.3040 - val_loss: 946.0867 - val_acc: 0.2934\n",
      "Epoch 237/300\n",
      "7471/7471 [==============================] - 3s 463us/step - loss: 2901.8880 - acc: 0.3040 - val_loss: 946.0971 - val_acc: 0.2934\n",
      "Epoch 238/300\n",
      "7471/7471 [==============================] - 3s 459us/step - loss: 2901.8989 - acc: 0.3040 - val_loss: 946.0614 - val_acc: 0.2934\n",
      "Epoch 239/300\n",
      "7471/7471 [==============================] - 3s 439us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.0897 - val_acc: 0.2934\n",
      "Epoch 240/300\n",
      "7471/7471 [==============================] - 3s 442us/step - loss: 2901.8877 - acc: 0.3040 - val_loss: 946.0864 - val_acc: 0.2934\n",
      "Epoch 241/300\n",
      "7471/7471 [==============================] - 3s 441us/step - loss: 2901.8856 - acc: 0.3040 - val_loss: 946.0967 - val_acc: 0.2934\n",
      "Epoch 242/300\n",
      "7471/7471 [==============================] - 3s 436us/step - loss: 2901.8934 - acc: 0.3040 - val_loss: 946.0937 - val_acc: 0.2934\n",
      "Epoch 243/300\n",
      "7471/7471 [==============================] - 3s 446us/step - loss: 2910.5782 - acc: 0.3038 - val_loss: 946.0982 - val_acc: 0.2934\n",
      "Epoch 244/300\n",
      "7471/7471 [==============================] - 3s 450us/step - loss: 2901.8901 - acc: 0.3040 - val_loss: 946.0802 - val_acc: 0.2934\n",
      "Epoch 245/300\n",
      "7471/7471 [==============================] - 3s 444us/step - loss: 2901.8813 - acc: 0.3040 - val_loss: 946.0995 - val_acc: 0.2934\n",
      "Epoch 246/300\n",
      "7471/7471 [==============================] - 3s 438us/step - loss: 3066.7760 - acc: 0.3040 - val_loss: 946.1022 - val_acc: 0.2934\n",
      "Epoch 247/300\n",
      "7471/7471 [==============================] - 3s 426us/step - loss: 2901.9031 - acc: 0.3040 - val_loss: 946.0862 - val_acc: 0.2934\n",
      "Epoch 248/300\n",
      "7471/7471 [==============================] - 3s 442us/step - loss: 2901.8972 - acc: 0.3040 - val_loss: 946.0830 - val_acc: 0.2934\n",
      "Epoch 249/300\n",
      "7471/7471 [==============================] - 3s 429us/step - loss: 2901.8869 - acc: 0.3040 - val_loss: 946.0960 - val_acc: 0.2934\n",
      "Epoch 250/300\n",
      "7471/7471 [==============================] - 3s 432us/step - loss: 2901.8766 - acc: 0.3040 - val_loss: 946.0916 - val_acc: 0.2934\n",
      "Epoch 251/300\n",
      "7471/7471 [==============================] - 3s 447us/step - loss: 2901.8967 - acc: 0.3040 - val_loss: 946.0867 - val_acc: 0.2934\n",
      "Epoch 252/300\n",
      "7471/7471 [==============================] - 3s 433us/step - loss: 2901.8909 - acc: 0.3040 - val_loss: 946.0720 - val_acc: 0.2934\n",
      "Epoch 253/300\n",
      "7471/7471 [==============================] - 3s 430us/step - loss: 2901.8867 - acc: 0.3040 - val_loss: 946.0762 - val_acc: 0.2934\n",
      "Epoch 254/300\n",
      "7471/7471 [==============================] - 3s 432us/step - loss: 2901.8787 - acc: 0.3040 - val_loss: 946.0870 - val_acc: 0.2934\n",
      "Epoch 255/300\n",
      "7471/7471 [==============================] - 3s 427us/step - loss: 2901.8929 - acc: 0.3040 - val_loss: 946.0809 - val_acc: 0.2934\n",
      "Epoch 256/300\n",
      "7471/7471 [==============================] - 3s 446us/step - loss: 2901.8860 - acc: 0.3040 - val_loss: 946.0944 - val_acc: 0.2934\n",
      "Epoch 257/300\n",
      "7471/7471 [==============================] - 3s 438us/step - loss: 2901.8857 - acc: 0.3040 - val_loss: 946.0859 - val_acc: 0.2934\n",
      "Epoch 258/300\n",
      "7471/7471 [==============================] - 3s 439us/step - loss: 2901.8796 - acc: 0.3040 - val_loss: 946.0882 - val_acc: 0.2934\n",
      "Epoch 259/300\n",
      "7471/7471 [==============================] - 3s 432us/step - loss: 1039484.8647 - acc: 0.3040 - val_loss: 946.2163 - val_acc: 0.2934\n",
      "Epoch 260/300\n",
      "7471/7471 [==============================] - 3s 444us/step - loss: 2901.9881 - acc: 0.3040 - val_loss: 946.2116 - val_acc: 0.2934\n",
      "Epoch 261/300\n",
      "7471/7471 [==============================] - 3s 435us/step - loss: 2901.9724 - acc: 0.3040 - val_loss: 946.1968 - val_acc: 0.2934\n",
      "Epoch 262/300\n",
      "7471/7471 [==============================] - 3s 427us/step - loss: 2901.9572 - acc: 0.3040 - val_loss: 946.1862 - val_acc: 0.2934\n",
      "Epoch 263/300\n",
      "7471/7471 [==============================] - 3s 445us/step - loss: 2901.9461 - acc: 0.3040 - val_loss: 946.1775 - val_acc: 0.2934\n",
      "Epoch 264/300\n",
      "7471/7471 [==============================] - 3s 453us/step - loss: 2901.9350 - acc: 0.3040 - val_loss: 946.1713 - val_acc: 0.2934\n",
      "Epoch 265/300\n",
      "7471/7471 [==============================] - 3s 446us/step - loss: 2901.9269 - acc: 0.3040 - val_loss: 946.1645 - val_acc: 0.2934\n",
      "Epoch 266/300\n",
      "7471/7471 [==============================] - 3s 468us/step - loss: 2901.9197 - acc: 0.3040 - val_loss: 946.1575 - val_acc: 0.2934\n",
      "Epoch 267/300\n",
      "7471/7471 [==============================] - 4s 522us/step - loss: 2901.9163 - acc: 0.3040 - val_loss: 946.1527 - val_acc: 0.2934\n",
      "Epoch 268/300\n",
      "7471/7471 [==============================] - 3s 452us/step - loss: 2901.9188 - acc: 0.3040 - val_loss: 946.1412 - val_acc: 0.2934\n",
      "Epoch 269/300\n",
      "7471/7471 [==============================] - 4s 472us/step - loss: 2901.9066 - acc: 0.3040 - val_loss: 946.1379 - val_acc: 0.2934\n",
      "Epoch 270/300\n",
      "7471/7471 [==============================] - 4s 546us/step - loss: 2901.8994 - acc: 0.3040 - val_loss: 946.1302 - val_acc: 0.2934\n",
      "Epoch 271/300\n",
      "7471/7471 [==============================] - 4s 485us/step - loss: 2901.9027 - acc: 0.3040 - val_loss: 946.1240 - val_acc: 0.2934\n",
      "Epoch 272/300\n",
      "7471/7471 [==============================] - 3s 440us/step - loss: 2901.8931 - acc: 0.3040 - val_loss: 946.1180 - val_acc: 0.2934\n",
      "Epoch 273/300\n",
      "7471/7471 [==============================] - 3s 443us/step - loss: 2901.8923 - acc: 0.3040 - val_loss: 946.0996 - val_acc: 0.2934\n",
      "Epoch 274/300\n",
      "7471/7471 [==============================] - 3s 445us/step - loss: 2901.8845 - acc: 0.3040 - val_loss: 946.1091 - val_acc: 0.2934\n",
      "Epoch 275/300\n",
      "7471/7471 [==============================] - 3s 447us/step - loss: 2901.8879 - acc: 0.3040 - val_loss: 946.1071 - val_acc: 0.2934\n",
      "Epoch 276/300\n",
      "7471/7471 [==============================] - 3s 452us/step - loss: 2901.8849 - acc: 0.3040 - val_loss: 946.0991 - val_acc: 0.2934\n",
      "Epoch 277/300\n",
      "7471/7471 [==============================] - 3s 446us/step - loss: 2901.8819 - acc: 0.3040 - val_loss: 946.0986 - val_acc: 0.2934\n",
      "Epoch 278/300\n",
      "7471/7471 [==============================] - 3s 446us/step - loss: 2901.8803 - acc: 0.3040 - val_loss: 946.0924 - val_acc: 0.2934\n",
      "Epoch 279/300\n",
      "7471/7471 [==============================] - 3s 441us/step - loss: 2901.8816 - acc: 0.3040 - val_loss: 946.0916 - val_acc: 0.2934\n",
      "Epoch 280/300\n",
      "7471/7471 [==============================] - 3s 444us/step - loss: 2901.8856 - acc: 0.3040 - val_loss: 946.0783 - val_acc: 0.2934\n",
      "Epoch 281/300\n",
      "7471/7471 [==============================] - 3s 432us/step - loss: 2901.8790 - acc: 0.3040 - val_loss: 946.0897 - val_acc: 0.2934\n",
      "Epoch 282/300\n",
      "7471/7471 [==============================] - 3s 441us/step - loss: 2901.8786 - acc: 0.3040 - val_loss: 946.0817 - val_acc: 0.2934\n",
      "Epoch 283/300\n",
      "7471/7471 [==============================] - 3s 451us/step - loss: 2901.8814 - acc: 0.3040 - val_loss: 946.0846 - val_acc: 0.2934\n",
      "Epoch 284/300\n",
      "7471/7471 [==============================] - 3s 441us/step - loss: 2901.8807 - acc: 0.3040 - val_loss: 946.0710 - val_acc: 0.2934\n",
      "Epoch 285/300\n",
      "7471/7471 [==============================] - 3s 442us/step - loss: 9176268.2317 - acc: 0.3040 - val_loss: 946.1025 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/300\n",
      "7471/7471 [==============================] - 3s 430us/step - loss: 2901.9682 - acc: 0.3040 - val_loss: 946.1017 - val_acc: 0.2934\n",
      "Epoch 287/300\n",
      "7471/7471 [==============================] - 3s 438us/step - loss: 2901.9945 - acc: 0.3038 - val_loss: 946.1011 - val_acc: 0.2934\n",
      "Epoch 288/300\n",
      "7471/7471 [==============================] - 3s 440us/step - loss: 2901.9678 - acc: 0.3040 - val_loss: 946.1008 - val_acc: 0.2934\n",
      "Epoch 289/300\n",
      "7471/7471 [==============================] - 3s 435us/step - loss: 2901.9620 - acc: 0.3040 - val_loss: 946.1006 - val_acc: 0.2934\n",
      "Epoch 290/300\n",
      "7471/7471 [==============================] - 3s 439us/step - loss: 2901.9581 - acc: 0.3040 - val_loss: 946.0984 - val_acc: 0.2934\n",
      "Epoch 291/300\n",
      "7471/7471 [==============================] - 3s 450us/step - loss: 2901.9550 - acc: 0.3040 - val_loss: 946.0976 - val_acc: 0.2934\n",
      "Epoch 292/300\n",
      "7471/7471 [==============================] - 3s 444us/step - loss: 2901.9786 - acc: 0.3040 - val_loss: 946.0971 - val_acc: 0.2934\n",
      "Epoch 293/300\n",
      "7471/7471 [==============================] - 3s 443us/step - loss: 2901.9474 - acc: 0.3040 - val_loss: 946.0973 - val_acc: 0.2934\n",
      "Epoch 294/300\n",
      "7471/7471 [==============================] - 3s 437us/step - loss: 2901.9422 - acc: 0.3040 - val_loss: 946.0950 - val_acc: 0.2934\n",
      "Epoch 295/300\n",
      "7471/7471 [==============================] - 3s 448us/step - loss: 2901.9396 - acc: 0.3040 - val_loss: 946.0945 - val_acc: 0.2934\n",
      "Epoch 296/300\n",
      "7471/7471 [==============================] - 3s 449us/step - loss: 2901.9337 - acc: 0.3040 - val_loss: 946.0952 - val_acc: 0.2934\n",
      "Epoch 297/300\n",
      "7471/7471 [==============================] - 3s 451us/step - loss: 2901.9344 - acc: 0.3040 - val_loss: 946.0961 - val_acc: 0.2934\n",
      "Epoch 298/300\n",
      "7471/7471 [==============================] - 3s 449us/step - loss: 2901.9256 - acc: 0.3040 - val_loss: 946.0950 - val_acc: 0.2934\n",
      "Epoch 299/300\n",
      "7471/7471 [==============================] - 3s 437us/step - loss: 2901.9239 - acc: 0.3040 - val_loss: 946.0950 - val_acc: 0.2934\n",
      "Epoch 300/300\n",
      "7471/7471 [==============================] - 3s 463us/step - loss: 2901.9198 - acc: 0.3040 - val_loss: 946.0868 - val_acc: 0.2934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd4a336d68>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "config = wandb.config\n",
    "\n",
    "config.batch_size = 128\n",
    "config.epochs = 300\n",
    "config.learning_rate = 0.01\n",
    "config.dropout = 0.8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_x_train.shape[1]))\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(6))\n",
    "adam = Adam(lr=config.learning_rate)\n",
    "model.compile(loss=mean_squared_error, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(input_x_train, input_y_train, \n",
    "          epochs=config.epochs, validation_data=(input_x_test, input_y_test), \n",
    "          callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Blogpost",
   "language": "python",
   "name": "blogpost_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
