{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to prepare dataset\n",
    "from utils import read_and_clean_data, split_X_y\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and clean data\n",
    "dataset = read_and_clean_data(\"../data/ABBREV.xlsx\")\n",
    "\n",
    "# Split dataset into X and y\n",
    "X, y = split_X_y(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scl_X = RobustScaler().fit(X_train)\n",
    "scl_y = RobustScaler().fit(y_train)\n",
    "\n",
    "input_x_train = scl_X.transform(X_train)\n",
    "input_y_train = scl_y.transform(y_train)\n",
    "\n",
    "input_x_test = scl_X.transform(X_test)\n",
    "input_y_test = scl_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies For Training In Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Wandb To Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/gilangrilhami/Predicting-Micronutrients-using-Neural-Networks-and-Random-Forest-notebooks/runs/jpv1o61y\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.init()\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paramaters for training\n",
    "\n",
    "config.batch_size = 128\n",
    "config.epochs = 1000\n",
    "config.learning_rate = 0.001\n",
    "config.dropout = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 02:11:29.940604 140156208547648 deprecation.py:506] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# model.add(Dense(input_x_train.shape[1]))\n",
    "# model.add(Dense(1024))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(config.dropout))\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(config.dropout))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(config.dropout))\n",
    "model.add(Dense(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimization method and cirterion\n",
    "\n",
    "adam = Adam(lr=config.learning_rate)\n",
    "model.compile(loss=mean_squared_error, optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 02:11:32.591274 140156208547648 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0718 02:11:32.646254 140156208547648 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0718 02:11:32.706245 140156208547648 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7471 samples, validate on 1319 samples\n",
      "Epoch 1/1000\n",
      "7471/7471 [==============================] - 1s 166us/sample - loss: 97543247.7569 - acc: 0.1761 - val_loss: 7264.2022 - val_acc: 0.1759\n",
      "Epoch 2/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 518128637340.1506 - acc: 0.1770 - val_loss: 1433.2732 - val_acc: 0.2691\n",
      "Epoch 3/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 218409170702.7297 - acc: 0.1992 - val_loss: 1255.7165 - val_acc: 0.2911\n",
      "Epoch 4/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 145378.0822 - acc: 0.1899 - val_loss: 1343.8781 - val_acc: 0.3086\n",
      "Epoch 5/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 303593.8392 - acc: 0.1961 - val_loss: 1462.2101 - val_acc: 0.3116\n",
      "Epoch 6/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 50422020660.9239 - acc: 0.1966 - val_loss: 1700.8865 - val_acc: 0.3116\n",
      "Epoch 7/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 18021401320.5081 - acc: 0.2008 - val_loss: 1662.1034 - val_acc: 0.2949\n",
      "Epoch 8/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 4003548125.0496 - acc: 0.2018 - val_loss: 2722.4128 - val_acc: 0.3033\n",
      "Epoch 9/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 12191689820.1592 - acc: 0.2105 - val_loss: 2822.8528 - val_acc: 0.2866\n",
      "Epoch 10/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 12929585593.8079 - acc: 0.2124 - val_loss: 2239.7752 - val_acc: 0.2729\n",
      "Epoch 11/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 39312324.4516 - acc: 0.2160 - val_loss: 1749.8476 - val_acc: 0.2798\n",
      "Epoch 12/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 51226128476.1713 - acc: 0.2198 - val_loss: 1625.8687 - val_acc: 0.2835\n",
      "Epoch 13/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 11547356129.3502 - acc: 0.2170 - val_loss: 1261.0178 - val_acc: 0.2828\n",
      "Epoch 14/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 49089250132.1723 - acc: 0.2218 - val_loss: 1291.8782 - val_acc: 0.2798\n",
      "Epoch 15/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2106883154.0812 - acc: 0.2221 - val_loss: 1217.4232 - val_acc: 0.2729\n",
      "Epoch 16/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 989992554.5516 - acc: 0.2250 - val_loss: 1511.7301 - val_acc: 0.2714\n",
      "Epoch 17/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 195663148993.2198 - acc: 0.2300 - val_loss: 1074.4836 - val_acc: 0.2889\n",
      "Epoch 18/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 308782.8613 - acc: 0.2243 - val_loss: 1023.8704 - val_acc: 0.2926\n",
      "Epoch 19/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 41094.9549 - acc: 0.2263 - val_loss: 1011.1509 - val_acc: 0.2851\n",
      "Epoch 20/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 1210483317.3375 - acc: 0.2366 - val_loss: 1008.1292 - val_acc: 0.2934\n",
      "Epoch 21/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 1613323.5132 - acc: 0.2326 - val_loss: 1005.0312 - val_acc: 0.2934\n",
      "Epoch 22/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 906642298.7000 - acc: 0.2361 - val_loss: 1007.8192 - val_acc: 0.2934\n",
      "Epoch 23/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 10797682686.8450 - acc: 0.2395 - val_loss: 1008.3020 - val_acc: 0.2934\n",
      "Epoch 24/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 1065228664.7303 - acc: 0.2304 - val_loss: 1006.3775 - val_acc: 0.2934\n",
      "Epoch 25/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 15667.4394 - acc: 0.2504 - val_loss: 993.4614 - val_acc: 0.2934\n",
      "Epoch 26/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 80065808.5622 - acc: 0.2488 - val_loss: 993.2057 - val_acc: 0.2934\n",
      "Epoch 27/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 617960.3917 - acc: 0.2415 - val_loss: 992.8495 - val_acc: 0.2934\n",
      "Epoch 28/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 4328.1711 - acc: 0.2491 - val_loss: 989.4374 - val_acc: 0.2934\n",
      "Epoch 29/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 43876.2399 - acc: 0.2415 - val_loss: 987.3252 - val_acc: 0.2934\n",
      "Epoch 30/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 348589.6670 - acc: 0.2483 - val_loss: 987.7048 - val_acc: 0.2942\n",
      "Epoch 31/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 18286184993.7537 - acc: 0.2532 - val_loss: 983.3691 - val_acc: 0.2987\n",
      "Epoch 32/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 98515.3244 - acc: 0.2487 - val_loss: 986.1263 - val_acc: 0.2911\n",
      "Epoch 33/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 985745.3060 - acc: 0.2528 - val_loss: 985.5607 - val_acc: 0.2926\n",
      "Epoch 34/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 801424918.3944 - acc: 0.2551 - val_loss: 985.9323 - val_acc: 0.2919\n",
      "Epoch 35/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 1222449.9869 - acc: 0.2621 - val_loss: 988.3166 - val_acc: 0.2904\n",
      "Epoch 36/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 3239424454.8521 - acc: 0.2636 - val_loss: 981.8708 - val_acc: 0.2926\n",
      "Epoch 37/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 1148879817.7923 - acc: 0.2662 - val_loss: 982.1996 - val_acc: 0.2926\n",
      "Epoch 38/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 159460.1959 - acc: 0.2623 - val_loss: 981.8945 - val_acc: 0.2934\n",
      "Epoch 39/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 11868299745.7200 - acc: 0.2685 - val_loss: 983.3510 - val_acc: 0.2911\n",
      "Epoch 40/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 378504343.0985 - acc: 0.2657 - val_loss: 984.1189 - val_acc: 0.2881\n",
      "Epoch 41/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 8097.9489 - acc: 0.2674 - val_loss: 983.8858 - val_acc: 0.2881\n",
      "Epoch 42/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 17559.1168 - acc: 0.2666 - val_loss: 984.0531 - val_acc: 0.2881\n",
      "Epoch 43/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 399994.5859 - acc: 0.2693 - val_loss: 984.4114 - val_acc: 0.2881\n",
      "Epoch 44/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 381450.8394 - acc: 0.2640 - val_loss: 989.2776 - val_acc: 0.2881\n",
      "Epoch 45/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 7924.1183 - acc: 0.2704 - val_loss: 989.2563 - val_acc: 0.2881\n",
      "Epoch 46/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2987852925.4834 - acc: 0.2669 - val_loss: 983.2008 - val_acc: 0.2881\n",
      "Epoch 47/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 8112060655.2813 - acc: 0.2688 - val_loss: 981.1292 - val_acc: 0.2919\n",
      "Epoch 48/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 4307.1445 - acc: 0.2773 - val_loss: 980.6323 - val_acc: 0.2926\n",
      "Epoch 49/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 34597.9907 - acc: 0.2712 - val_loss: 980.5368 - val_acc: 0.2934\n",
      "Epoch 50/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 36747.1416 - acc: 0.2740 - val_loss: 980.5799 - val_acc: 0.2934\n",
      "Epoch 51/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 8722961000.4099 - acc: 0.2741 - val_loss: 980.5811 - val_acc: 0.2934\n",
      "Epoch 52/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 117551.5914 - acc: 0.2767 - val_loss: 980.6613 - val_acc: 0.2934\n",
      "Epoch 53/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 3218702234.9509 - acc: 0.2831 - val_loss: 980.4657 - val_acc: 0.2934\n",
      "Epoch 54/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 26985.8197 - acc: 0.2775 - val_loss: 980.2820 - val_acc: 0.2934\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 88us/sample - loss: 36025.8825 - acc: 0.2812 - val_loss: 980.2042 - val_acc: 0.2934\n",
      "Epoch 56/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 3178.8977 - acc: 0.2832 - val_loss: 980.2448 - val_acc: 0.2934\n",
      "Epoch 57/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2171923207.8982 - acc: 0.2827 - val_loss: 980.4085 - val_acc: 0.2934\n",
      "Epoch 58/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 1068375870.3629 - acc: 0.2851 - val_loss: 980.5160 - val_acc: 0.2934\n",
      "Epoch 59/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 3504.5418 - acc: 0.2827 - val_loss: 979.8272 - val_acc: 0.2934\n",
      "Epoch 60/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 1761712072.9485 - acc: 0.2851 - val_loss: 979.5663 - val_acc: 0.2934\n",
      "Epoch 61/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 253764762.3046 - acc: 0.2844 - val_loss: 979.0049 - val_acc: 0.2919\n",
      "Epoch 62/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 24034240051.8297 - acc: 0.2880 - val_loss: 978.8093 - val_acc: 0.2919\n",
      "Epoch 63/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 74766.3277 - acc: 0.2876 - val_loss: 978.8093 - val_acc: 0.2934\n",
      "Epoch 64/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 13080.0544 - acc: 0.2898 - val_loss: 978.8182 - val_acc: 0.2934\n",
      "Epoch 65/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 3814691073.8589 - acc: 0.2876 - val_loss: 979.0568 - val_acc: 0.2919\n",
      "Epoch 66/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 9840.8839 - acc: 0.2872 - val_loss: 979.0215 - val_acc: 0.2919\n",
      "Epoch 67/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2990.7263 - acc: 0.2905 - val_loss: 978.9951 - val_acc: 0.2919\n",
      "Epoch 68/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 72176.7283 - acc: 0.2889 - val_loss: 978.9976 - val_acc: 0.2919\n",
      "Epoch 69/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 7752507793.5076 - acc: 0.2889 - val_loss: 978.8890 - val_acc: 0.2926\n",
      "Epoch 70/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 190300.9802 - acc: 0.2884 - val_loss: 978.7850 - val_acc: 0.2934\n",
      "Epoch 71/1000\n",
      "7471/7471 [==============================] - 1s 111us/sample - loss: 2287345563.6530 - acc: 0.2949 - val_loss: 978.8565 - val_acc: 0.2919\n",
      "Epoch 72/1000\n",
      "7471/7471 [==============================] - 1s 122us/sample - loss: 9503299713.8736 - acc: 0.2941 - val_loss: 978.7762 - val_acc: 0.2919\n",
      "Epoch 73/1000\n",
      "7471/7471 [==============================] - 1s 117us/sample - loss: 8541.7460 - acc: 0.2961 - val_loss: 978.7717 - val_acc: 0.2919\n",
      "Epoch 74/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 3814.7250 - acc: 0.2963 - val_loss: 978.7357 - val_acc: 0.2919\n",
      "Epoch 75/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 3115.5362 - acc: 0.2962 - val_loss: 978.7125 - val_acc: 0.2919\n",
      "Epoch 76/1000\n",
      "7471/7471 [==============================] - 1s 108us/sample - loss: 3731.0084 - acc: 0.2929 - val_loss: 978.7734 - val_acc: 0.2919\n",
      "Epoch 77/1000\n",
      "7471/7471 [==============================] - 1s 112us/sample - loss: 2969.6840 - acc: 0.2945 - val_loss: 978.7778 - val_acc: 0.2919\n",
      "Epoch 78/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 41521.7763 - acc: 0.2959 - val_loss: 978.8064 - val_acc: 0.2919\n",
      "Epoch 79/1000\n",
      "7471/7471 [==============================] - 0s 45us/sample - loss: 12217.8444 - acc: 0.2958 - val_loss: 978.7858 - val_acc: 0.2919\n",
      "Epoch 80/1000\n",
      "7471/7471 [==============================] - 0s 54us/sample - loss: 3292.8201 - acc: 0.2939 - val_loss: 978.7577 - val_acc: 0.2919\n",
      "Epoch 81/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 3793.6561 - acc: 0.2984 - val_loss: 978.7547 - val_acc: 0.2919\n",
      "Epoch 82/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 3146.3631 - acc: 0.2962 - val_loss: 978.7515 - val_acc: 0.2919\n",
      "Epoch 83/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 11238958489.1455 - acc: 0.2973 - val_loss: 978.8630 - val_acc: 0.2919\n",
      "Epoch 84/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 4608693.2644 - acc: 0.2963 - val_loss: 978.8640 - val_acc: 0.2934\n",
      "Epoch 85/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 333019.1553 - acc: 0.2992 - val_loss: 978.8580 - val_acc: 0.2934\n",
      "Epoch 86/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 3428.3822 - acc: 0.2973 - val_loss: 978.8537 - val_acc: 0.2934\n",
      "Epoch 87/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 3134.9370 - acc: 0.2996 - val_loss: 978.8491 - val_acc: 0.2934\n",
      "Epoch 88/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 631566.3896 - acc: 0.2990 - val_loss: 978.8464 - val_acc: 0.2934\n",
      "Epoch 89/1000\n",
      "7471/7471 [==============================] - 1s 71us/sample - loss: 60416.7645 - acc: 0.2993 - val_loss: 978.8411 - val_acc: 0.2934\n",
      "Epoch 90/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 15063.5113 - acc: 0.3021 - val_loss: 978.8350 - val_acc: 0.2934\n",
      "Epoch 91/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 16145588.7991 - acc: 0.2989 - val_loss: 978.8129 - val_acc: 0.2934\n",
      "Epoch 92/1000\n",
      "7471/7471 [==============================] - 1s 113us/sample - loss: 437675231.4666 - acc: 0.3002 - val_loss: 978.6872 - val_acc: 0.2934\n",
      "Epoch 93/1000\n",
      "7471/7471 [==============================] - 1s 108us/sample - loss: 3271.6194 - acc: 0.2996 - val_loss: 978.6798 - val_acc: 0.2934\n",
      "Epoch 94/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2950.4915 - acc: 0.3000 - val_loss: 978.6736 - val_acc: 0.2934\n",
      "Epoch 95/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 5062.6974 - acc: 0.3016 - val_loss: 978.6668 - val_acc: 0.2934\n",
      "Epoch 96/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 28940.8796 - acc: 0.2988 - val_loss: 978.6600 - val_acc: 0.2934\n",
      "Epoch 97/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2945.1290 - acc: 0.2992 - val_loss: 978.6513 - val_acc: 0.2934\n",
      "Epoch 98/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2977.7854 - acc: 0.3000 - val_loss: 978.6416 - val_acc: 0.2934\n",
      "Epoch 99/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 3347.9627 - acc: 0.3008 - val_loss: 978.6308 - val_acc: 0.2934\n",
      "Epoch 100/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 3533.7775 - acc: 0.2994 - val_loss: 978.6186 - val_acc: 0.2934\n",
      "Epoch 101/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2945.2587 - acc: 0.3008 - val_loss: 978.6050 - val_acc: 0.2934\n",
      "Epoch 102/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2988.1835 - acc: 0.3018 - val_loss: 978.5892 - val_acc: 0.2934\n",
      "Epoch 103/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 47901.3210 - acc: 0.3000 - val_loss: 978.5743 - val_acc: 0.2934\n",
      "Epoch 104/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 8309.0382 - acc: 0.3009 - val_loss: 978.5540 - val_acc: 0.2934\n",
      "Epoch 105/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 7640.0531 - acc: 0.3012 - val_loss: 978.5320 - val_acc: 0.2934\n",
      "Epoch 106/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2946.7860 - acc: 0.3032 - val_loss: 978.5072 - val_acc: 0.2934\n",
      "Epoch 107/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 15652.5229 - acc: 0.3001 - val_loss: 978.4791 - val_acc: 0.2934\n",
      "Epoch 108/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2939.9744 - acc: 0.3034 - val_loss: 978.4481 - val_acc: 0.2934\n",
      "Epoch 109/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 3032.1802 - acc: 0.3021 - val_loss: 978.4129 - val_acc: 0.2934\n",
      "Epoch 110/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2945.1395 - acc: 0.3024 - val_loss: 978.3733 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 10051.8434 - acc: 0.3014 - val_loss: 978.3299 - val_acc: 0.2934\n",
      "Epoch 112/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2945.4161 - acc: 0.3034 - val_loss: 978.2806 - val_acc: 0.2934\n",
      "Epoch 113/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 3060.2609 - acc: 0.3013 - val_loss: 978.2240 - val_acc: 0.2934\n",
      "Epoch 114/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2939.3426 - acc: 0.3024 - val_loss: 978.1621 - val_acc: 0.2934\n",
      "Epoch 115/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2999.8243 - acc: 0.3025 - val_loss: 978.0934 - val_acc: 0.2934\n",
      "Epoch 116/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2935.2438 - acc: 0.3022 - val_loss: 978.0153 - val_acc: 0.2934\n",
      "Epoch 117/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 7225.6712 - acc: 0.3018 - val_loss: 977.9292 - val_acc: 0.2934\n",
      "Epoch 118/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2939.3335 - acc: 0.3024 - val_loss: 977.8330 - val_acc: 0.2934\n",
      "Epoch 119/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2938.4990 - acc: 0.3038 - val_loss: 977.7220 - val_acc: 0.2934\n",
      "Epoch 120/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2938.7854 - acc: 0.3029 - val_loss: 977.6014 - val_acc: 0.2934\n",
      "Epoch 121/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2939.6780 - acc: 0.3024 - val_loss: 977.4636 - val_acc: 0.2934\n",
      "Epoch 122/1000\n",
      "7471/7471 [==============================] - 1s 111us/sample - loss: 2938.6265 - acc: 0.3040 - val_loss: 977.3149 - val_acc: 0.2934\n",
      "Epoch 123/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2956.3740 - acc: 0.3024 - val_loss: 977.1462 - val_acc: 0.2934\n",
      "Epoch 124/1000\n",
      "7471/7471 [==============================] - 1s 82us/sample - loss: 2953.4894 - acc: 0.3029 - val_loss: 976.9589 - val_acc: 0.2934\n",
      "Epoch 125/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2938.2514 - acc: 0.3034 - val_loss: 976.7517 - val_acc: 0.2934\n",
      "Epoch 126/1000\n",
      "7471/7471 [==============================] - 1s 79us/sample - loss: 2939.8482 - acc: 0.3029 - val_loss: 976.5260 - val_acc: 0.2934\n",
      "Epoch 127/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 3210.0908 - acc: 0.3042 - val_loss: 976.2782 - val_acc: 0.2934\n",
      "Epoch 128/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2937.0231 - acc: 0.3037 - val_loss: 976.0040 - val_acc: 0.2934\n",
      "Epoch 129/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2945.2649 - acc: 0.3038 - val_loss: 975.7067 - val_acc: 0.2934\n",
      "Epoch 130/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 8142.3973 - acc: 0.3025 - val_loss: 975.3984 - val_acc: 0.2934\n",
      "Epoch 131/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 3047.1729 - acc: 0.3046 - val_loss: 975.0700 - val_acc: 0.2934\n",
      "Epoch 132/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2935.6810 - acc: 0.3040 - val_loss: 974.7074 - val_acc: 0.2934\n",
      "Epoch 133/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 3368.4031 - acc: 0.3024 - val_loss: 974.3127 - val_acc: 0.2934\n",
      "Epoch 134/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 12462527.8494 - acc: 0.3026 - val_loss: 973.9579 - val_acc: 0.2934\n",
      "Epoch 135/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2937.5358 - acc: 0.3041 - val_loss: 973.9152 - val_acc: 0.2934\n",
      "Epoch 136/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2935.5152 - acc: 0.3037 - val_loss: 973.8676 - val_acc: 0.2934\n",
      "Epoch 137/1000\n",
      "7471/7471 [==============================] - 1s 75us/sample - loss: 2934.4092 - acc: 0.3038 - val_loss: 973.8146 - val_acc: 0.2934\n",
      "Epoch 138/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 8439463011.5495 - acc: 0.3036 - val_loss: 973.8141 - val_acc: 0.2934\n",
      "Epoch 139/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2936.0961 - acc: 0.3037 - val_loss: 973.7598 - val_acc: 0.2934\n",
      "Epoch 140/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2942.6528 - acc: 0.3040 - val_loss: 973.6990 - val_acc: 0.2934\n",
      "Epoch 141/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2934.2370 - acc: 0.3044 - val_loss: 973.6302 - val_acc: 0.2934\n",
      "Epoch 142/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2936.8475 - acc: 0.3036 - val_loss: 973.5549 - val_acc: 0.2934\n",
      "Epoch 143/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2963.4403 - acc: 0.3041 - val_loss: 973.4690 - val_acc: 0.2934\n",
      "Epoch 144/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2934.2588 - acc: 0.3042 - val_loss: 973.3726 - val_acc: 0.2934\n",
      "Epoch 145/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2933.6643 - acc: 0.3038 - val_loss: 973.2663 - val_acc: 0.2934\n",
      "Epoch 146/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 3833.1132 - acc: 0.3036 - val_loss: 973.1501 - val_acc: 0.2934\n",
      "Epoch 147/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2935.7543 - acc: 0.3041 - val_loss: 972.9074 - val_acc: 0.2934\n",
      "Epoch 148/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 5870962.9489 - acc: 0.3041 - val_loss: 972.6219 - val_acc: 0.2934\n",
      "Epoch 149/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2934.4629 - acc: 0.3037 - val_loss: 972.5253 - val_acc: 0.2934\n",
      "Epoch 150/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2933.3122 - acc: 0.3040 - val_loss: 972.4383 - val_acc: 0.2934\n",
      "Epoch 151/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 3025.2467 - acc: 0.3041 - val_loss: 972.3556 - val_acc: 0.2934\n",
      "Epoch 152/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2933.5701 - acc: 0.3050 - val_loss: 972.2647 - val_acc: 0.2934\n",
      "Epoch 153/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2932.6040 - acc: 0.3037 - val_loss: 972.1599 - val_acc: 0.2934\n",
      "Epoch 154/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2933.1160 - acc: 0.3044 - val_loss: 972.0533 - val_acc: 0.2934\n",
      "Epoch 155/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2932.1789 - acc: 0.3040 - val_loss: 971.9332 - val_acc: 0.2934\n",
      "Epoch 156/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2931.9397 - acc: 0.3038 - val_loss: 971.7918 - val_acc: 0.2934\n",
      "Epoch 157/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2933.3346 - acc: 0.3037 - val_loss: 971.6666 - val_acc: 0.2934\n",
      "Epoch 158/1000\n",
      "7471/7471 [==============================] - 1s 70us/sample - loss: 3277.8485 - acc: 0.3044 - val_loss: 971.5180 - val_acc: 0.2934\n",
      "Epoch 159/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2944.7123 - acc: 0.3038 - val_loss: 971.3619 - val_acc: 0.2934\n",
      "Epoch 160/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2931.5638 - acc: 0.3041 - val_loss: 971.1832 - val_acc: 0.2934\n",
      "Epoch 161/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2932.5292 - acc: 0.3042 - val_loss: 970.9758 - val_acc: 0.2934\n",
      "Epoch 162/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2931.0379 - acc: 0.3038 - val_loss: 970.7762 - val_acc: 0.2934\n",
      "Epoch 163/1000\n",
      "7471/7471 [==============================] - 0s 65us/sample - loss: 175075666.2306 - acc: 0.3040 - val_loss: 970.6842 - val_acc: 0.2934\n",
      "Epoch 164/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2931.0983 - acc: 0.3040 - val_loss: 970.6360 - val_acc: 0.2934\n",
      "Epoch 165/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2930.5799 - acc: 0.3041 - val_loss: 970.5781 - val_acc: 0.2934\n",
      "Epoch 166/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2930.6227 - acc: 0.3040 - val_loss: 970.5105 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/1000\n",
      "7471/7471 [==============================] - 1s 69us/sample - loss: 2932.2496 - acc: 0.3042 - val_loss: 970.4366 - val_acc: 0.2934\n",
      "Epoch 168/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 1143554.5590 - acc: 0.3040 - val_loss: 970.3411 - val_acc: 0.2934\n",
      "Epoch 169/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2930.2022 - acc: 0.3036 - val_loss: 970.2390 - val_acc: 0.2934\n",
      "Epoch 170/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2929.7784 - acc: 0.3036 - val_loss: 970.1147 - val_acc: 0.2934\n",
      "Epoch 171/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2930.0551 - acc: 0.3040 - val_loss: 969.9874 - val_acc: 0.2934\n",
      "Epoch 172/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2930.0145 - acc: 0.3042 - val_loss: 969.8332 - val_acc: 0.2934\n",
      "Epoch 173/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2929.7016 - acc: 0.3040 - val_loss: 969.6554 - val_acc: 0.2934\n",
      "Epoch 174/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2933.6478 - acc: 0.3038 - val_loss: 969.3527 - val_acc: 0.2934\n",
      "Epoch 175/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2930.3847 - acc: 0.3037 - val_loss: 969.0787 - val_acc: 0.2934\n",
      "Epoch 176/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2930.6751 - acc: 0.3030 - val_loss: 968.8321 - val_acc: 0.2934\n",
      "Epoch 177/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2951.6223 - acc: 0.3033 - val_loss: 968.5165 - val_acc: 0.2934\n",
      "Epoch 178/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2936.5343 - acc: 0.3040 - val_loss: 968.2524 - val_acc: 0.2934\n",
      "Epoch 179/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2927.9144 - acc: 0.3040 - val_loss: 967.9892 - val_acc: 0.2934\n",
      "Epoch 180/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2927.5815 - acc: 0.3036 - val_loss: 967.6757 - val_acc: 0.2934\n",
      "Epoch 181/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2927.9780 - acc: 0.3037 - val_loss: 967.3164 - val_acc: 0.2934\n",
      "Epoch 182/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 3020.9695 - acc: 0.3036 - val_loss: 967.0100 - val_acc: 0.2934\n",
      "Epoch 183/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2932.9094 - acc: 0.3037 - val_loss: 966.6494 - val_acc: 0.2934\n",
      "Epoch 184/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2926.7067 - acc: 0.3040 - val_loss: 966.2572 - val_acc: 0.2934\n",
      "Epoch 185/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2925.9349 - acc: 0.3032 - val_loss: 965.7881 - val_acc: 0.2934\n",
      "Epoch 186/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2925.8290 - acc: 0.3037 - val_loss: 965.2490 - val_acc: 0.2934\n",
      "Epoch 187/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2927.6255 - acc: 0.3040 - val_loss: 964.7523 - val_acc: 0.2934\n",
      "Epoch 188/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2925.6808 - acc: 0.3038 - val_loss: 964.2491 - val_acc: 0.2934\n",
      "Epoch 189/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2924.0813 - acc: 0.3049 - val_loss: 963.6254 - val_acc: 0.2934\n",
      "Epoch 190/1000\n",
      "7471/7471 [==============================] - 1s 114us/sample - loss: 2943.2193 - acc: 0.3037 - val_loss: 963.2663 - val_acc: 0.2934\n",
      "Epoch 191/1000\n",
      "7471/7471 [==============================] - 1s 123us/sample - loss: 2923.5874 - acc: 0.3030 - val_loss: 962.8529 - val_acc: 0.2934\n",
      "Epoch 192/1000\n",
      "7471/7471 [==============================] - 1s 122us/sample - loss: 2913.0330 - acc: 0.3038 - val_loss: 962.7033 - val_acc: 0.2934\n",
      "Epoch 193/1000\n",
      "7471/7471 [==============================] - 1s 119us/sample - loss: 2922.2290 - acc: 0.3036 - val_loss: 962.1899 - val_acc: 0.2934\n",
      "Epoch 194/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 5077281.6398 - acc: 0.3038 - val_loss: 961.5889 - val_acc: 0.2934\n",
      "Epoch 195/1000\n",
      "7471/7471 [==============================] - 1s 111us/sample - loss: 2922.7585 - acc: 0.3041 - val_loss: 960.8837 - val_acc: 0.2934\n",
      "Epoch 196/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 3276.3062 - acc: 0.3037 - val_loss: 960.8125 - val_acc: 0.2934\n",
      "Epoch 197/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2926.1902 - acc: 0.3030 - val_loss: 960.1851 - val_acc: 0.2934\n",
      "Epoch 198/1000\n",
      "7471/7471 [==============================] - 1s 117us/sample - loss: 2960.2469 - acc: 0.3041 - val_loss: 960.3032 - val_acc: 0.2934\n",
      "Epoch 199/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 3006.1861 - acc: 0.3037 - val_loss: 959.7227 - val_acc: 0.2934\n",
      "Epoch 200/1000\n",
      "7471/7471 [==============================] - 1s 121us/sample - loss: 2951.5539 - acc: 0.3037 - val_loss: 959.5956 - val_acc: 0.2934\n",
      "Epoch 201/1000\n",
      "7471/7471 [==============================] - 1s 120us/sample - loss: 2921.5294 - acc: 0.3032 - val_loss: 958.9558 - val_acc: 0.2934\n",
      "Epoch 202/1000\n",
      "7471/7471 [==============================] - 1s 115us/sample - loss: 2918.2537 - acc: 0.3041 - val_loss: 958.5024 - val_acc: 0.2934\n",
      "Epoch 203/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2751570.8959 - acc: 0.3037 - val_loss: 958.6043 - val_acc: 0.2934\n",
      "Epoch 204/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2920.3417 - acc: 0.3040 - val_loss: 958.6236 - val_acc: 0.2934\n",
      "Epoch 205/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2920.8129 - acc: 0.3040 - val_loss: 958.2942 - val_acc: 0.2934\n",
      "Epoch 206/1000\n",
      "7471/7471 [==============================] - 1s 113us/sample - loss: 2920.8047 - acc: 0.3033 - val_loss: 958.0616 - val_acc: 0.2934\n",
      "Epoch 207/1000\n",
      "7471/7471 [==============================] - 1s 129us/sample - loss: 2925.4248 - acc: 0.3038 - val_loss: 957.7790 - val_acc: 0.2934\n",
      "Epoch 208/1000\n",
      "7471/7471 [==============================] - 1s 117us/sample - loss: 2919.6129 - acc: 0.3036 - val_loss: 957.4190 - val_acc: 0.2934\n",
      "Epoch 209/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 2918.6846 - acc: 0.3033 - val_loss: 957.1237 - val_acc: 0.2934\n",
      "Epoch 210/1000\n",
      "7471/7471 [==============================] - 1s 114us/sample - loss: 2354.3604 - acc: 0.3042 - val_loss: 956.3899 - val_acc: 0.2934\n",
      "Epoch 211/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2919.6788 - acc: 0.3040 - val_loss: 956.4950 - val_acc: 0.2934\n",
      "Epoch 212/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 2928.8365 - acc: 0.3038 - val_loss: 956.2932 - val_acc: 0.2934\n",
      "Epoch 213/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 2919.1563 - acc: 0.3042 - val_loss: 956.1536 - val_acc: 0.2934\n",
      "Epoch 214/1000\n",
      "7471/7471 [==============================] - 1s 108us/sample - loss: 4563.2149 - acc: 0.3044 - val_loss: 956.2503 - val_acc: 0.2934\n",
      "Epoch 215/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2951.3845 - acc: 0.3032 - val_loss: 956.2094 - val_acc: 0.2934\n",
      "Epoch 216/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2913.4990 - acc: 0.3034 - val_loss: 955.9424 - val_acc: 0.2934\n",
      "Epoch 217/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2918.9762 - acc: 0.3044 - val_loss: 955.9681 - val_acc: 0.2934\n",
      "Epoch 218/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2933.1643 - acc: 0.3038 - val_loss: 956.0080 - val_acc: 0.2934\n",
      "Epoch 219/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2919.5149 - acc: 0.3030 - val_loss: 955.9688 - val_acc: 0.2934\n",
      "Epoch 220/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2916.7849 - acc: 0.3034 - val_loss: 955.0989 - val_acc: 0.2934\n",
      "Epoch 221/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2916.5606 - acc: 0.3044 - val_loss: 955.3200 - val_acc: 0.2934\n",
      "Epoch 222/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2913.5483 - acc: 0.3038 - val_loss: 954.8189 - val_acc: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2970.4865 - acc: 0.3038 - val_loss: 954.4545 - val_acc: 0.2934\n",
      "Epoch 224/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2910.8973 - acc: 0.3036 - val_loss: 953.6300 - val_acc: 0.2934\n",
      "Epoch 225/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2913.4609 - acc: 0.3036 - val_loss: 952.0402 - val_acc: 0.2934\n",
      "Epoch 226/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2912.9187 - acc: 0.3040 - val_loss: 952.3162 - val_acc: 0.2934\n",
      "Epoch 227/1000\n",
      "7471/7471 [==============================] - 1s 111us/sample - loss: 2910.3274 - acc: 0.3036 - val_loss: 951.6027 - val_acc: 0.2934\n",
      "Epoch 228/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2910.9889 - acc: 0.3037 - val_loss: 951.5534 - val_acc: 0.2934\n",
      "Epoch 229/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 3058.3090 - acc: 0.3037 - val_loss: 951.8850 - val_acc: 0.2934\n",
      "Epoch 230/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2913.3886 - acc: 0.3034 - val_loss: 951.1668 - val_acc: 0.2934\n",
      "Epoch 231/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2910.7904 - acc: 0.3041 - val_loss: 950.5139 - val_acc: 0.2934\n",
      "Epoch 232/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2911.3975 - acc: 0.3041 - val_loss: 950.2508 - val_acc: 0.2934\n",
      "Epoch 233/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2929.0294 - acc: 0.3029 - val_loss: 949.8992 - val_acc: 0.2934\n",
      "Epoch 234/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2908.0459 - acc: 0.3041 - val_loss: 949.5442 - val_acc: 0.2934\n",
      "Epoch 235/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 8474.0708 - acc: 0.3034 - val_loss: 950.0520 - val_acc: 0.2934\n",
      "Epoch 236/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2907.9689 - acc: 0.3034 - val_loss: 949.3806 - val_acc: 0.2934\n",
      "Epoch 237/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2908.3793 - acc: 0.3038 - val_loss: 948.7483 - val_acc: 0.2934\n",
      "Epoch 238/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 3001.8168 - acc: 0.3040 - val_loss: 948.8818 - val_acc: 0.2934\n",
      "Epoch 239/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2913.7607 - acc: 0.3040 - val_loss: 948.9019 - val_acc: 0.2934\n",
      "Epoch 240/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2908.1109 - acc: 0.3034 - val_loss: 948.3917 - val_acc: 0.2934\n",
      "Epoch 241/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 192682.3424 - acc: 0.3040 - val_loss: 949.6620 - val_acc: 0.2934\n",
      "Epoch 242/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2915.1324 - acc: 0.3036 - val_loss: 949.2433 - val_acc: 0.2934\n",
      "Epoch 243/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2911.7743 - acc: 0.3036 - val_loss: 948.2598 - val_acc: 0.2934\n",
      "Epoch 244/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2932.9873 - acc: 0.3041 - val_loss: 948.1813 - val_acc: 0.2934\n",
      "Epoch 245/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2911.7586 - acc: 0.3041 - val_loss: 948.0772 - val_acc: 0.2934\n",
      "Epoch 246/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2908.4329 - acc: 0.3034 - val_loss: 947.6042 - val_acc: 0.2934\n",
      "Epoch 247/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2913.2929 - acc: 0.3038 - val_loss: 947.0745 - val_acc: 0.2934\n",
      "Epoch 248/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2908.7317 - acc: 0.3033 - val_loss: 946.6848 - val_acc: 0.2934\n",
      "Epoch 249/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2908.5810 - acc: 0.3036 - val_loss: 947.1162 - val_acc: 0.2934\n",
      "Epoch 250/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2905.7564 - acc: 0.3038 - val_loss: 946.8234 - val_acc: 0.2934\n",
      "Epoch 251/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2904.8108 - acc: 0.3030 - val_loss: 945.9227 - val_acc: 0.2934\n",
      "Epoch 252/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2903.1778 - acc: 0.3025 - val_loss: 945.2987 - val_acc: 0.2934\n",
      "Epoch 253/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2907.4308 - acc: 0.3034 - val_loss: 945.0301 - val_acc: 0.2934\n",
      "Epoch 254/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2933.2500 - acc: 0.3029 - val_loss: 945.1870 - val_acc: 0.2934\n",
      "Epoch 255/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2900.6265 - acc: 0.3038 - val_loss: 944.7128 - val_acc: 0.2934\n",
      "Epoch 256/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2909.0974 - acc: 0.3037 - val_loss: 944.1526 - val_acc: 0.2934\n",
      "Epoch 257/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2892.9978 - acc: 0.3029 - val_loss: 943.7882 - val_acc: 0.2934\n",
      "Epoch 258/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 56230625.8819 - acc: 0.3033 - val_loss: 943.3900 - val_acc: 0.2934\n",
      "Epoch 259/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2910.2137 - acc: 0.3037 - val_loss: 942.2051 - val_acc: 0.2934\n",
      "Epoch 260/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 3361.0000 - acc: 0.3034 - val_loss: 941.8926 - val_acc: 0.2934\n",
      "Epoch 261/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2890.0728 - acc: 0.3034 - val_loss: 939.2922 - val_acc: 0.2934\n",
      "Epoch 262/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2899.3866 - acc: 0.3030 - val_loss: 939.2751 - val_acc: 0.2934\n",
      "Epoch 263/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2895.4821 - acc: 0.3034 - val_loss: 938.8211 - val_acc: 0.2934\n",
      "Epoch 264/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2911.0064 - acc: 0.3034 - val_loss: 938.6340 - val_acc: 0.2934\n",
      "Epoch 265/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2894.1699 - acc: 0.3041 - val_loss: 936.9527 - val_acc: 0.2934\n",
      "Epoch 266/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2873.0942 - acc: 0.3034 - val_loss: 934.8364 - val_acc: 0.2934\n",
      "Epoch 267/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2885.8869 - acc: 0.3034 - val_loss: 935.6572 - val_acc: 0.2934\n",
      "Epoch 268/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2898.0202 - acc: 0.3034 - val_loss: 935.4188 - val_acc: 0.2934\n",
      "Epoch 269/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2785.7438 - acc: 0.3034 - val_loss: 935.2609 - val_acc: 0.2934\n",
      "Epoch 270/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2894.4571 - acc: 0.3029 - val_loss: 935.1488 - val_acc: 0.2934\n",
      "Epoch 271/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 3070.4862 - acc: 0.3041 - val_loss: 934.0873 - val_acc: 0.2934\n",
      "Epoch 272/1000\n",
      "7471/7471 [==============================] - 1s 109us/sample - loss: 2889.0219 - acc: 0.3032 - val_loss: 934.4732 - val_acc: 0.2934\n",
      "Epoch 273/1000\n",
      "7471/7471 [==============================] - 1s 75us/sample - loss: 2907.4600 - acc: 0.3040 - val_loss: 934.8656 - val_acc: 0.2934\n",
      "Epoch 274/1000\n",
      "7471/7471 [==============================] - 0s 61us/sample - loss: 2904.2230 - acc: 0.3036 - val_loss: 936.7364 - val_acc: 0.2934\n",
      "Epoch 275/1000\n",
      "7471/7471 [==============================] - 1s 80us/sample - loss: 2952.2755 - acc: 0.3030 - val_loss: 937.3785 - val_acc: 0.2934\n",
      "Epoch 276/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2890.5149 - acc: 0.3034 - val_loss: 935.9960 - val_acc: 0.2934\n",
      "Epoch 277/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2901.2107 - acc: 0.3034 - val_loss: 935.7093 - val_acc: 0.2934\n",
      "Epoch 278/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2896.6223 - acc: 0.3036 - val_loss: 935.6588 - val_acc: 0.2934\n",
      "Epoch 279/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2911.9981 - acc: 0.3037 - val_loss: 934.4949 - val_acc: 0.2934\n",
      "Epoch 280/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 2891.9298 - acc: 0.3030 - val_loss: 932.2798 - val_acc: 0.2934\n",
      "Epoch 281/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2918.9447 - acc: 0.3033 - val_loss: 932.8842 - val_acc: 0.2934\n",
      "Epoch 282/1000\n",
      "7471/7471 [==============================] - 1s 113us/sample - loss: 2898.1883 - acc: 0.3037 - val_loss: 930.8165 - val_acc: 0.2934\n",
      "Epoch 283/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 3814835544.3333 - acc: 0.3030 - val_loss: 932.4321 - val_acc: 0.2934\n",
      "Epoch 284/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2935.1453 - acc: 0.3030 - val_loss: 930.9848 - val_acc: 0.2934\n",
      "Epoch 285/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2891.1530 - acc: 0.3036 - val_loss: 931.0497 - val_acc: 0.2934\n",
      "Epoch 286/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2896.2974 - acc: 0.3030 - val_loss: 932.0360 - val_acc: 0.2934\n",
      "Epoch 287/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 3411.9827 - acc: 0.3036 - val_loss: 933.8523 - val_acc: 0.2934\n",
      "Epoch 288/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2893.5991 - acc: 0.3036 - val_loss: 933.4197 - val_acc: 0.2934\n",
      "Epoch 289/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2896.8361 - acc: 0.3034 - val_loss: 934.1036 - val_acc: 0.2934\n",
      "Epoch 290/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2895.1719 - acc: 0.3040 - val_loss: 932.6427 - val_acc: 0.2934\n",
      "Epoch 291/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2896.5158 - acc: 0.3038 - val_loss: 932.2026 - val_acc: 0.2934\n",
      "Epoch 292/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2848.4984 - acc: 0.3037 - val_loss: 932.8809 - val_acc: 0.2934\n",
      "Epoch 293/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 10661290.6807 - acc: 0.3033 - val_loss: 933.7389 - val_acc: 0.2934\n",
      "Epoch 294/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 5393.1538 - acc: 0.3036 - val_loss: 933.3238 - val_acc: 0.2934\n",
      "Epoch 295/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2882.3876 - acc: 0.3040 - val_loss: 931.8937 - val_acc: 0.2934\n",
      "Epoch 296/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 2881.4760 - acc: 0.3036 - val_loss: 929.0121 - val_acc: 0.2934\n",
      "Epoch 297/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2764.9135 - acc: 0.3028 - val_loss: 926.6739 - val_acc: 0.2934\n",
      "Epoch 298/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 3049.7599 - acc: 0.3038 - val_loss: 928.0706 - val_acc: 0.2934\n",
      "Epoch 299/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2884.5181 - acc: 0.3032 - val_loss: 928.7147 - val_acc: 0.2934\n",
      "Epoch 300/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2930.3247 - acc: 0.3040 - val_loss: 929.7892 - val_acc: 0.2934\n",
      "Epoch 301/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2898.1601 - acc: 0.3037 - val_loss: 928.3211 - val_acc: 0.2934\n",
      "Epoch 302/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2887.9261 - acc: 0.3034 - val_loss: 927.9176 - val_acc: 0.2934\n",
      "Epoch 303/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2864.2340 - acc: 0.3036 - val_loss: 925.0043 - val_acc: 0.2934\n",
      "Epoch 304/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2915.5237 - acc: 0.3034 - val_loss: 924.0031 - val_acc: 0.2934\n",
      "Epoch 305/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 3017.5155 - acc: 0.3036 - val_loss: 920.7750 - val_acc: 0.2934\n",
      "Epoch 306/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2894.4302 - acc: 0.3034 - val_loss: 921.0948 - val_acc: 0.2934\n",
      "Epoch 307/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2873.3468 - acc: 0.3034 - val_loss: 916.6236 - val_acc: 0.2934\n",
      "Epoch 308/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2851.0836 - acc: 0.3038 - val_loss: 914.0277 - val_acc: 0.2934\n",
      "Epoch 309/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2884.7941 - acc: 0.3032 - val_loss: 923.3215 - val_acc: 0.2934\n",
      "Epoch 310/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2876.4237 - acc: 0.3033 - val_loss: 923.4901 - val_acc: 0.2934\n",
      "Epoch 311/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2873.2296 - acc: 0.3034 - val_loss: 921.4403 - val_acc: 0.2934\n",
      "Epoch 312/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2883.7831 - acc: 0.3034 - val_loss: 918.2762 - val_acc: 0.2934\n",
      "Epoch 313/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2878.1717 - acc: 0.3037 - val_loss: 920.3777 - val_acc: 0.2934\n",
      "Epoch 314/1000\n",
      "7471/7471 [==============================] - 1s 110us/sample - loss: 2911.9675 - acc: 0.3036 - val_loss: 925.2449 - val_acc: 0.2934\n",
      "Epoch 315/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2896.1971 - acc: 0.3032 - val_loss: 925.9890 - val_acc: 0.2934\n",
      "Epoch 316/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2875.2285 - acc: 0.3032 - val_loss: 924.9769 - val_acc: 0.2934\n",
      "Epoch 317/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2868.9566 - acc: 0.3032 - val_loss: 922.6347 - val_acc: 0.2934\n",
      "Epoch 318/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 6742.6209 - acc: 0.3038 - val_loss: 921.4032 - val_acc: 0.2934\n",
      "Epoch 319/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 55187115.8371 - acc: 0.3033 - val_loss: 922.9987 - val_acc: 0.2934\n",
      "Epoch 320/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 2867.3783 - acc: 0.3036 - val_loss: 923.4923 - val_acc: 0.2934\n",
      "Epoch 321/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2870.2751 - acc: 0.3037 - val_loss: 923.2051 - val_acc: 0.2934\n",
      "Epoch 322/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2871.9294 - acc: 0.3036 - val_loss: 921.4620 - val_acc: 0.2934\n",
      "Epoch 323/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2864.6320 - acc: 0.3034 - val_loss: 916.8223 - val_acc: 0.2934\n",
      "Epoch 324/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2861.0972 - acc: 0.3034 - val_loss: 917.8510 - val_acc: 0.2934\n",
      "Epoch 325/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2857.8041 - acc: 0.3032 - val_loss: 915.7977 - val_acc: 0.2934\n",
      "Epoch 326/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2882.1243 - acc: 0.3034 - val_loss: 914.4029 - val_acc: 0.2934\n",
      "Epoch 327/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 4612.3063 - acc: 0.3040 - val_loss: 915.8348 - val_acc: 0.2934\n",
      "Epoch 328/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2881.3671 - acc: 0.3037 - val_loss: 919.7258 - val_acc: 0.2934\n",
      "Epoch 329/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2867.3068 - acc: 0.3034 - val_loss: 918.4277 - val_acc: 0.2934\n",
      "Epoch 330/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2900.2926 - acc: 0.3041 - val_loss: 923.3835 - val_acc: 0.2934\n",
      "Epoch 331/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2871.3064 - acc: 0.3036 - val_loss: 922.5341 - val_acc: 0.2934\n",
      "Epoch 332/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2867.2196 - acc: 0.3041 - val_loss: 920.7621 - val_acc: 0.2934\n",
      "Epoch 333/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2874.6776 - acc: 0.3037 - val_loss: 918.7339 - val_acc: 0.2934\n",
      "Epoch 334/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2863.0458 - acc: 0.3038 - val_loss: 917.3279 - val_acc: 0.2934\n",
      "Epoch 335/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2869.4754 - acc: 0.3040 - val_loss: 915.6521 - val_acc: 0.2934\n",
      "Epoch 336/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2892.2245 - acc: 0.3037 - val_loss: 915.4248 - val_acc: 0.2934\n",
      "Epoch 337/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2934.6162 - acc: 0.3037 - val_loss: 915.5108 - val_acc: 0.2934\n",
      "Epoch 338/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2851.6605 - acc: 0.3032 - val_loss: 917.1477 - val_acc: 0.2934\n",
      "Epoch 339/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2843.7218 - acc: 0.3034 - val_loss: 914.1356 - val_acc: 0.2934\n",
      "Epoch 340/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2847.3486 - acc: 0.3036 - val_loss: 908.6867 - val_acc: 0.2934\n",
      "Epoch 341/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2804.7236 - acc: 0.3036 - val_loss: 911.4273 - val_acc: 0.2934\n",
      "Epoch 342/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2858.5196 - acc: 0.3034 - val_loss: 909.0688 - val_acc: 0.2934\n",
      "Epoch 343/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2861.9311 - acc: 0.3041 - val_loss: 904.1258 - val_acc: 0.2934\n",
      "Epoch 344/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2847.7780 - acc: 0.3033 - val_loss: 902.1359 - val_acc: 0.2934\n",
      "Epoch 345/1000\n",
      "7471/7471 [==============================] - 1s 110us/sample - loss: 2855.6418 - acc: 0.3036 - val_loss: 902.8196 - val_acc: 0.2934\n",
      "Epoch 346/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2868.1288 - acc: 0.3033 - val_loss: 898.5709 - val_acc: 0.2934\n",
      "Epoch 347/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2881.0424 - acc: 0.3038 - val_loss: 903.4323 - val_acc: 0.2934\n",
      "Epoch 348/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2855.8433 - acc: 0.3037 - val_loss: 904.7596 - val_acc: 0.2934\n",
      "Epoch 349/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2862.5528 - acc: 0.3040 - val_loss: 906.7871 - val_acc: 0.2934\n",
      "Epoch 350/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2771.9844 - acc: 0.3038 - val_loss: 902.3572 - val_acc: 0.2934\n",
      "Epoch 351/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2831.4041 - acc: 0.3040 - val_loss: 904.8506 - val_acc: 0.2934\n",
      "Epoch 352/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2926.9764 - acc: 0.3037 - val_loss: 914.9071 - val_acc: 0.2934\n",
      "Epoch 353/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2841.4583 - acc: 0.3036 - val_loss: 909.4627 - val_acc: 0.2934\n",
      "Epoch 354/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2846.5012 - acc: 0.3037 - val_loss: 907.4841 - val_acc: 0.2934\n",
      "Epoch 355/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2857.3202 - acc: 0.3036 - val_loss: 906.3003 - val_acc: 0.2934\n",
      "Epoch 356/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2840.4696 - acc: 0.3038 - val_loss: 901.7282 - val_acc: 0.2934\n",
      "Epoch 357/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2818.5969 - acc: 0.3036 - val_loss: 890.7306 - val_acc: 0.2934\n",
      "Epoch 358/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2692.0615 - acc: 0.3037 - val_loss: 893.3135 - val_acc: 0.2934\n",
      "Epoch 359/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 79363.7577 - acc: 0.3032 - val_loss: 901.7790 - val_acc: 0.2934\n",
      "Epoch 360/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2846.7816 - acc: 0.3037 - val_loss: 899.2608 - val_acc: 0.2934\n",
      "Epoch 361/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2832.5648 - acc: 0.3032 - val_loss: 901.5239 - val_acc: 0.2934\n",
      "Epoch 362/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2891.8222 - acc: 0.3037 - val_loss: 913.2404 - val_acc: 0.2934\n",
      "Epoch 363/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2842.2682 - acc: 0.3036 - val_loss: 909.7431 - val_acc: 0.2934\n",
      "Epoch 364/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2847.9482 - acc: 0.3040 - val_loss: 907.3252 - val_acc: 0.2934\n",
      "Epoch 365/1000\n",
      "7471/7471 [==============================] - 1s 111us/sample - loss: 2849.2992 - acc: 0.3033 - val_loss: 902.9008 - val_acc: 0.2934\n",
      "Epoch 366/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2862.4633 - acc: 0.3036 - val_loss: 900.5171 - val_acc: 0.2934\n",
      "Epoch 367/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2861.1655 - acc: 0.3034 - val_loss: 898.8570 - val_acc: 0.2934\n",
      "Epoch 368/1000\n",
      "7471/7471 [==============================] - 1s 114us/sample - loss: 2895.4839 - acc: 0.3036 - val_loss: 904.0632 - val_acc: 0.2934\n",
      "Epoch 369/1000\n",
      "7471/7471 [==============================] - 1s 118us/sample - loss: 2839.3016 - acc: 0.3032 - val_loss: 898.9269 - val_acc: 0.2934\n",
      "Epoch 370/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2817.1611 - acc: 0.3032 - val_loss: 894.7758 - val_acc: 0.2934\n",
      "Epoch 371/1000\n",
      "7471/7471 [==============================] - 1s 109us/sample - loss: 2849.8255 - acc: 0.3026 - val_loss: 897.8242 - val_acc: 0.2934\n",
      "Epoch 372/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2846.8584 - acc: 0.3037 - val_loss: 893.6696 - val_acc: 0.2934\n",
      "Epoch 373/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2845.8825 - acc: 0.3037 - val_loss: 901.1969 - val_acc: 0.2934\n",
      "Epoch 374/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2823.1459 - acc: 0.3040 - val_loss: 892.0602 - val_acc: 0.2934\n",
      "Epoch 375/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2888.5903 - acc: 0.3034 - val_loss: 896.9871 - val_acc: 0.2934\n",
      "Epoch 376/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2901.0830 - acc: 0.3038 - val_loss: 894.9239 - val_acc: 0.2934\n",
      "Epoch 377/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2859.5464 - acc: 0.3032 - val_loss: 901.0124 - val_acc: 0.2934\n",
      "Epoch 378/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2849.0814 - acc: 0.3038 - val_loss: 894.6603 - val_acc: 0.2934\n",
      "Epoch 379/1000\n",
      "7471/7471 [==============================] - 1s 110us/sample - loss: 3922.0929 - acc: 0.3037 - val_loss: 889.1692 - val_acc: 0.2934\n",
      "Epoch 380/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2829.6820 - acc: 0.3037 - val_loss: 892.2384 - val_acc: 0.2934\n",
      "Epoch 381/1000\n",
      "7471/7471 [==============================] - 1s 113us/sample - loss: 2877.4576 - acc: 0.3041 - val_loss: 888.5215 - val_acc: 0.2934\n",
      "Epoch 382/1000\n",
      "7471/7471 [==============================] - 1s 116us/sample - loss: 2818.3403 - acc: 0.3040 - val_loss: 882.8546 - val_acc: 0.2934\n",
      "Epoch 383/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2860.7550 - acc: 0.3036 - val_loss: 897.3504 - val_acc: 0.2934\n",
      "Epoch 384/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2807.3991 - acc: 0.3040 - val_loss: 885.7463 - val_acc: 0.2934\n",
      "Epoch 385/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2909.9138 - acc: 0.3040 - val_loss: 882.5665 - val_acc: 0.2934\n",
      "Epoch 386/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2813.6935 - acc: 0.3034 - val_loss: 882.7946 - val_acc: 0.2934\n",
      "Epoch 387/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2815.7485 - acc: 0.3040 - val_loss: 883.3076 - val_acc: 0.2934\n",
      "Epoch 388/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2815.6258 - acc: 0.3040 - val_loss: 880.4444 - val_acc: 0.2934\n",
      "Epoch 389/1000\n",
      "7471/7471 [==============================] - 1s 112us/sample - loss: 2843.9327 - acc: 0.3038 - val_loss: 872.9498 - val_acc: 0.2934\n",
      "Epoch 390/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2810.1987 - acc: 0.3037 - val_loss: 877.0527 - val_acc: 0.2934\n",
      "Epoch 391/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 81us/sample - loss: 2842.8144 - acc: 0.3040 - val_loss: 876.8936 - val_acc: 0.2934\n",
      "Epoch 392/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2867.8220 - acc: 0.3040 - val_loss: 884.5746 - val_acc: 0.2934\n",
      "Epoch 393/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2820.5330 - acc: 0.3040 - val_loss: 879.0230 - val_acc: 0.2934\n",
      "Epoch 394/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2831.8643 - acc: 0.3038 - val_loss: 885.2817 - val_acc: 0.2934\n",
      "Epoch 395/1000\n",
      "7471/7471 [==============================] - 1s 110us/sample - loss: 2872.7303 - acc: 0.3040 - val_loss: 885.9426 - val_acc: 0.2934\n",
      "Epoch 396/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2853.4764 - acc: 0.3038 - val_loss: 887.6224 - val_acc: 0.2934\n",
      "Epoch 397/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2754.0919 - acc: 0.3040 - val_loss: 890.6507 - val_acc: 0.2934\n",
      "Epoch 398/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2807.4512 - acc: 0.3040 - val_loss: 883.6419 - val_acc: 0.2934\n",
      "Epoch 399/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2834.7602 - acc: 0.3040 - val_loss: 889.6766 - val_acc: 0.2934\n",
      "Epoch 400/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2808.8270 - acc: 0.3041 - val_loss: 887.3200 - val_acc: 0.2934\n",
      "Epoch 401/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2803.4818 - acc: 0.3040 - val_loss: 884.0263 - val_acc: 0.2934\n",
      "Epoch 402/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2896.3703 - acc: 0.3040 - val_loss: 889.5964 - val_acc: 0.2934\n",
      "Epoch 403/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2807.8678 - acc: 0.3040 - val_loss: 876.3197 - val_acc: 0.2934\n",
      "Epoch 404/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 30663.9102 - acc: 0.3042 - val_loss: 892.0746 - val_acc: 0.2934\n",
      "Epoch 405/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2887.7433 - acc: 0.3040 - val_loss: 895.6469 - val_acc: 0.2934\n",
      "Epoch 406/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2808.6901 - acc: 0.3040 - val_loss: 888.6679 - val_acc: 0.2934\n",
      "Epoch 407/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2799.1225 - acc: 0.3040 - val_loss: 883.3845 - val_acc: 0.2934\n",
      "Epoch 408/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2845.6379 - acc: 0.3040 - val_loss: 876.0883 - val_acc: 0.2934\n",
      "Epoch 409/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2782.7791 - acc: 0.3040 - val_loss: 858.2371 - val_acc: 0.2934\n",
      "Epoch 410/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2809.3602 - acc: 0.3040 - val_loss: 870.7954 - val_acc: 0.2934\n",
      "Epoch 411/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2819.9191 - acc: 0.3040 - val_loss: 862.7527 - val_acc: 0.2934\n",
      "Epoch 412/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2774.3107 - acc: 0.3040 - val_loss: 847.9302 - val_acc: 0.2934\n",
      "Epoch 413/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2658.9651 - acc: 0.3040 - val_loss: 837.7229 - val_acc: 0.2934\n",
      "Epoch 414/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2821.0227 - acc: 0.3040 - val_loss: 851.3516 - val_acc: 0.2934\n",
      "Epoch 415/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2778.1930 - acc: 0.3040 - val_loss: 854.5151 - val_acc: 0.2934\n",
      "Epoch 416/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2811.8742 - acc: 0.3040 - val_loss: 854.9168 - val_acc: 0.2934\n",
      "Epoch 417/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2970.7348 - acc: 0.3040 - val_loss: 863.5261 - val_acc: 0.2934\n",
      "Epoch 418/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 3444.3698 - acc: 0.3040 - val_loss: 862.2346 - val_acc: 0.2934\n",
      "Epoch 419/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2790.2415 - acc: 0.3040 - val_loss: 859.1229 - val_acc: 0.2934\n",
      "Epoch 420/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2853.4829 - acc: 0.3040 - val_loss: 865.1696 - val_acc: 0.2934\n",
      "Epoch 421/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2859.5759 - acc: 0.3041 - val_loss: 868.0643 - val_acc: 0.2934\n",
      "Epoch 422/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2813.9997 - acc: 0.3038 - val_loss: 864.8514 - val_acc: 0.2934\n",
      "Epoch 423/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2794.8879 - acc: 0.3040 - val_loss: 855.9168 - val_acc: 0.2934\n",
      "Epoch 424/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2766.8943 - acc: 0.3040 - val_loss: 852.5240 - val_acc: 0.2934\n",
      "Epoch 425/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2773.6846 - acc: 0.3040 - val_loss: 854.0111 - val_acc: 0.2934\n",
      "Epoch 426/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2782.4106 - acc: 0.3038 - val_loss: 854.4460 - val_acc: 0.2934\n",
      "Epoch 427/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2794.9744 - acc: 0.3041 - val_loss: 855.5277 - val_acc: 0.2934\n",
      "Epoch 428/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2808.2934 - acc: 0.3040 - val_loss: 846.7266 - val_acc: 0.2934\n",
      "Epoch 429/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2805.2594 - acc: 0.3040 - val_loss: 860.8423 - val_acc: 0.2934\n",
      "Epoch 430/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2809.7850 - acc: 0.3040 - val_loss: 861.9108 - val_acc: 0.2934\n",
      "Epoch 431/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2782.8925 - acc: 0.3040 - val_loss: 855.8079 - val_acc: 0.2934\n",
      "Epoch 432/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2789.2864 - acc: 0.3040 - val_loss: 861.2388 - val_acc: 0.2934\n",
      "Epoch 433/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2843.2999 - acc: 0.3040 - val_loss: 849.9075 - val_acc: 0.2934\n",
      "Epoch 434/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2401.2205 - acc: 0.3040 - val_loss: 850.7689 - val_acc: 0.2934\n",
      "Epoch 435/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2809.1087 - acc: 0.3040 - val_loss: 859.4791 - val_acc: 0.2934\n",
      "Epoch 436/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2763.5657 - acc: 0.3038 - val_loss: 849.8406 - val_acc: 0.2934\n",
      "Epoch 437/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2803.0075 - acc: 0.3040 - val_loss: 856.1377 - val_acc: 0.2934\n",
      "Epoch 438/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2809.2987 - acc: 0.3040 - val_loss: 858.7684 - val_acc: 0.2934\n",
      "Epoch 439/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 3007.4925 - acc: 0.3038 - val_loss: 853.6350 - val_acc: 0.2934\n",
      "Epoch 440/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2765.0506 - acc: 0.3038 - val_loss: 850.7313 - val_acc: 0.2934\n",
      "Epoch 441/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2782.4926 - acc: 0.3040 - val_loss: 851.2844 - val_acc: 0.2934\n",
      "Epoch 442/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2740.0592 - acc: 0.3041 - val_loss: 835.9102 - val_acc: 0.2934\n",
      "Epoch 443/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2789.9609 - acc: 0.3040 - val_loss: 833.0499 - val_acc: 0.2934\n",
      "Epoch 444/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2765.7571 - acc: 0.3038 - val_loss: 841.4241 - val_acc: 0.2934\n",
      "Epoch 445/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2794.4919 - acc: 0.3040 - val_loss: 840.7075 - val_acc: 0.2934\n",
      "Epoch 446/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2808.5271 - acc: 0.3040 - val_loss: 856.3240 - val_acc: 0.2934\n",
      "Epoch 447/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2793.2907 - acc: 0.3040 - val_loss: 846.0162 - val_acc: 0.2934\n",
      "Epoch 448/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2794.0264 - acc: 0.3038 - val_loss: 850.2815 - val_acc: 0.2934\n",
      "Epoch 449/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2764.0125 - acc: 0.3040 - val_loss: 840.4473 - val_acc: 0.2934\n",
      "Epoch 450/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2830.9309 - acc: 0.3040 - val_loss: 835.6903 - val_acc: 0.2934\n",
      "Epoch 451/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2771.0303 - acc: 0.3040 - val_loss: 835.5343 - val_acc: 0.2934\n",
      "Epoch 452/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2851.2593 - acc: 0.3040 - val_loss: 864.0899 - val_acc: 0.2934\n",
      "Epoch 453/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2778.8697 - acc: 0.3040 - val_loss: 856.1217 - val_acc: 0.2934\n",
      "Epoch 454/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2778.0178 - acc: 0.3038 - val_loss: 847.7518 - val_acc: 0.2934\n",
      "Epoch 455/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2796.1691 - acc: 0.3038 - val_loss: 849.4426 - val_acc: 0.2934\n",
      "Epoch 456/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2791.8043 - acc: 0.3030 - val_loss: 842.3474 - val_acc: 0.2934\n",
      "Epoch 457/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2784.2006 - acc: 0.3041 - val_loss: 843.3235 - val_acc: 0.2934\n",
      "Epoch 458/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2775.4272 - acc: 0.3038 - val_loss: 832.1730 - val_acc: 0.2934\n",
      "Epoch 459/1000\n",
      "7471/7471 [==============================] - 0s 58us/sample - loss: 2753.7844 - acc: 0.3036 - val_loss: 838.6816 - val_acc: 0.2934\n",
      "Epoch 460/1000\n",
      "7471/7471 [==============================] - 0s 50us/sample - loss: 2766.6663 - acc: 0.3040 - val_loss: 823.7554 - val_acc: 0.2934\n",
      "Epoch 461/1000\n",
      "7471/7471 [==============================] - 1s 71us/sample - loss: 2862.0687 - acc: 0.3041 - val_loss: 842.4707 - val_acc: 0.2934\n",
      "Epoch 462/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2793.6864 - acc: 0.3038 - val_loss: 840.9307 - val_acc: 0.2934\n",
      "Epoch 463/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2777.9960 - acc: 0.3040 - val_loss: 836.2464 - val_acc: 0.2934\n",
      "Epoch 464/1000\n",
      "7471/7471 [==============================] - 1s 107us/sample - loss: 2746.1626 - acc: 0.3040 - val_loss: 820.4739 - val_acc: 0.2934\n",
      "Epoch 465/1000\n",
      "7471/7471 [==============================] - 1s 81us/sample - loss: 2816.8606 - acc: 0.3038 - val_loss: 830.5636 - val_acc: 0.2934\n",
      "Epoch 466/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2843.9903 - acc: 0.3040 - val_loss: 840.4189 - val_acc: 0.2934\n",
      "Epoch 467/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2788.5508 - acc: 0.3040 - val_loss: 842.2108 - val_acc: 0.2934\n",
      "Epoch 468/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2784.6049 - acc: 0.3040 - val_loss: 856.7464 - val_acc: 0.2934\n",
      "Epoch 469/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2779.9096 - acc: 0.3040 - val_loss: 848.5194 - val_acc: 0.2934\n",
      "Epoch 470/1000\n",
      "7471/7471 [==============================] - 1s 71us/sample - loss: 2752.5399 - acc: 0.3040 - val_loss: 839.5982 - val_acc: 0.2934\n",
      "Epoch 471/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2758.4723 - acc: 0.3040 - val_loss: 842.6146 - val_acc: 0.2934\n",
      "Epoch 472/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2816.1722 - acc: 0.3040 - val_loss: 835.4605 - val_acc: 0.2934\n",
      "Epoch 473/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2526.2627 - acc: 0.3040 - val_loss: 835.7334 - val_acc: 0.2934\n",
      "Epoch 474/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2782.3975 - acc: 0.3040 - val_loss: 828.5041 - val_acc: 0.2934\n",
      "Epoch 475/1000\n",
      "7471/7471 [==============================] - 0s 56us/sample - loss: 2740.8662 - acc: 0.3040 - val_loss: 834.2682 - val_acc: 0.2934\n",
      "Epoch 476/1000\n",
      "7471/7471 [==============================] - 1s 69us/sample - loss: 2736.8415 - acc: 0.3040 - val_loss: 830.1336 - val_acc: 0.2934\n",
      "Epoch 477/1000\n",
      "7471/7471 [==============================] - 0s 50us/sample - loss: 2769.9028 - acc: 0.3041 - val_loss: 844.4947 - val_acc: 0.2934\n",
      "Epoch 478/1000\n",
      "7471/7471 [==============================] - 0s 49us/sample - loss: 2732.9714 - acc: 0.3041 - val_loss: 833.5432 - val_acc: 0.2934\n",
      "Epoch 479/1000\n",
      "7471/7471 [==============================] - 0s 57us/sample - loss: 2765.7858 - acc: 0.3040 - val_loss: 836.7129 - val_acc: 0.2934\n",
      "Epoch 480/1000\n",
      "7471/7471 [==============================] - 0s 54us/sample - loss: 2754.8419 - acc: 0.3038 - val_loss: 832.3957 - val_acc: 0.2934\n",
      "Epoch 481/1000\n",
      "7471/7471 [==============================] - 1s 77us/sample - loss: 1548.8799 - acc: 0.3040 - val_loss: 835.0944 - val_acc: 0.2934\n",
      "Epoch 482/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2762.4224 - acc: 0.3038 - val_loss: 847.1942 - val_acc: 0.2934\n",
      "Epoch 483/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2776.7852 - acc: 0.3040 - val_loss: 840.4710 - val_acc: 0.2934\n",
      "Epoch 484/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2769.2170 - acc: 0.3040 - val_loss: 826.5635 - val_acc: 0.2934\n",
      "Epoch 485/1000\n",
      "7471/7471 [==============================] - 1s 114us/sample - loss: 2747.3973 - acc: 0.3038 - val_loss: 825.8603 - val_acc: 0.2934\n",
      "Epoch 486/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2802.7202 - acc: 0.3040 - val_loss: 827.3086 - val_acc: 0.2934\n",
      "Epoch 487/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2776.4765 - acc: 0.3040 - val_loss: 838.6086 - val_acc: 0.2934\n",
      "Epoch 488/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2812.0918 - acc: 0.3041 - val_loss: 843.8200 - val_acc: 0.2934\n",
      "Epoch 489/1000\n",
      "7471/7471 [==============================] - 1s 115us/sample - loss: 2739.5082 - acc: 0.3040 - val_loss: 841.8782 - val_acc: 0.2934\n",
      "Epoch 490/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2795.6913 - acc: 0.3040 - val_loss: 834.8594 - val_acc: 0.2934\n",
      "Epoch 491/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2770.6332 - acc: 0.3040 - val_loss: 852.5488 - val_acc: 0.2934\n",
      "Epoch 492/1000\n",
      "7471/7471 [==============================] - 1s 113us/sample - loss: 2740.4817 - acc: 0.3041 - val_loss: 834.3887 - val_acc: 0.2934\n",
      "Epoch 493/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2752.9001 - acc: 0.3038 - val_loss: 828.2293 - val_acc: 0.2934\n",
      "Epoch 494/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2725.0846 - acc: 0.3038 - val_loss: 840.2412 - val_acc: 0.2934\n",
      "Epoch 495/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2726.5062 - acc: 0.3041 - val_loss: 824.7618 - val_acc: 0.2934\n",
      "Epoch 496/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2849.2194 - acc: 0.3038 - val_loss: 846.8413 - val_acc: 0.2934\n",
      "Epoch 497/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2782.0564 - acc: 0.3040 - val_loss: 841.0038 - val_acc: 0.2934\n",
      "Epoch 498/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2713.0416 - acc: 0.3040 - val_loss: 829.2085 - val_acc: 0.2934\n",
      "Epoch 499/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2732.3339 - acc: 0.3040 - val_loss: 825.0766 - val_acc: 0.2934\n",
      "Epoch 500/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2832.3900 - acc: 0.3040 - val_loss: 831.4025 - val_acc: 0.2934\n",
      "Epoch 501/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2726.2123 - acc: 0.3038 - val_loss: 819.7593 - val_acc: 0.2934\n",
      "Epoch 502/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2764.8553 - acc: 0.3040 - val_loss: 830.6637 - val_acc: 0.2934\n",
      "Epoch 503/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2723.9975 - acc: 0.3040 - val_loss: 819.9371 - val_acc: 0.2934\n",
      "Epoch 504/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2774.9500 - acc: 0.3038 - val_loss: 832.7432 - val_acc: 0.2934\n",
      "Epoch 505/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 26192.4377 - acc: 0.3040 - val_loss: 838.8896 - val_acc: 0.2934\n",
      "Epoch 506/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2737.2661 - acc: 0.3040 - val_loss: 826.7172 - val_acc: 0.2934\n",
      "Epoch 507/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2728.8981 - acc: 0.3040 - val_loss: 828.9010 - val_acc: 0.2934\n",
      "Epoch 508/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2773.1434 - acc: 0.3040 - val_loss: 825.7590 - val_acc: 0.2934\n",
      "Epoch 509/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2769.2032 - acc: 0.3040 - val_loss: 840.5576 - val_acc: 0.2934\n",
      "Epoch 510/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2719.2259 - acc: 0.3040 - val_loss: 825.6259 - val_acc: 0.2934\n",
      "Epoch 511/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2758.2665 - acc: 0.3040 - val_loss: 817.6542 - val_acc: 0.2934\n",
      "Epoch 512/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2737.8696 - acc: 0.3040 - val_loss: 825.3063 - val_acc: 0.2934\n",
      "Epoch 513/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2753.9499 - acc: 0.3038 - val_loss: 823.1609 - val_acc: 0.2934\n",
      "Epoch 514/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2850.1088 - acc: 0.3040 - val_loss: 846.0421 - val_acc: 0.2934\n",
      "Epoch 515/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2777.6558 - acc: 0.3040 - val_loss: 837.2466 - val_acc: 0.2934\n",
      "Epoch 516/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2720.6159 - acc: 0.3041 - val_loss: 847.9704 - val_acc: 0.2934\n",
      "Epoch 517/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2785.7554 - acc: 0.3040 - val_loss: 861.3385 - val_acc: 0.2934\n",
      "Epoch 518/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2761.4094 - acc: 0.3040 - val_loss: 844.3484 - val_acc: 0.2934\n",
      "Epoch 519/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2818.7767 - acc: 0.3040 - val_loss: 838.3157 - val_acc: 0.2934\n",
      "Epoch 520/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2780.1360 - acc: 0.3040 - val_loss: 833.6313 - val_acc: 0.2934\n",
      "Epoch 521/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2739.8994 - acc: 0.3040 - val_loss: 827.9097 - val_acc: 0.2934\n",
      "Epoch 522/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2762.8815 - acc: 0.3040 - val_loss: 845.2614 - val_acc: 0.2934\n",
      "Epoch 523/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2770.3094 - acc: 0.3040 - val_loss: 851.5951 - val_acc: 0.2934\n",
      "Epoch 524/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2772.3319 - acc: 0.3038 - val_loss: 845.0894 - val_acc: 0.2934\n",
      "Epoch 525/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2724.9597 - acc: 0.3040 - val_loss: 831.5002 - val_acc: 0.2934\n",
      "Epoch 526/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2730.3189 - acc: 0.3040 - val_loss: 818.2437 - val_acc: 0.2934\n",
      "Epoch 527/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2777.0069 - acc: 0.3041 - val_loss: 813.2452 - val_acc: 0.2934\n",
      "Epoch 528/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2800.3574 - acc: 0.3040 - val_loss: 817.9361 - val_acc: 0.2934\n",
      "Epoch 529/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2764.5015 - acc: 0.3040 - val_loss: 836.3229 - val_acc: 0.2934\n",
      "Epoch 530/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2709.7205 - acc: 0.3040 - val_loss: 828.5813 - val_acc: 0.2934\n",
      "Epoch 531/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2738.7496 - acc: 0.3040 - val_loss: 822.1926 - val_acc: 0.2934\n",
      "Epoch 532/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2730.2363 - acc: 0.3040 - val_loss: 823.5156 - val_acc: 0.2934\n",
      "Epoch 533/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2723.8559 - acc: 0.3040 - val_loss: 816.6382 - val_acc: 0.2934\n",
      "Epoch 534/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2770.4852 - acc: 0.3040 - val_loss: 828.1661 - val_acc: 0.2934\n",
      "Epoch 535/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2746.9260 - acc: 0.3040 - val_loss: 835.0168 - val_acc: 0.2934\n",
      "Epoch 536/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2713.9023 - acc: 0.3040 - val_loss: 832.0190 - val_acc: 0.2934\n",
      "Epoch 537/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2738.5954 - acc: 0.3040 - val_loss: 828.6317 - val_acc: 0.2934\n",
      "Epoch 538/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2740.9543 - acc: 0.3040 - val_loss: 834.0019 - val_acc: 0.2934\n",
      "Epoch 539/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2725.1866 - acc: 0.3040 - val_loss: 825.0841 - val_acc: 0.2934\n",
      "Epoch 540/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2722.7549 - acc: 0.3040 - val_loss: 811.1844 - val_acc: 0.2934\n",
      "Epoch 541/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2733.8593 - acc: 0.3041 - val_loss: 815.7609 - val_acc: 0.2934\n",
      "Epoch 542/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2741.4538 - acc: 0.3040 - val_loss: 836.7241 - val_acc: 0.2934\n",
      "Epoch 543/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2718.3901 - acc: 0.3040 - val_loss: 828.1303 - val_acc: 0.2934\n",
      "Epoch 544/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2726.4996 - acc: 0.3040 - val_loss: 826.7642 - val_acc: 0.2934\n",
      "Epoch 545/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2735.9341 - acc: 0.3040 - val_loss: 822.5807 - val_acc: 0.2934\n",
      "Epoch 546/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2690.7915 - acc: 0.3040 - val_loss: 814.6312 - val_acc: 0.2934\n",
      "Epoch 547/1000\n",
      "7471/7471 [==============================] - 1s 111us/sample - loss: 2702.3995 - acc: 0.3040 - val_loss: 806.3160 - val_acc: 0.2934\n",
      "Epoch 548/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2775.5065 - acc: 0.3040 - val_loss: 820.7272 - val_acc: 0.2934\n",
      "Epoch 549/1000\n",
      "7471/7471 [==============================] - 1s 104us/sample - loss: 2814.2046 - acc: 0.3040 - val_loss: 840.5405 - val_acc: 0.2934\n",
      "Epoch 550/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2749.5218 - acc: 0.3040 - val_loss: 832.9279 - val_acc: 0.2934\n",
      "Epoch 551/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2711.1141 - acc: 0.3040 - val_loss: 839.8591 - val_acc: 0.2934\n",
      "Epoch 552/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2700.1917 - acc: 0.3040 - val_loss: 827.3964 - val_acc: 0.2934\n",
      "Epoch 553/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2707.3379 - acc: 0.3040 - val_loss: 828.1013 - val_acc: 0.2934\n",
      "Epoch 554/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2698.4286 - acc: 0.3040 - val_loss: 832.8170 - val_acc: 0.2934\n",
      "Epoch 555/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2747.8284 - acc: 0.3040 - val_loss: 830.5593 - val_acc: 0.2934\n",
      "Epoch 556/1000\n",
      "7471/7471 [==============================] - 1s 71us/sample - loss: 2770.4417 - acc: 0.3040 - val_loss: 824.5108 - val_acc: 0.2934\n",
      "Epoch 557/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2741.4715 - acc: 0.3040 - val_loss: 836.7130 - val_acc: 0.2934\n",
      "Epoch 558/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2691.9664 - acc: 0.3040 - val_loss: 832.9680 - val_acc: 0.2934\n",
      "Epoch 559/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2750.5238 - acc: 0.3040 - val_loss: 834.7556 - val_acc: 0.2934\n",
      "Epoch 560/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2697.9268 - acc: 0.3040 - val_loss: 823.7177 - val_acc: 0.2934\n",
      "Epoch 561/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2752.3219 - acc: 0.3040 - val_loss: 829.8840 - val_acc: 0.2934\n",
      "Epoch 562/1000\n",
      "7471/7471 [==============================] - 1s 82us/sample - loss: 2741.0595 - acc: 0.3040 - val_loss: 828.8329 - val_acc: 0.2934\n",
      "Epoch 563/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2718.6971 - acc: 0.3038 - val_loss: 823.6176 - val_acc: 0.2934\n",
      "Epoch 564/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2724.3603 - acc: 0.3040 - val_loss: 820.5676 - val_acc: 0.2934\n",
      "Epoch 565/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2737.8927 - acc: 0.3037 - val_loss: 810.4429 - val_acc: 0.2934\n",
      "Epoch 566/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2689.2047 - acc: 0.3038 - val_loss: 805.7912 - val_acc: 0.2934\n",
      "Epoch 567/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2729.4709 - acc: 0.3040 - val_loss: 805.0266 - val_acc: 0.2934\n",
      "Epoch 568/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2711.8562 - acc: 0.3038 - val_loss: 810.4768 - val_acc: 0.2934\n",
      "Epoch 569/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2711.0548 - acc: 0.3040 - val_loss: 821.1624 - val_acc: 0.2934\n",
      "Epoch 570/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2708.0469 - acc: 0.3040 - val_loss: 803.9870 - val_acc: 0.2934\n",
      "Epoch 571/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2787.0188 - acc: 0.3040 - val_loss: 813.7127 - val_acc: 0.2934\n",
      "Epoch 572/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2700.4520 - acc: 0.3040 - val_loss: 810.4995 - val_acc: 0.2934\n",
      "Epoch 573/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2807.7736 - acc: 0.3040 - val_loss: 825.6985 - val_acc: 0.2934\n",
      "Epoch 574/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2815.9913 - acc: 0.3040 - val_loss: 837.1865 - val_acc: 0.2934\n",
      "Epoch 575/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2693.8567 - acc: 0.3040 - val_loss: 826.8412 - val_acc: 0.2934\n",
      "Epoch 576/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2722.3831 - acc: 0.3040 - val_loss: 816.1776 - val_acc: 0.2934\n",
      "Epoch 577/1000\n",
      "7471/7471 [==============================] - 1s 105us/sample - loss: 2708.9493 - acc: 0.3040 - val_loss: 809.3765 - val_acc: 0.2934\n",
      "Epoch 578/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2675.2455 - acc: 0.3040 - val_loss: 813.0258 - val_acc: 0.2934\n",
      "Epoch 579/1000\n",
      "7471/7471 [==============================] - 1s 82us/sample - loss: 2666.0412 - acc: 0.3040 - val_loss: 811.7663 - val_acc: 0.2934\n",
      "Epoch 580/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2703.1312 - acc: 0.3040 - val_loss: 830.8979 - val_acc: 0.2934\n",
      "Epoch 581/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2724.9794 - acc: 0.3040 - val_loss: 830.0467 - val_acc: 0.2934\n",
      "Epoch 582/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2739.9729 - acc: 0.3040 - val_loss: 828.2123 - val_acc: 0.2934\n",
      "Epoch 583/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2785.6190 - acc: 0.3040 - val_loss: 832.9324 - val_acc: 0.2934\n",
      "Epoch 584/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2704.4229 - acc: 0.3040 - val_loss: 825.5939 - val_acc: 0.2934\n",
      "Epoch 585/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2697.7074 - acc: 0.3040 - val_loss: 814.9174 - val_acc: 0.2934\n",
      "Epoch 586/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 3600.1109 - acc: 0.3040 - val_loss: 830.9505 - val_acc: 0.2934\n",
      "Epoch 587/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2681.4469 - acc: 0.3040 - val_loss: 835.5462 - val_acc: 0.2934\n",
      "Epoch 588/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2827.0042 - acc: 0.3040 - val_loss: 836.7048 - val_acc: 0.2934\n",
      "Epoch 589/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2707.8327 - acc: 0.3040 - val_loss: 823.2429 - val_acc: 0.2934\n",
      "Epoch 590/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2721.0810 - acc: 0.3041 - val_loss: 821.3757 - val_acc: 0.2934\n",
      "Epoch 591/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2742.6935 - acc: 0.3040 - val_loss: 830.2217 - val_acc: 0.2934\n",
      "Epoch 592/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2805.0556 - acc: 0.3040 - val_loss: 851.1003 - val_acc: 0.2934\n",
      "Epoch 593/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2689.4035 - acc: 0.3040 - val_loss: 846.3674 - val_acc: 0.2934\n",
      "Epoch 594/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2765.8435 - acc: 0.3040 - val_loss: 833.7869 - val_acc: 0.2934\n",
      "Epoch 595/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2737.0549 - acc: 0.3040 - val_loss: 837.1712 - val_acc: 0.2934\n",
      "Epoch 596/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2697.7429 - acc: 0.3040 - val_loss: 845.7600 - val_acc: 0.2934\n",
      "Epoch 597/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2706.1759 - acc: 0.3040 - val_loss: 838.9522 - val_acc: 0.2934\n",
      "Epoch 598/1000\n",
      "7471/7471 [==============================] - 0s 55us/sample - loss: 2728.4967 - acc: 0.3040 - val_loss: 834.2458 - val_acc: 0.2934\n",
      "Epoch 599/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2752.1257 - acc: 0.3038 - val_loss: 830.5773 - val_acc: 0.2934\n",
      "Epoch 600/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2703.0494 - acc: 0.3040 - val_loss: 842.7753 - val_acc: 0.2934\n",
      "Epoch 601/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2739.3727 - acc: 0.3040 - val_loss: 837.6543 - val_acc: 0.2934\n",
      "Epoch 602/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2707.9020 - acc: 0.3040 - val_loss: 838.4643 - val_acc: 0.2934\n",
      "Epoch 603/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2674.8299 - acc: 0.3040 - val_loss: 845.2677 - val_acc: 0.2934\n",
      "Epoch 604/1000\n",
      "7471/7471 [==============================] - 1s 108us/sample - loss: 2706.9301 - acc: 0.3040 - val_loss: 849.8011 - val_acc: 0.2934\n",
      "Epoch 605/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2752.7384 - acc: 0.3040 - val_loss: 857.8582 - val_acc: 0.2934\n",
      "Epoch 606/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2703.7602 - acc: 0.3040 - val_loss: 847.6725 - val_acc: 0.2934\n",
      "Epoch 607/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2707.9605 - acc: 0.3040 - val_loss: 831.4849 - val_acc: 0.2934\n",
      "Epoch 608/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2686.2895 - acc: 0.3040 - val_loss: 825.3439 - val_acc: 0.2934\n",
      "Epoch 609/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2707.2948 - acc: 0.3041 - val_loss: 816.0220 - val_acc: 0.2934\n",
      "Epoch 610/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2713.4257 - acc: 0.3040 - val_loss: 832.0246 - val_acc: 0.2934\n",
      "Epoch 611/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2649.9718 - acc: 0.3040 - val_loss: 820.1497 - val_acc: 0.2934\n",
      "Epoch 612/1000\n",
      "7471/7471 [==============================] - 1s 82us/sample - loss: 2694.7757 - acc: 0.3040 - val_loss: 830.7986 - val_acc: 0.2934\n",
      "Epoch 613/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2697.6577 - acc: 0.3040 - val_loss: 822.6908 - val_acc: 0.2934\n",
      "Epoch 614/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2738.8832 - acc: 0.3040 - val_loss: 840.7549 - val_acc: 0.2934\n",
      "Epoch 615/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2699.7747 - acc: 0.3040 - val_loss: 836.3311 - val_acc: 0.2934\n",
      "Epoch 616/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2723.6055 - acc: 0.3040 - val_loss: 837.5258 - val_acc: 0.2934\n",
      "Epoch 617/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2781.5358 - acc: 0.3040 - val_loss: 855.0301 - val_acc: 0.2934\n",
      "Epoch 618/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2721.4845 - acc: 0.3040 - val_loss: 839.9947 - val_acc: 0.2934\n",
      "Epoch 619/1000\n",
      "7471/7471 [==============================] - 1s 115us/sample - loss: 2679.8019 - acc: 0.3038 - val_loss: 833.5854 - val_acc: 0.2934\n",
      "Epoch 620/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 3036.8881 - acc: 0.3040 - val_loss: 842.8180 - val_acc: 0.2934\n",
      "Epoch 621/1000\n",
      "7471/7471 [==============================] - 1s 106us/sample - loss: 2727.5287 - acc: 0.3040 - val_loss: 858.6254 - val_acc: 0.2934\n",
      "Epoch 622/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2773.2848 - acc: 0.3040 - val_loss: 855.1594 - val_acc: 0.2934\n",
      "Epoch 623/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2698.9341 - acc: 0.3040 - val_loss: 848.2044 - val_acc: 0.2934\n",
      "Epoch 624/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2736.2086 - acc: 0.3040 - val_loss: 844.8638 - val_acc: 0.2934\n",
      "Epoch 625/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2752.1554 - acc: 0.3040 - val_loss: 846.9306 - val_acc: 0.2934\n",
      "Epoch 626/1000\n",
      "7471/7471 [==============================] - 1s 82us/sample - loss: 2791.4927 - acc: 0.3040 - val_loss: 833.6259 - val_acc: 0.2934\n",
      "Epoch 627/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2696.4104 - acc: 0.3040 - val_loss: 836.4899 - val_acc: 0.2934\n",
      "Epoch 628/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2744.0285 - acc: 0.3040 - val_loss: 843.4279 - val_acc: 0.2934\n",
      "Epoch 629/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2775.3962 - acc: 0.3040 - val_loss: 847.0255 - val_acc: 0.2934\n",
      "Epoch 630/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2663.5858 - acc: 0.3040 - val_loss: 843.6334 - val_acc: 0.2934\n",
      "Epoch 631/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2738.4444 - acc: 0.3040 - val_loss: 842.8297 - val_acc: 0.2934\n",
      "Epoch 632/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2716.4097 - acc: 0.3040 - val_loss: 846.7229 - val_acc: 0.2934\n",
      "Epoch 633/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2684.3943 - acc: 0.3040 - val_loss: 833.7844 - val_acc: 0.2934\n",
      "Epoch 634/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2642.3118 - acc: 0.3040 - val_loss: 841.9532 - val_acc: 0.2934\n",
      "Epoch 635/1000\n",
      "7471/7471 [==============================] - 0s 59us/sample - loss: 2701.8135 - acc: 0.3040 - val_loss: 833.0929 - val_acc: 0.2934\n",
      "Epoch 636/1000\n",
      "7471/7471 [==============================] - 1s 81us/sample - loss: 2759.6989 - acc: 0.3040 - val_loss: 834.4531 - val_acc: 0.2934\n",
      "Epoch 637/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 268076254.4650 - acc: 0.3040 - val_loss: 846.0512 - val_acc: 0.2934\n",
      "Epoch 638/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2666.7551 - acc: 0.3040 - val_loss: 839.9229 - val_acc: 0.2934\n",
      "Epoch 639/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2731.8511 - acc: 0.3040 - val_loss: 835.0607 - val_acc: 0.2934\n",
      "Epoch 640/1000\n",
      "7471/7471 [==============================] - 1s 78us/sample - loss: 2774.2814 - acc: 0.3040 - val_loss: 853.7011 - val_acc: 0.2934\n",
      "Epoch 641/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 5800.2813 - acc: 0.3040 - val_loss: 855.4203 - val_acc: 0.2934\n",
      "Epoch 642/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2704.0089 - acc: 0.3040 - val_loss: 848.2034 - val_acc: 0.2934\n",
      "Epoch 643/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2699.3603 - acc: 0.3040 - val_loss: 829.7450 - val_acc: 0.2934\n",
      "Epoch 644/1000\n",
      "7471/7471 [==============================] - 1s 103us/sample - loss: 2657.3407 - acc: 0.3040 - val_loss: 843.6321 - val_acc: 0.2934\n",
      "Epoch 645/1000\n",
      "7471/7471 [==============================] - 1s 102us/sample - loss: 2751.6316 - acc: 0.3040 - val_loss: 846.6390 - val_acc: 0.2934\n",
      "Epoch 646/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2726.3665 - acc: 0.3040 - val_loss: 847.0833 - val_acc: 0.2934\n",
      "Epoch 647/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2719.4378 - acc: 0.3040 - val_loss: 856.1652 - val_acc: 0.2934\n",
      "Epoch 648/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2675.8601 - acc: 0.3040 - val_loss: 856.5483 - val_acc: 0.2934\n",
      "Epoch 649/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2671.0568 - acc: 0.3038 - val_loss: 852.4784 - val_acc: 0.2934\n",
      "Epoch 650/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2718.6088 - acc: 0.3040 - val_loss: 856.7345 - val_acc: 0.2934\n",
      "Epoch 651/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2683.4073 - acc: 0.3040 - val_loss: 857.2156 - val_acc: 0.2934\n",
      "Epoch 652/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2691.7341 - acc: 0.3040 - val_loss: 855.9511 - val_acc: 0.2934\n",
      "Epoch 653/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2725.1749 - acc: 0.3038 - val_loss: 852.0620 - val_acc: 0.2934\n",
      "Epoch 654/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2726.3656 - acc: 0.3040 - val_loss: 860.4830 - val_acc: 0.2934\n",
      "Epoch 655/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2716.5764 - acc: 0.3040 - val_loss: 860.9601 - val_acc: 0.2934\n",
      "Epoch 656/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2649.5178 - acc: 0.3040 - val_loss: 834.1588 - val_acc: 0.2934\n",
      "Epoch 657/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2685.6134 - acc: 0.3040 - val_loss: 843.4103 - val_acc: 0.2934\n",
      "Epoch 658/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 7816.3617 - acc: 0.3040 - val_loss: 859.9975 - val_acc: 0.2934\n",
      "Epoch 659/1000\n",
      "7471/7471 [==============================] - 1s 85us/sample - loss: 2754.4401 - acc: 0.3040 - val_loss: 861.0205 - val_acc: 0.2934\n",
      "Epoch 660/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2737.3263 - acc: 0.3040 - val_loss: 863.0652 - val_acc: 0.2934\n",
      "Epoch 661/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2689.4986 - acc: 0.3041 - val_loss: 856.9603 - val_acc: 0.2934\n",
      "Epoch 662/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2677.5376 - acc: 0.3040 - val_loss: 852.3064 - val_acc: 0.2934\n",
      "Epoch 663/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2695.6721 - acc: 0.3040 - val_loss: 853.5591 - val_acc: 0.2934\n",
      "Epoch 664/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2691.7756 - acc: 0.3040 - val_loss: 853.8616 - val_acc: 0.2934\n",
      "Epoch 665/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2653.2687 - acc: 0.3040 - val_loss: 850.9650 - val_acc: 0.2934\n",
      "Epoch 666/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2702.3269 - acc: 0.3040 - val_loss: 848.9185 - val_acc: 0.2934\n",
      "Epoch 667/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2698.4899 - acc: 0.3040 - val_loss: 848.2883 - val_acc: 0.2934\n",
      "Epoch 668/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2703.6854 - acc: 0.3040 - val_loss: 850.2758 - val_acc: 0.2934\n",
      "Epoch 669/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2671.1792 - acc: 0.3041 - val_loss: 855.7781 - val_acc: 0.2934\n",
      "Epoch 670/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2692.5857 - acc: 0.3040 - val_loss: 865.8569 - val_acc: 0.2934\n",
      "Epoch 671/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2670.2901 - acc: 0.3040 - val_loss: 867.9308 - val_acc: 0.2934\n",
      "Epoch 672/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2662.4538 - acc: 0.3040 - val_loss: 869.9337 - val_acc: 0.2934\n",
      "Epoch 673/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 40715.7036 - acc: 0.3040 - val_loss: 868.6943 - val_acc: 0.2934\n",
      "Epoch 674/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2685.1256 - acc: 0.3040 - val_loss: 860.5134 - val_acc: 0.2934\n",
      "Epoch 675/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2679.3011 - acc: 0.3040 - val_loss: 859.9281 - val_acc: 0.2934\n",
      "Epoch 676/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2747.0192 - acc: 0.3040 - val_loss: 873.7830 - val_acc: 0.2934\n",
      "Epoch 677/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2694.6333 - acc: 0.3040 - val_loss: 863.4933 - val_acc: 0.2934\n",
      "Epoch 678/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2692.4495 - acc: 0.3040 - val_loss: 858.1329 - val_acc: 0.2934\n",
      "Epoch 679/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2642.3546 - acc: 0.3040 - val_loss: 852.0352 - val_acc: 0.2934\n",
      "Epoch 680/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2717.3206 - acc: 0.3040 - val_loss: 858.2409 - val_acc: 0.2934\n",
      "Epoch 681/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2792.3412 - acc: 0.3040 - val_loss: 856.0439 - val_acc: 0.2934\n",
      "Epoch 682/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2650.6758 - acc: 0.3038 - val_loss: 852.3031 - val_acc: 0.2934\n",
      "Epoch 683/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2752.2677 - acc: 0.3040 - val_loss: 867.7613 - val_acc: 0.2934\n",
      "Epoch 684/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2729.4644 - acc: 0.3040 - val_loss: 873.4729 - val_acc: 0.2934\n",
      "Epoch 685/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 2668.3153 - acc: 0.3040 - val_loss: 860.1030 - val_acc: 0.2934\n",
      "Epoch 686/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 2687.5694 - acc: 0.3040 - val_loss: 873.2229 - val_acc: 0.2934\n",
      "Epoch 687/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2724.3414 - acc: 0.3040 - val_loss: 871.3363 - val_acc: 0.2934\n",
      "Epoch 688/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2650.7988 - acc: 0.3041 - val_loss: 864.4455 - val_acc: 0.2934\n",
      "Epoch 689/1000\n",
      "7471/7471 [==============================] - 1s 82us/sample - loss: 2661.4041 - acc: 0.3040 - val_loss: 865.8921 - val_acc: 0.2934\n",
      "Epoch 690/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2644.0565 - acc: 0.3040 - val_loss: 854.1885 - val_acc: 0.2934\n",
      "Epoch 691/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 2683.3229 - acc: 0.3040 - val_loss: 853.2874 - val_acc: 0.2934\n",
      "Epoch 692/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2720.9718 - acc: 0.3040 - val_loss: 866.1468 - val_acc: 0.2934\n",
      "Epoch 693/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2687.7944 - acc: 0.3040 - val_loss: 859.8599 - val_acc: 0.2934\n",
      "Epoch 694/1000\n",
      "7471/7471 [==============================] - 1s 114us/sample - loss: 2690.0704 - acc: 0.3040 - val_loss: 858.9780 - val_acc: 0.2934\n",
      "Epoch 695/1000\n",
      "7471/7471 [==============================] - 1s 89us/sample - loss: 2723.5414 - acc: 0.3038 - val_loss: 861.6844 - val_acc: 0.2934\n",
      "Epoch 696/1000\n",
      "7471/7471 [==============================] - 1s 98us/sample - loss: 2732.8710 - acc: 0.3040 - val_loss: 851.5109 - val_acc: 0.2934\n",
      "Epoch 697/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2666.5512 - acc: 0.3040 - val_loss: 852.4348 - val_acc: 0.2934\n",
      "Epoch 698/1000\n",
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2709.6294 - acc: 0.3040 - val_loss: 849.6201 - val_acc: 0.2934\n",
      "Epoch 699/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 2710.7677 - acc: 0.3040 - val_loss: 838.1389 - val_acc: 0.2934\n",
      "Epoch 700/1000\n",
      "7471/7471 [==============================] - 1s 86us/sample - loss: 2759.4468 - acc: 0.3040 - val_loss: 839.8317 - val_acc: 0.2934\n",
      "Epoch 701/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2726.1591 - acc: 0.3040 - val_loss: 857.5838 - val_acc: 0.2934\n",
      "Epoch 702/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 2661.4476 - acc: 0.3040 - val_loss: 855.8554 - val_acc: 0.2934\n",
      "Epoch 703/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 2735.1167 - acc: 0.3040 - val_loss: 856.0721 - val_acc: 0.2934\n",
      "Epoch 704/1000\n",
      "7471/7471 [==============================] - 1s 83us/sample - loss: 2769.6794 - acc: 0.3040 - val_loss: 852.3770 - val_acc: 0.2934\n",
      "Epoch 705/1000\n",
      "7471/7471 [==============================] - 1s 84us/sample - loss: 20114.7862 - acc: 0.3038 - val_loss: 853.7854 - val_acc: 0.2934\n",
      "Epoch 706/1000\n",
      "7471/7471 [==============================] - 1s 93us/sample - loss: 2701.4749 - acc: 0.3040 - val_loss: 859.9126 - val_acc: 0.2934\n",
      "Epoch 707/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2720.1917 - acc: 0.3040 - val_loss: 862.2994 - val_acc: 0.2934\n",
      "Epoch 708/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2736.8705 - acc: 0.3040 - val_loss: 867.4236 - val_acc: 0.2934\n",
      "Epoch 709/1000\n",
      "7471/7471 [==============================] - 1s 100us/sample - loss: 2718.0126 - acc: 0.3038 - val_loss: 861.1314 - val_acc: 0.2934\n",
      "Epoch 710/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2684.5210 - acc: 0.3040 - val_loss: 860.7673 - val_acc: 0.2934\n",
      "Epoch 711/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 845.1946 - acc: 0.3040 - val_loss: 850.2690 - val_acc: 0.2934\n",
      "Epoch 712/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2679.6802 - acc: 0.3040 - val_loss: 856.3970 - val_acc: 0.2934\n",
      "Epoch 713/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2638.6105 - acc: 0.3038 - val_loss: 849.0757 - val_acc: 0.2934\n",
      "Epoch 714/1000\n",
      "7471/7471 [==============================] - 1s 101us/sample - loss: 2654.7349 - acc: 0.3040 - val_loss: 844.6095 - val_acc: 0.2934\n",
      "Epoch 715/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2649.1489 - acc: 0.3040 - val_loss: 848.3458 - val_acc: 0.2934\n",
      "Epoch 716/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2637.2498 - acc: 0.3040 - val_loss: 842.7432 - val_acc: 0.2934\n",
      "Epoch 717/1000\n",
      "7471/7471 [==============================] - 1s 97us/sample - loss: 2679.0453 - acc: 0.3040 - val_loss: 855.5127 - val_acc: 0.2934\n",
      "Epoch 718/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2812.2965 - acc: 0.3040 - val_loss: 865.2634 - val_acc: 0.2934\n",
      "Epoch 719/1000\n",
      "7471/7471 [==============================] - 1s 95us/sample - loss: 2664.7361 - acc: 0.3040 - val_loss: 869.0240 - val_acc: 0.2934\n",
      "Epoch 720/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2692.2544 - acc: 0.3040 - val_loss: 863.9792 - val_acc: 0.2934\n",
      "Epoch 721/1000\n",
      "7471/7471 [==============================] - 1s 99us/sample - loss: 2781.2566 - acc: 0.3040 - val_loss: 866.4164 - val_acc: 0.2934\n",
      "Epoch 722/1000\n",
      "7471/7471 [==============================] - 1s 94us/sample - loss: 2759.9769 - acc: 0.3040 - val_loss: 863.1694 - val_acc: 0.2934\n",
      "Epoch 723/1000\n",
      "7471/7471 [==============================] - 1s 96us/sample - loss: 2769.9169 - acc: 0.3040 - val_loss: 854.0905 - val_acc: 0.2934\n",
      "Epoch 724/1000\n",
      "7471/7471 [==============================] - 1s 90us/sample - loss: 27657.6110 - acc: 0.3040 - val_loss: 864.9680 - val_acc: 0.2934\n",
      "Epoch 725/1000\n",
      "7471/7471 [==============================] - 1s 91us/sample - loss: 2798.3289 - acc: 0.3040 - val_loss: 855.0124 - val_acc: 0.2934\n",
      "Epoch 726/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2728.0264 - acc: 0.3040 - val_loss: 867.8873 - val_acc: 0.2934\n",
      "Epoch 727/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7471/7471 [==============================] - 1s 87us/sample - loss: 2697.5890 - acc: 0.3040 - val_loss: 866.9785 - val_acc: 0.2934\n",
      "Epoch 728/1000\n",
      "7471/7471 [==============================] - 1s 88us/sample - loss: 2703.5863 - acc: 0.3040 - val_loss: 858.2035 - val_acc: 0.2934\n",
      "Epoch 729/1000\n",
      "7471/7471 [==============================] - 1s 92us/sample - loss: 2767.3864 - acc: 0.3040 - val_loss: 872.5220 - val_acc: 0.2934\n",
      "Epoch 730/1000\n",
      "7168/7471 [===========================>..] - ETA: 0s - loss: 2763.1702 - acc: 0.3044"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8c2041403915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(input_x_train, input_y_train, \n\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           callbacks=[WandbCallback()])\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/wandb/keras/__init__.py\u001b[0m in \u001b[0;36mnew_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    410\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# Callbacks batch end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    253\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3495\u001b[0m     \"\"\"\n\u001b[1;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3497\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3536\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3538\u001b[0;31m     \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3539\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3540\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(input_x_train, input_y_train, \n",
    "          epochs=config.epochs, validation_data=(input_x_test, input_y_test), \n",
    "          callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Blogpost",
   "language": "python",
   "name": "blogpost_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
