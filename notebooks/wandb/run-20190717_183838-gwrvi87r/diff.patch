diff --git a/notebooks/Part_3.ipynb b/notebooks/Part_3.ipynb
index 03b7fa5..02302e1 100644
--- a/notebooks/Part_3.ipynb
+++ b/notebooks/Part_3.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -22,7 +22,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -42,7 +42,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -53,7 +53,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -69,6 +69,376 @@
     "input_y_test = scl_y.transform(y_test)"
    ]
   },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Import Dependencies For Training In Keras"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from tensorflow.keras.models import Sequential\n",
+    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
+    "from tensorflow.keras.optimizers import Adam\n",
+    "from tensorflow.keras.losses import mean_squared_error"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Set Up Wandb To Monitor Training"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/gilangrilhami/Predicting-Micronutrients-using-Neural-Networks-and-Random-Forest-notebooks/runs/gwrvi87r\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
+       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
+       "    "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "import wandb\n",
+    "from wandb.keras import WandbCallback\n",
+    "\n",
+    "wandb.init()\n",
+    "config = wandb.config"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Set up paramaters for training\n",
+    "\n",
+    "config.batch_size = 128\n",
+    "config.epochs = 1000\n",
+    "config.learning_rate = 0.001\n",
+    "config.dropout = 0.8"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Building the model"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "WARNING: Logging before flag parsing goes to stderr.\n",
+      "W0718 01:38:43.230565 140301917030208 deprecation.py:506] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
+      "Instructions for updating:\n",
+      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
+     ]
+    }
+   ],
+   "source": [
+    "model = Sequential()\n",
+    "model.add(Dense(input_x_train.shape[1]))\n",
+    "model.add(Dense(1024))\n",
+    "model.add(Activation('relu'))\n",
+    "model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(512))\n",
+    "model.add(Activation('relu'))\n",
+    "model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(128))\n",
+    "model.add(Activation('relu'))\n",
+    "model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(64))\n",
+    "model.add(Activation('relu'))\n",
+    "model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(32))\n",
+    "model.add(Activation('relu'))\n",
+    "model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(6))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Training the Model"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Set the optimization method and cirterion\n",
+    "\n",
+    "adam = Adam(lr=config.learning_rate)\n",
+    "model.compile(loss=mean_squared_error, optimizer=adam, metrics=['accuracy'])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {
+    "scrolled": true
+   },
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "W0718 01:39:01.506063 140301917030208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
+      "W0718 01:39:01.557744 140301917030208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
+      "W0718 01:39:01.618568 140301917030208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
+      "W0718 01:39:01.680527 140301917030208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
+      "W0718 01:39:01.739275 140301917030208 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Train on 7471 samples, validate on 1319 samples\n",
+      "Epoch 1/1000\n",
+      "7471/7471 [==============================] - 3s 436us/sample - loss: 208166779938.9456 - acc: 0.1810 - val_loss: 1071.5138 - val_acc: 0.1478\n",
+      "Epoch 2/1000\n",
+      "7471/7471 [==============================] - 2s 334us/sample - loss: 64152522329.7605 - acc: 0.1648 - val_loss: 1111.1232 - val_acc: 0.1660\n",
+      "Epoch 3/1000\n",
+      "7471/7471 [==============================] - 3s 339us/sample - loss: 705386323594.6384 - acc: 0.1672 - val_loss: 1012.5758 - val_acc: 0.1463\n",
+      "Epoch 4/1000\n",
+      "7471/7471 [==============================] - 2s 331us/sample - loss: 566716.5365 - acc: 0.1684 - val_loss: 979.7837 - val_acc: 0.1425\n",
+      "Epoch 5/1000\n",
+      "7471/7471 [==============================] - 2s 330us/sample - loss: 526432.6374 - acc: 0.1676 - val_loss: 980.1983 - val_acc: 0.1425\n",
+      "Epoch 6/1000\n",
+      "7471/7471 [==============================] - 2s 328us/sample - loss: 15995073531.3057 - acc: 0.1578 - val_loss: 980.6106 - val_acc: 0.1380\n",
+      "Epoch 7/1000\n",
+      "7471/7471 [==============================] - 2s 320us/sample - loss: 25106662.9110 - acc: 0.1715 - val_loss: 979.8038 - val_acc: 0.1433\n",
+      "Epoch 8/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 29950521837.8010 - acc: 0.1585 - val_loss: 979.8413 - val_acc: 0.1433\n",
+      "Epoch 9/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 750090.4573 - acc: 0.1566 - val_loss: 979.8411 - val_acc: 0.1433\n",
+      "Epoch 10/1000\n",
+      "7471/7471 [==============================] - 2s 314us/sample - loss: 36064538.5750 - acc: 0.1518 - val_loss: 979.8446 - val_acc: 0.1433\n",
+      "Epoch 11/1000\n",
+      "7471/7471 [==============================] - 2s 325us/sample - loss: 156374.7644 - acc: 0.1535 - val_loss: 979.8441 - val_acc: 0.1433\n",
+      "Epoch 12/1000\n",
+      "7471/7471 [==============================] - 2s 310us/sample - loss: 38122.6454 - acc: 0.1504 - val_loss: 979.8432 - val_acc: 0.1433\n",
+      "Epoch 13/1000\n",
+      "7471/7471 [==============================] - 2s 319us/sample - loss: 22964.6062 - acc: 0.1483 - val_loss: 979.8424 - val_acc: 0.1433\n",
+      "Epoch 14/1000\n",
+      "7471/7471 [==============================] - 2s 317us/sample - loss: 68378038717.7354 - acc: 0.1507 - val_loss: 979.9109 - val_acc: 0.1433\n",
+      "Epoch 15/1000\n",
+      "7471/7471 [==============================] - 2s 304us/sample - loss: 1459912.4617 - acc: 0.1474 - val_loss: 979.9109 - val_acc: 0.1433\n",
+      "Epoch 16/1000\n",
+      "7471/7471 [==============================] - 2s 292us/sample - loss: 39199.5842 - acc: 0.1504 - val_loss: 979.9099 - val_acc: 0.1433\n",
+      "Epoch 17/1000\n",
+      "7471/7471 [==============================] - 2s 305us/sample - loss: 54054.8597 - acc: 0.1456 - val_loss: 979.9091 - val_acc: 0.1433\n",
+      "Epoch 18/1000\n",
+      "7471/7471 [==============================] - 2s 332us/sample - loss: 161871.0024 - acc: 0.1443 - val_loss: 979.9079 - val_acc: 0.1433\n",
+      "Epoch 19/1000\n",
+      "7471/7471 [==============================] - 3s 353us/sample - loss: 188328.5884 - acc: 0.1442 - val_loss: 979.9067 - val_acc: 0.1433\n",
+      "Epoch 20/1000\n",
+      "7471/7471 [==============================] - 3s 336us/sample - loss: 45858934.0987 - acc: 0.1405 - val_loss: 979.9059 - val_acc: 0.1433\n",
+      "Epoch 21/1000\n",
+      "7471/7471 [==============================] - 2s 308us/sample - loss: 740129.3330 - acc: 0.1444 - val_loss: 979.9043 - val_acc: 0.1433\n",
+      "Epoch 22/1000\n",
+      "7471/7471 [==============================] - 3s 356us/sample - loss: 6903.9926 - acc: 0.1412 - val_loss: 979.9026 - val_acc: 0.1433\n",
+      "Epoch 23/1000\n",
+      "7471/7471 [==============================] - 2s 333us/sample - loss: 18016.0702 - acc: 0.1428 - val_loss: 979.9005 - val_acc: 0.1433\n",
+      "Epoch 24/1000\n",
+      "7471/7471 [==============================] - 2s 331us/sample - loss: 435463379.2730 - acc: 0.1430 - val_loss: 979.9513 - val_acc: 0.1433\n",
+      "Epoch 25/1000\n",
+      "7471/7471 [==============================] - 3s 340us/sample - loss: 3468.0871 - acc: 0.1438 - val_loss: 979.9490 - val_acc: 0.1433\n",
+      "Epoch 26/1000\n",
+      "7471/7471 [==============================] - 3s 338us/sample - loss: 5133.9683 - acc: 0.1408 - val_loss: 979.9464 - val_acc: 0.1433\n",
+      "Epoch 27/1000\n",
+      "7471/7471 [==============================] - 3s 343us/sample - loss: 4839.4386 - acc: 0.1432 - val_loss: 979.9436 - val_acc: 0.1433\n",
+      "Epoch 28/1000\n",
+      "7471/7471 [==============================] - 3s 340us/sample - loss: 3004.9963 - acc: 0.1436 - val_loss: 979.9403 - val_acc: 0.1433\n",
+      "Epoch 29/1000\n",
+      "7471/7471 [==============================] - 3s 340us/sample - loss: 26990898494.0058 - acc: 0.1443 - val_loss: 979.8617 - val_acc: 0.1433\n",
+      "Epoch 30/1000\n",
+      "7471/7471 [==============================] - 2s 289us/sample - loss: 3095.0875 - acc: 0.1420 - val_loss: 979.8586 - val_acc: 0.1433\n",
+      "Epoch 31/1000\n",
+      "7471/7471 [==============================] - 2s 334us/sample - loss: 7329.5718 - acc: 0.1427 - val_loss: 979.8552 - val_acc: 0.1433\n",
+      "Epoch 32/1000\n",
+      "7471/7471 [==============================] - 2s 308us/sample - loss: 4818.9731 - acc: 0.1440 - val_loss: 979.8510 - val_acc: 0.1433\n",
+      "Epoch 33/1000\n",
+      "7471/7471 [==============================] - 2s 325us/sample - loss: 3263.3942 - acc: 0.1420 - val_loss: 979.8466 - val_acc: 0.1433\n",
+      "Epoch 34/1000\n",
+      "7471/7471 [==============================] - 2s 315us/sample - loss: 5483.0821 - acc: 0.1423 - val_loss: 979.8416 - val_acc: 0.1433\n",
+      "Epoch 35/1000\n",
+      "7471/7471 [==============================] - 2s 310us/sample - loss: 7188.0474 - acc: 0.1431 - val_loss: 979.8354 - val_acc: 0.1433\n",
+      "Epoch 36/1000\n",
+      "7471/7471 [==============================] - 2s 315us/sample - loss: 20772.7009 - acc: 0.1438 - val_loss: 979.8285 - val_acc: 0.1433\n",
+      "Epoch 37/1000\n",
+      "7471/7471 [==============================] - 2s 307us/sample - loss: 2953.7161 - acc: 0.1421 - val_loss: 979.8212 - val_acc: 0.1433\n",
+      "Epoch 38/1000\n",
+      "7471/7471 [==============================] - 2s 315us/sample - loss: 18748.1665 - acc: 0.1399 - val_loss: 979.8140 - val_acc: 0.1433\n",
+      "Epoch 39/1000\n",
+      "7471/7471 [==============================] - 2s 325us/sample - loss: 2987.2585 - acc: 0.1424 - val_loss: 979.8049 - val_acc: 0.1433\n",
+      "Epoch 40/1000\n",
+      "7471/7471 [==============================] - 2s 306us/sample - loss: 3984.9518 - acc: 0.1428 - val_loss: 979.7946 - val_acc: 0.1433\n",
+      "Epoch 41/1000\n",
+      "7471/7471 [==============================] - 3s 337us/sample - loss: 3083.0056 - acc: 0.1403 - val_loss: 979.7830 - val_acc: 0.1433\n",
+      "Epoch 42/1000\n",
+      "7471/7471 [==============================] - 2s 320us/sample - loss: 2988.0930 - acc: 0.1419 - val_loss: 979.7699 - val_acc: 0.1433\n",
+      "Epoch 43/1000\n",
+      "7471/7471 [==============================] - 2s 309us/sample - loss: 3183.1155 - acc: 0.1417 - val_loss: 979.7552 - val_acc: 0.1433\n",
+      "Epoch 44/1000\n",
+      "7471/7471 [==============================] - 2s 303us/sample - loss: 4347.4664 - acc: 0.1409 - val_loss: 979.7392 - val_acc: 0.1433\n",
+      "Epoch 45/1000\n",
+      "7471/7471 [==============================] - 2s 329us/sample - loss: 2949.9212 - acc: 0.1412 - val_loss: 979.7207 - val_acc: 0.1433\n",
+      "Epoch 46/1000\n",
+      "7471/7471 [==============================] - 2s 325us/sample - loss: 3043.9440 - acc: 0.1409 - val_loss: 979.6996 - val_acc: 0.1433\n",
+      "Epoch 47/1000\n",
+      "7471/7471 [==============================] - 2s 316us/sample - loss: 3271.5094 - acc: 0.1408 - val_loss: 979.6757 - val_acc: 0.1433\n",
+      "Epoch 48/1000\n",
+      "7471/7471 [==============================] - 2s 330us/sample - loss: 3272.9492 - acc: 0.1430 - val_loss: 979.6496 - val_acc: 0.1433\n",
+      "Epoch 49/1000\n",
+      "7471/7471 [==============================] - 2s 315us/sample - loss: 2989.9220 - acc: 0.1430 - val_loss: 979.6198 - val_acc: 0.1433\n",
+      "Epoch 50/1000\n",
+      "7471/7471 [==============================] - 2s 319us/sample - loss: 3165.8639 - acc: 0.1421 - val_loss: 979.5870 - val_acc: 0.1433\n",
+      "Epoch 51/1000\n",
+      "7471/7471 [==============================] - 2s 319us/sample - loss: 4856004.3031 - acc: 0.1424 - val_loss: 979.4964 - val_acc: 0.1433\n",
+      "Epoch 52/1000\n",
+      "7471/7471 [==============================] - 2s 319us/sample - loss: 3256.2953 - acc: 0.1417 - val_loss: 979.4607 - val_acc: 0.1433\n",
+      "Epoch 53/1000\n",
+      "7471/7471 [==============================] - 2s 332us/sample - loss: 2980.5757 - acc: 0.1421 - val_loss: 979.4210 - val_acc: 0.1433\n",
+      "Epoch 54/1000\n",
+      "7471/7471 [==============================] - 2s 328us/sample - loss: 2963.1581 - acc: 0.1426 - val_loss: 979.3729 - val_acc: 0.1433\n",
+      "Epoch 55/1000\n",
+      "7471/7471 [==============================] - 3s 335us/sample - loss: 5867.3379 - acc: 0.1417 - val_loss: 979.3213 - val_acc: 0.1433\n",
+      "Epoch 56/1000\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "7471/7471 [==============================] - 3s 344us/sample - loss: 86468.1853 - acc: 0.1420 - val_loss: 979.2568 - val_acc: 0.1433\n",
+      "Epoch 57/1000\n",
+      "7471/7471 [==============================] - 2s 322us/sample - loss: 2966.9746 - acc: 0.1427 - val_loss: 979.1919 - val_acc: 0.1433\n",
+      "Epoch 58/1000\n",
+      "7471/7471 [==============================] - 2s 324us/sample - loss: 2943.2705 - acc: 0.2712 - val_loss: 979.1198 - val_acc: 0.2934\n",
+      "Epoch 59/1000\n",
+      "7471/7471 [==============================] - 3s 338us/sample - loss: 1056317406.7404 - acc: 0.3025 - val_loss: 978.9515 - val_acc: 0.2934\n",
+      "Epoch 60/1000\n",
+      "7471/7471 [==============================] - 3s 340us/sample - loss: 2945.7651 - acc: 0.3030 - val_loss: 978.9447 - val_acc: 0.2934\n",
+      "Epoch 61/1000\n",
+      "7471/7471 [==============================] - 2s 320us/sample - loss: 2962.0273 - acc: 0.3026 - val_loss: 978.9372 - val_acc: 0.2934\n",
+      "Epoch 62/1000\n",
+      "7471/7471 [==============================] - 2s 307us/sample - loss: 2943.9511 - acc: 0.3028 - val_loss: 978.9287 - val_acc: 0.2934\n",
+      "Epoch 63/1000\n",
+      "7471/7471 [==============================] - 2s 328us/sample - loss: 2949.0357 - acc: 0.3034 - val_loss: 978.9190 - val_acc: 0.2934\n",
+      "Epoch 64/1000\n",
+      "7471/7471 [==============================] - 2s 318us/sample - loss: 2941.6965 - acc: 0.3034 - val_loss: 978.9083 - val_acc: 0.2934\n",
+      "Epoch 65/1000\n",
+      "7471/7471 [==============================] - 2s 305us/sample - loss: 2945.9083 - acc: 0.3041 - val_loss: 978.8962 - val_acc: 0.2934\n",
+      "Epoch 66/1000\n",
+      "7471/7471 [==============================] - 2s 312us/sample - loss: 2944.2542 - acc: 0.3036 - val_loss: 978.8826 - val_acc: 0.2934\n",
+      "Epoch 67/1000\n",
+      "7471/7471 [==============================] - 3s 340us/sample - loss: 2942.5008 - acc: 0.3028 - val_loss: 978.8672 - val_acc: 0.2934\n",
+      "Epoch 68/1000\n",
+      "7471/7471 [==============================] - 2s 334us/sample - loss: 2940.3185 - acc: 0.3037 - val_loss: 978.8501 - val_acc: 0.2934\n",
+      "Epoch 69/1000\n",
+      "7471/7471 [==============================] - 2s 328us/sample - loss: 2967.7432 - acc: 0.3034 - val_loss: 978.8310 - val_acc: 0.2934\n",
+      "Epoch 70/1000\n",
+      "7471/7471 [==============================] - 2s 328us/sample - loss: 2939.5503 - acc: 0.3030 - val_loss: 978.8091 - val_acc: 0.2934\n",
+      "Epoch 71/1000\n",
+      "7471/7471 [==============================] - 2s 321us/sample - loss: 2940.4933 - acc: 0.3032 - val_loss: 978.7849 - val_acc: 0.2934\n",
+      "Epoch 72/1000\n",
+      "7471/7471 [==============================] - 3s 335us/sample - loss: 2942.8471 - acc: 0.3040 - val_loss: 978.7574 - val_acc: 0.2934\n",
+      "Epoch 73/1000\n",
+      "7471/7471 [==============================] - 3s 355us/sample - loss: 2942.8248 - acc: 0.3038 - val_loss: 978.7265 - val_acc: 0.2934\n",
+      "Epoch 74/1000\n",
+      "7471/7471 [==============================] - 3s 343us/sample - loss: 2940.1410 - acc: 0.3038 - val_loss: 978.6921 - val_acc: 0.2934\n",
+      "Epoch 75/1000\n",
+      "7471/7471 [==============================] - 3s 347us/sample - loss: 3066.1610 - acc: 0.3040 - val_loss: 978.6535 - val_acc: 0.2934\n",
+      "Epoch 76/1000\n",
+      "7471/7471 [==============================] - 3s 342us/sample - loss: 2939.8030 - acc: 0.3036 - val_loss: 978.6099 - val_acc: 0.2934\n",
+      "Epoch 77/1000\n",
+      "7471/7471 [==============================] - 2s 328us/sample - loss: 2939.7560 - acc: 0.3034 - val_loss: 978.5612 - val_acc: 0.2934\n",
+      "Epoch 78/1000\n",
+      "7471/7471 [==============================] - 2s 331us/sample - loss: 2939.2216 - acc: 0.3040 - val_loss: 978.5066 - val_acc: 0.2934\n",
+      "Epoch 79/1000\n",
+      "7471/7471 [==============================] - 2s 313us/sample - loss: 2942.3665 - acc: 0.3038 - val_loss: 978.4445 - val_acc: 0.2934\n",
+      "Epoch 80/1000\n",
+      "7471/7471 [==============================] - 3s 338us/sample - loss: 2940.8646 - acc: 0.3040 - val_loss: 978.3760 - val_acc: 0.2934\n",
+      "Epoch 81/1000\n",
+      "7471/7471 [==============================] - 2s 319us/sample - loss: 2941.5803 - acc: 0.3029 - val_loss: 978.2983 - val_acc: 0.2934\n",
+      "Epoch 82/1000\n",
+      "7471/7471 [==============================] - 2s 327us/sample - loss: 2967.5665 - acc: 0.3036 - val_loss: 978.2125 - val_acc: 0.2934\n",
+      "Epoch 83/1000\n",
+      "7471/7471 [==============================] - 2s 319us/sample - loss: 2941.3262 - acc: 0.3034 - val_loss: 978.1169 - val_acc: 0.2934\n",
+      "Epoch 84/1000\n",
+      "7471/7471 [==============================] - 2s 319us/sample - loss: 2938.8800 - acc: 0.3038 - val_loss: 978.0093 - val_acc: 0.2934\n",
+      "Epoch 85/1000\n",
+      "7471/7471 [==============================] - 2s 314us/sample - loss: 2940.5596 - acc: 0.3036 - val_loss: 977.8889 - val_acc: 0.2934\n",
+      "Epoch 86/1000\n",
+      "7471/7471 [==============================] - 3s 349us/sample - loss: 2995.0594 - acc: 0.3037 - val_loss: 977.7571 - val_acc: 0.2934\n",
+      "Epoch 87/1000\n",
+      "7471/7471 [==============================] - 3s 344us/sample - loss: 2938.8144 - acc: 0.3040 - val_loss: 977.6063 - val_acc: 0.2934\n",
+      "Epoch 88/1000\n",
+      "7471/7471 [==============================] - 3s 358us/sample - loss: 2938.1199 - acc: 0.3037 - val_loss: 977.4433 - val_acc: 0.2934\n",
+      "Epoch 89/1000\n",
+      "7471/7471 [==============================] - 2s 333us/sample - loss: 2938.5115 - acc: 0.3038 - val_loss: 977.2717 - val_acc: 0.2934\n",
+      "Epoch 90/1000\n",
+      "7471/7471 [==============================] - 3s 338us/sample - loss: 2937.6875 - acc: 0.3038 - val_loss: 977.0580 - val_acc: 0.2934\n",
+      "Epoch 91/1000\n",
+      "7471/7471 [==============================] - 2s 308us/sample - loss: 3429.5413 - acc: 0.3036 - val_loss: 976.8343 - val_acc: 0.2934\n",
+      "Epoch 92/1000\n",
+      "7471/7471 [==============================] - 3s 337us/sample - loss: 2956.6002 - acc: 0.3036 - val_loss: 976.5944 - val_acc: 0.2934\n",
+      "Epoch 93/1000\n",
+      "7471/7471 [==============================] - 3s 347us/sample - loss: 2938.1645 - acc: 0.3041 - val_loss: 976.3298 - val_acc: 0.2934\n",
+      "Epoch 94/1000\n",
+      "7471/7471 [==============================] - 3s 349us/sample - loss: 2936.5797 - acc: 0.3041 - val_loss: 976.0394 - val_acc: 0.2934\n",
+      "Epoch 95/1000\n",
+      "7471/7471 [==============================] - 3s 343us/sample - loss: 2936.3208 - acc: 0.3041 - val_loss: 975.7292 - val_acc: 0.2934\n",
+      "Epoch 96/1000\n",
+      "7471/7471 [==============================] - 3s 340us/sample - loss: 2996.4090 - acc: 0.3040 - val_loss: 975.4014 - val_acc: 0.2934\n",
+      "Epoch 97/1000\n",
+      "7471/7471 [==============================] - 2s 302us/sample - loss: 2935.6330 - acc: 0.3036 - val_loss: 975.0446 - val_acc: 0.2934\n",
+      "Epoch 98/1000\n",
+      "7471/7471 [==============================] - 2s 327us/sample - loss: 2935.9242 - acc: 0.3037 - val_loss: 974.6683 - val_acc: 0.2934\n",
+      "Epoch 99/1000\n",
+      "7471/7471 [==============================] - 3s 371us/sample - loss: 3031.4426 - acc: 0.3042 - val_loss: 974.2776 - val_acc: 0.2934\n",
+      "Epoch 100/1000\n",
+      "3648/7471 [=============>................] - ETA: 1s - loss: 5243.2673 - acc: 0.3026"
+     ]
+    }
+   ],
+   "source": [
+    "model.fit(input_x_train, input_y_train, \n",
+    "          epochs=config.epochs, validation_data=(input_x_test, input_y_test), \n",
+    "          callbacks=[WandbCallback()])"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
