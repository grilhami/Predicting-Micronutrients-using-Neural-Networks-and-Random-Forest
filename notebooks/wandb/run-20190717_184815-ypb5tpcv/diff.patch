diff --git a/notebooks/Part_3.ipynb b/notebooks/Part_3.ipynb
index 03b7fa5..691f457 100644
--- a/notebooks/Part_3.ipynb
+++ b/notebooks/Part_3.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -22,7 +22,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -42,7 +42,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -53,7 +53,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -69,6 +69,1195 @@
     "input_y_test = scl_y.transform(y_test)"
    ]
   },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Import Dependencies For Training In Keras"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from tensorflow.keras.models import Sequential\n",
+    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
+    "from tensorflow.keras.optimizers import Adam\n",
+    "from tensorflow.keras.losses import mean_squared_error"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Set Up Wandb To Monitor Training"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/gilangrilhami/Predicting-Micronutrients-using-Neural-Networks-and-Random-Forest-notebooks/runs/ypb5tpcv\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
+       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
+       "    "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "import wandb\n",
+    "from wandb.keras import WandbCallback\n",
+    "\n",
+    "wandb.init()\n",
+    "config = wandb.config"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Set up paramaters for training\n",
+    "\n",
+    "config.batch_size = 128\n",
+    "config.epochs = 1000\n",
+    "config.learning_rate = 0.001\n",
+    "config.dropout = 0.8"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Building the model"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "WARNING: Logging before flag parsing goes to stderr.\n",
+      "W0718 01:48:17.949216 139762342823744 deprecation.py:506] From /mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
+      "Instructions for updating:\n",
+      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
+     ]
+    }
+   ],
+   "source": [
+    "model = Sequential()\n",
+    "# model.add(Dense(input_x_train.shape[1]))\n",
+    "# model.add(Dense(1024))\n",
+    "# model.add(Activation('relu'))\n",
+    "# model.add(Dropout(config.dropout))\n",
+    "# model.add(Dense(512))\n",
+    "# model.add(Activation('relu'))\n",
+    "# model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(128))\n",
+    "model.add(Activation('relu'))\n",
+    "model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(64))\n",
+    "model.add(Activation('relu'))\n",
+    "model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(32))\n",
+    "model.add(Activation('relu'))\n",
+    "model.add(Dropout(config.dropout))\n",
+    "model.add(Dense(6))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Training the Model"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Set the optimization method and cirterion\n",
+    "\n",
+    "adam = Adam(lr=config.learning_rate)\n",
+    "model.compile(loss=mean_squared_error, optimizer=adam, metrics=['accuracy'])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "metadata": {
+    "scrolled": true
+   },
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "W0718 01:48:22.847671 139762342823744 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
+      "W0718 01:48:22.911624 139762342823744 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Train on 7471 samples, validate on 1319 samples\n",
+      "Epoch 1/1000\n",
+      "7471/7471 [==============================] - 3s 357us/sample - loss: 3966508511.8291 - acc: 0.2093 - val_loss: 1924.3691 - val_acc: 0.2745\n",
+      "Epoch 2/1000\n",
+      "7471/7471 [==============================] - 2s 278us/sample - loss: 2499141075.6120 - acc: 0.2174 - val_loss: 1551.3302 - val_acc: 0.2578\n",
+      "Epoch 3/1000\n",
+      "7471/7471 [==============================] - 2s 279us/sample - loss: 8413852822.5520 - acc: 0.2196 - val_loss: 986.5402 - val_acc: 0.1865\n",
+      "Epoch 4/1000\n",
+      "7471/7471 [==============================] - 2s 290us/sample - loss: 4108865183.0629 - acc: 0.2162 - val_loss: 970.0044 - val_acc: 0.1971\n",
+      "Epoch 5/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 12736487424.7277 - acc: 0.2324 - val_loss: 970.7417 - val_acc: 0.2388\n",
+      "Epoch 6/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 1030976867.2573 - acc: 0.2466 - val_loss: 970.2905 - val_acc: 0.2623\n",
+      "Epoch 7/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 88247.4007 - acc: 0.2573 - val_loss: 971.8534 - val_acc: 0.2699\n",
+      "Epoch 8/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 2835533773.8017 - acc: 0.2463 - val_loss: 968.0490 - val_acc: 0.2752\n",
+      "Epoch 9/1000\n",
+      "7471/7471 [==============================] - 2s 233us/sample - loss: 67992541.2407 - acc: 0.2523 - val_loss: 966.6333 - val_acc: 0.2691\n",
+      "Epoch 10/1000\n",
+      "7471/7471 [==============================] - 2s 230us/sample - loss: 2223346399.6794 - acc: 0.2577 - val_loss: 957.5425 - val_acc: 0.1933\n",
+      "Epoch 11/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 677614077.6028 - acc: 0.2184 - val_loss: 967.0082 - val_acc: 0.2024\n",
+      "Epoch 12/1000\n",
+      "7471/7471 [==============================] - 2s 284us/sample - loss: 19061.3039 - acc: 0.2201 - val_loss: 955.8609 - val_acc: 0.2206\n",
+      "Epoch 13/1000\n",
+      "7471/7471 [==============================] - 2s 286us/sample - loss: 531733.8467 - acc: 0.2146 - val_loss: 964.7280 - val_acc: 0.1842\n",
+      "Epoch 14/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 181111954.3819 - acc: 0.2206 - val_loss: 965.2095 - val_acc: 0.2138\n",
+      "Epoch 15/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 1663646742.0231 - acc: 0.2306 - val_loss: 972.6969 - val_acc: 0.2130\n",
+      "Epoch 16/1000\n",
+      "7471/7471 [==============================] - 2s 296us/sample - loss: 12925.7453 - acc: 0.2286 - val_loss: 965.6844 - val_acc: 0.2161\n",
+      "Epoch 17/1000\n",
+      "7471/7471 [==============================] - 2s 286us/sample - loss: 8929.3105 - acc: 0.2188 - val_loss: 965.0510 - val_acc: 0.2115\n",
+      "Epoch 18/1000\n",
+      "7471/7471 [==============================] - 2s 312us/sample - loss: 819834.7695 - acc: 0.2340 - val_loss: 963.8153 - val_acc: 0.2252\n",
+      "Epoch 19/1000\n",
+      "7471/7471 [==============================] - 2s 282us/sample - loss: 153234911.4716 - acc: 0.2179 - val_loss: 970.6586 - val_acc: 0.1926\n",
+      "Epoch 20/1000\n",
+      "7471/7471 [==============================] - 2s 299us/sample - loss: 1418889820.3060 - acc: 0.2255 - val_loss: 965.7134 - val_acc: 0.1941\n",
+      "Epoch 21/1000\n",
+      "7471/7471 [==============================] - 2s 302us/sample - loss: 22298.8516 - acc: 0.2227 - val_loss: 965.9686 - val_acc: 0.2100\n",
+      "Epoch 22/1000\n",
+      "7471/7471 [==============================] - 2s 302us/sample - loss: 2427745235.4104 - acc: 0.2300 - val_loss: 964.0780 - val_acc: 0.1948\n",
+      "Epoch 23/1000\n",
+      "7471/7471 [==============================] - 2s 307us/sample - loss: 30628.5129 - acc: 0.2298 - val_loss: 949.3742 - val_acc: 0.2009\n",
+      "Epoch 24/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 109164.3162 - acc: 0.2251 - val_loss: 964.5190 - val_acc: 0.1842\n",
+      "Epoch 25/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 3395440.6538 - acc: 0.2190 - val_loss: 958.0530 - val_acc: 0.1941\n",
+      "Epoch 26/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 324618.6660 - acc: 0.2250 - val_loss: 958.1312 - val_acc: 0.1857\n",
+      "Epoch 27/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 8318.7460 - acc: 0.2230 - val_loss: 951.8995 - val_acc: 0.1850\n",
+      "Epoch 28/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 285741.2649 - acc: 0.2320 - val_loss: 950.4407 - val_acc: 0.2009\n",
+      "Epoch 29/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 11101.0374 - acc: 0.2282 - val_loss: 946.1563 - val_acc: 0.2092\n",
+      "Epoch 30/1000\n",
+      "7471/7471 [==============================] - 2s 281us/sample - loss: 30014.0049 - acc: 0.2357 - val_loss: 927.1705 - val_acc: 0.2146\n",
+      "Epoch 31/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 2691.3318 - acc: 0.2336 - val_loss: 932.0998 - val_acc: 0.2199\n",
+      "Epoch 32/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 265009.6929 - acc: 0.2304 - val_loss: 935.6848 - val_acc: 0.2009\n",
+      "Epoch 33/1000\n",
+      "7471/7471 [==============================] - 2s 284us/sample - loss: 119791.2663 - acc: 0.2201 - val_loss: 933.0894 - val_acc: 0.1933\n",
+      "Epoch 34/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 3731.6630 - acc: 0.2234 - val_loss: 899.3829 - val_acc: 0.2138\n",
+      "Epoch 35/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 9930.3121 - acc: 0.2279 - val_loss: 916.9178 - val_acc: 0.2070\n",
+      "Epoch 36/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 30833.6726 - acc: 0.2247 - val_loss: 908.6571 - val_acc: 0.2092\n",
+      "Epoch 37/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 4965.0711 - acc: 0.2214 - val_loss: 909.5387 - val_acc: 0.1941\n",
+      "Epoch 38/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 35235.4809 - acc: 0.2239 - val_loss: 900.6075 - val_acc: 0.2039\n",
+      "Epoch 39/1000\n",
+      "7471/7471 [==============================] - 2s 280us/sample - loss: 2891.2058 - acc: 0.2219 - val_loss: 912.0464 - val_acc: 0.1911\n",
+      "Epoch 40/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 3553.0746 - acc: 0.2162 - val_loss: 888.1503 - val_acc: 0.1986\n",
+      "Epoch 41/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 2608.8444 - acc: 0.2131 - val_loss: 894.9386 - val_acc: 0.2017\n",
+      "Epoch 42/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 2150.2409 - acc: 0.2175 - val_loss: 869.8267 - val_acc: 0.2062\n",
+      "Epoch 43/1000\n",
+      "7471/7471 [==============================] - 2s 254us/sample - loss: 2712.8297 - acc: 0.2116 - val_loss: 896.2583 - val_acc: 0.2055\n",
+      "Epoch 44/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 321953878.5333 - acc: 0.2108 - val_loss: 892.6316 - val_acc: 0.1948\n",
+      "Epoch 45/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 3772.9586 - acc: 0.2069 - val_loss: 904.7227 - val_acc: 0.1835\n",
+      "Epoch 46/1000\n",
+      "7471/7471 [==============================] - 2s 273us/sample - loss: 5928.4395 - acc: 0.2071 - val_loss: 898.9772 - val_acc: 0.1706\n",
+      "Epoch 47/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 3440.2498 - acc: 0.2100 - val_loss: 869.4396 - val_acc: 0.1873\n",
+      "Epoch 48/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 2816.6128 - acc: 0.2123 - val_loss: 873.4861 - val_acc: 0.1827\n",
+      "Epoch 49/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 5820635111.6788 - acc: 0.2101 - val_loss: 887.2368 - val_acc: 0.1964\n",
+      "Epoch 50/1000\n",
+      "7471/7471 [==============================] - 2s 261us/sample - loss: 3708.9757 - acc: 0.2067 - val_loss: 885.8793 - val_acc: 0.1933\n",
+      "Epoch 51/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 2645.5457 - acc: 0.2080 - val_loss: 885.7632 - val_acc: 0.1926\n",
+      "Epoch 52/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 2812.8496 - acc: 0.2039 - val_loss: 860.2358 - val_acc: 0.1880\n",
+      "Epoch 53/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 132924944.8561 - acc: 0.2092 - val_loss: 879.3937 - val_acc: 0.1895\n",
+      "Epoch 54/1000\n",
+      "7471/7471 [==============================] - 2s 246us/sample - loss: 9932.6697 - acc: 0.2130 - val_loss: 867.6655 - val_acc: 0.1941\n",
+      "Epoch 55/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 17265.6206 - acc: 0.2108 - val_loss: 846.3767 - val_acc: 0.1941\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 56/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 6886.7095 - acc: 0.2072 - val_loss: 852.5446 - val_acc: 0.1964\n",
+      "Epoch 57/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 41731.8435 - acc: 0.2118 - val_loss: 883.2731 - val_acc: 0.2055\n",
+      "Epoch 58/1000\n",
+      "7471/7471 [==============================] - 2s 261us/sample - loss: 3366.7640 - acc: 0.2071 - val_loss: 860.8715 - val_acc: 0.1994\n",
+      "Epoch 59/1000\n",
+      "7471/7471 [==============================] - 2s 283us/sample - loss: 3505.4498 - acc: 0.2057 - val_loss: 837.6285 - val_acc: 0.1979\n",
+      "Epoch 60/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2856.5260 - acc: 0.2028 - val_loss: 836.0506 - val_acc: 0.1918\n",
+      "Epoch 61/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 2860.5496 - acc: 0.2044 - val_loss: 838.2833 - val_acc: 0.1941\n",
+      "Epoch 62/1000\n",
+      "7471/7471 [==============================] - 3s 357us/sample - loss: 27029.6371 - acc: 0.2033 - val_loss: 842.9126 - val_acc: 0.1979\n",
+      "Epoch 63/1000\n",
+      "7471/7471 [==============================] - 3s 353us/sample - loss: 21175.2355 - acc: 0.1954 - val_loss: 856.1320 - val_acc: 0.1895\n",
+      "Epoch 64/1000\n",
+      "7471/7471 [==============================] - 3s 360us/sample - loss: 2450.1694 - acc: 0.2017 - val_loss: 832.6260 - val_acc: 0.1895\n",
+      "Epoch 65/1000\n",
+      "7471/7471 [==============================] - 2s 260us/sample - loss: 2887.1265 - acc: 0.1938 - val_loss: 861.9637 - val_acc: 0.1857\n",
+      "Epoch 66/1000\n",
+      "7471/7471 [==============================] - 2s 256us/sample - loss: 2630.1915 - acc: 0.1887 - val_loss: 845.6191 - val_acc: 0.1926\n",
+      "Epoch 67/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 3137.0601 - acc: 0.1944 - val_loss: 843.3904 - val_acc: 0.1933\n",
+      "Epoch 68/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 21508.5255 - acc: 0.1966 - val_loss: 845.1986 - val_acc: 0.1789\n",
+      "Epoch 69/1000\n",
+      "7471/7471 [==============================] - 2s 285us/sample - loss: 2961.0960 - acc: 0.1958 - val_loss: 827.1030 - val_acc: 0.1820\n",
+      "Epoch 70/1000\n",
+      "7471/7471 [==============================] - 2s 292us/sample - loss: 2908.6702 - acc: 0.1917 - val_loss: 840.1124 - val_acc: 0.1668\n",
+      "Epoch 71/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 12695227604.8068 - acc: 0.1895 - val_loss: 886.5146 - val_acc: 0.1827\n",
+      "Epoch 72/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 2182.2938 - acc: 0.1945 - val_loss: 846.1136 - val_acc: 0.1736\n",
+      "Epoch 73/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 43800.9100 - acc: 0.1902 - val_loss: 857.3590 - val_acc: 0.1751\n",
+      "Epoch 74/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2301.3333 - acc: 0.1925 - val_loss: 903.2619 - val_acc: 0.2123\n",
+      "Epoch 75/1000\n",
+      "7471/7471 [==============================] - 2s 260us/sample - loss: 39798900.9997 - acc: 0.1969 - val_loss: 848.7530 - val_acc: 0.1827\n",
+      "Epoch 76/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 2749.4190 - acc: 0.1942 - val_loss: 828.8689 - val_acc: 0.1850\n",
+      "Epoch 77/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 3593.1665 - acc: 0.1897 - val_loss: 837.8488 - val_acc: 0.1797\n",
+      "Epoch 78/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 2836.3085 - acc: 0.1883 - val_loss: 849.4205 - val_acc: 0.1797\n",
+      "Epoch 79/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 8379.7924 - acc: 0.1879 - val_loss: 834.9620 - val_acc: 0.1888\n",
+      "Epoch 80/1000\n",
+      "7471/7471 [==============================] - 2s 281us/sample - loss: 2669617444.6145 - acc: 0.1945 - val_loss: 833.8868 - val_acc: 0.1835\n",
+      "Epoch 81/1000\n",
+      "7471/7471 [==============================] - 2s 256us/sample - loss: 7949.3898 - acc: 0.1843 - val_loss: 842.4512 - val_acc: 0.1827\n",
+      "Epoch 82/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 2755.2694 - acc: 0.1831 - val_loss: 812.2003 - val_acc: 0.1850\n",
+      "Epoch 83/1000\n",
+      "7471/7471 [==============================] - 2s 257us/sample - loss: 2024.1761 - acc: 0.1842 - val_loss: 826.2697 - val_acc: 0.1835\n",
+      "Epoch 84/1000\n",
+      "7471/7471 [==============================] - 2s 273us/sample - loss: 2538.0989 - acc: 0.1832 - val_loss: 842.2445 - val_acc: 0.1713\n",
+      "Epoch 85/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 2657.5391 - acc: 0.1867 - val_loss: 828.1099 - val_acc: 0.1850\n",
+      "Epoch 86/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 2245.7715 - acc: 0.1823 - val_loss: 822.0260 - val_acc: 0.1804\n",
+      "Epoch 87/1000\n",
+      "7471/7471 [==============================] - 2s 257us/sample - loss: 1518.3494 - acc: 0.1874 - val_loss: 835.8628 - val_acc: 0.1804\n",
+      "Epoch 88/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 2690.1647 - acc: 0.1791 - val_loss: 834.3119 - val_acc: 0.1774\n",
+      "Epoch 89/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 2823.5662 - acc: 0.1823 - val_loss: 834.5671 - val_acc: 0.1812\n",
+      "Epoch 90/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2514.1553 - acc: 0.1832 - val_loss: 830.9386 - val_acc: 0.1812\n",
+      "Epoch 91/1000\n",
+      "7471/7471 [==============================] - 2s 260us/sample - loss: 2824.2322 - acc: 0.1808 - val_loss: 842.0350 - val_acc: 0.1812\n",
+      "Epoch 92/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 1927.7020 - acc: 0.1842 - val_loss: 826.2208 - val_acc: 0.1827\n",
+      "Epoch 93/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 2863.5849 - acc: 0.1835 - val_loss: 858.1267 - val_acc: 0.1744\n",
+      "Epoch 94/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 375143.0111 - acc: 0.1806 - val_loss: 835.9360 - val_acc: 0.1751\n",
+      "Epoch 95/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 2833.1080 - acc: 0.1834 - val_loss: 838.3933 - val_acc: 0.1782\n",
+      "Epoch 96/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 2777.7966 - acc: 0.1910 - val_loss: 845.1980 - val_acc: 0.1751\n",
+      "Epoch 97/1000\n",
+      "7471/7471 [==============================] - 2s 253us/sample - loss: 2836.0932 - acc: 0.1840 - val_loss: 847.4138 - val_acc: 0.1827\n",
+      "Epoch 98/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 2914.0356 - acc: 0.1846 - val_loss: 859.7673 - val_acc: 0.1729\n",
+      "Epoch 99/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 2764.6429 - acc: 0.1840 - val_loss: 846.4112 - val_acc: 0.1789\n",
+      "Epoch 100/1000\n",
+      "7471/7471 [==============================] - 2s 261us/sample - loss: 2801.5623 - acc: 0.1885 - val_loss: 870.0989 - val_acc: 0.1888\n",
+      "Epoch 101/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 2905.8682 - acc: 0.1902 - val_loss: 835.3355 - val_acc: 0.1804\n",
+      "Epoch 102/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 2631.6308 - acc: 0.1882 - val_loss: 846.5264 - val_acc: 0.1736\n",
+      "Epoch 103/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 198117.2803 - acc: 0.1831 - val_loss: 862.1808 - val_acc: 0.1774\n",
+      "Epoch 104/1000\n",
+      "7471/7471 [==============================] - 2s 247us/sample - loss: 190331.5868 - acc: 0.1794 - val_loss: 842.4786 - val_acc: 0.1774\n",
+      "Epoch 105/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 7980.5485 - acc: 0.1822 - val_loss: 838.4551 - val_acc: 0.1820\n",
+      "Epoch 106/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 3035.8115 - acc: 0.1808 - val_loss: 824.1161 - val_acc: 0.1804\n",
+      "Epoch 107/1000\n",
+      "7471/7471 [==============================] - 2s 280us/sample - loss: 2778.0869 - acc: 0.1835 - val_loss: 856.4762 - val_acc: 0.1766\n",
+      "Epoch 108/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 1719.7118 - acc: 0.1787 - val_loss: 864.4767 - val_acc: 0.1789\n",
+      "Epoch 109/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 1259.0162 - acc: 0.1815 - val_loss: 978.6443 - val_acc: 0.1865\n",
+      "Epoch 110/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 4069.5680 - acc: 0.1855 - val_loss: 845.4168 - val_acc: 0.1812\n",
+      "Epoch 111/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 2594.8139 - acc: 0.1882 - val_loss: 858.2755 - val_acc: 0.1820\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 112/1000\n",
+      "7471/7471 [==============================] - 2s 285us/sample - loss: 2244.5008 - acc: 0.1854 - val_loss: 885.7796 - val_acc: 0.1895\n",
+      "Epoch 113/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 2835.6333 - acc: 0.1867 - val_loss: 853.6291 - val_acc: 0.1873\n",
+      "Epoch 114/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 2135.0576 - acc: 0.1847 - val_loss: 857.5315 - val_acc: 0.1827\n",
+      "Epoch 115/1000\n",
+      "7471/7471 [==============================] - 2s 257us/sample - loss: 2342.4815 - acc: 0.2146 - val_loss: 874.7968 - val_acc: 0.2547\n",
+      "Epoch 116/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 3288.5065 - acc: 0.2563 - val_loss: 861.1310 - val_acc: 0.2517\n",
+      "Epoch 117/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 1296.3356 - acc: 0.2498 - val_loss: 854.3648 - val_acc: 0.2563\n",
+      "Epoch 118/1000\n",
+      "7471/7471 [==============================] - 2s 260us/sample - loss: 1056.7782 - acc: 0.2524 - val_loss: 863.8525 - val_acc: 0.2676\n",
+      "Epoch 119/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 2002.1661 - acc: 0.2518 - val_loss: 996.0010 - val_acc: 0.2616\n",
+      "Epoch 120/1000\n",
+      "7471/7471 [==============================] - 2s 273us/sample - loss: 37675.9250 - acc: 0.2467 - val_loss: 967.8766 - val_acc: 0.2563\n",
+      "Epoch 121/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 1991.5971 - acc: 0.2498 - val_loss: 883.4013 - val_acc: 0.2547\n",
+      "Epoch 122/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 1397.7544 - acc: 0.2486 - val_loss: 899.8608 - val_acc: 0.2600\n",
+      "Epoch 123/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 1097.9801 - acc: 0.2535 - val_loss: 931.0347 - val_acc: 0.2752\n",
+      "Epoch 124/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 2008.1891 - acc: 0.2516 - val_loss: 905.2368 - val_acc: 0.2608\n",
+      "Epoch 125/1000\n",
+      "7471/7471 [==============================] - 2s 256us/sample - loss: 2681.7456 - acc: 0.2499 - val_loss: 882.4463 - val_acc: 0.2578\n",
+      "Epoch 126/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2033.7989 - acc: 0.2486 - val_loss: 999.5306 - val_acc: 0.2555\n",
+      "Epoch 127/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 1085.3093 - acc: 0.2476 - val_loss: 874.5508 - val_acc: 0.2517\n",
+      "Epoch 128/1000\n",
+      "7471/7471 [==============================] - 2s 287us/sample - loss: 2823.8048 - acc: 0.2476 - val_loss: 870.0748 - val_acc: 0.2555\n",
+      "Epoch 129/1000\n",
+      "7471/7471 [==============================] - 2s 313us/sample - loss: 960.0944 - acc: 0.2512 - val_loss: 843.4890 - val_acc: 0.2631\n",
+      "Epoch 130/1000\n",
+      "7471/7471 [==============================] - 2s 301us/sample - loss: 2511.2417 - acc: 0.2531 - val_loss: 881.6280 - val_acc: 0.2532\n",
+      "Epoch 131/1000\n",
+      "7471/7471 [==============================] - 2s 302us/sample - loss: 2807.3163 - acc: 0.2488 - val_loss: 881.5549 - val_acc: 0.2585\n",
+      "Epoch 132/1000\n",
+      "7471/7471 [==============================] - 2s 296us/sample - loss: 2493.0268 - acc: 0.2432 - val_loss: 900.1216 - val_acc: 0.2472\n",
+      "Epoch 133/1000\n",
+      "7471/7471 [==============================] - 2s 305us/sample - loss: 223888.1254 - acc: 0.2451 - val_loss: 888.9924 - val_acc: 0.2479\n",
+      "Epoch 134/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 2734.2065 - acc: 0.2463 - val_loss: 886.3438 - val_acc: 0.2494\n",
+      "Epoch 135/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 1708.9961 - acc: 0.2479 - val_loss: 871.9773 - val_acc: 0.2517\n",
+      "Epoch 136/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2392.0933 - acc: 0.2503 - val_loss: 861.2906 - val_acc: 0.2532\n",
+      "Epoch 137/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 716.6152 - acc: 0.2523 - val_loss: 861.8834 - val_acc: 0.2547\n",
+      "Epoch 138/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 1656.2387 - acc: 0.2421 - val_loss: 890.4787 - val_acc: 0.2532\n",
+      "Epoch 139/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 3156.6738 - acc: 0.2478 - val_loss: 898.2603 - val_acc: 0.2532\n",
+      "Epoch 140/1000\n",
+      "7471/7471 [==============================] - 2s 261us/sample - loss: 2622.2035 - acc: 0.2478 - val_loss: 852.6692 - val_acc: 0.2525\n",
+      "Epoch 141/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 2472.0329 - acc: 0.2492 - val_loss: 868.7198 - val_acc: 0.2532\n",
+      "Epoch 142/1000\n",
+      "7471/7471 [==============================] - 2s 253us/sample - loss: 2194.4713 - acc: 0.2470 - val_loss: 914.2988 - val_acc: 0.2555\n",
+      "Epoch 143/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 1410.7340 - acc: 0.2480 - val_loss: 889.2578 - val_acc: 0.2509\n",
+      "Epoch 144/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 735.9119 - acc: 0.2466 - val_loss: 892.7234 - val_acc: 0.2472\n",
+      "Epoch 145/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 340134710.6453 - acc: 0.2514 - val_loss: 894.4550 - val_acc: 0.2441\n",
+      "Epoch 146/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 1634.6042 - acc: 0.2421 - val_loss: 878.7720 - val_acc: 0.2449\n",
+      "Epoch 147/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2863.6263 - acc: 0.2431 - val_loss: 897.3246 - val_acc: 0.2373\n",
+      "Epoch 148/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 1330.4296 - acc: 0.2466 - val_loss: 894.9856 - val_acc: 0.2388\n",
+      "Epoch 149/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2792.9427 - acc: 0.2428 - val_loss: 885.6642 - val_acc: 0.2381\n",
+      "Epoch 150/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 2339.3152 - acc: 0.2436 - val_loss: 865.7564 - val_acc: 0.2381\n",
+      "Epoch 151/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 74439.1323 - acc: 0.2437 - val_loss: 870.1626 - val_acc: 0.2449\n",
+      "Epoch 152/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 2247.4615 - acc: 0.2494 - val_loss: 887.7957 - val_acc: 0.2350\n",
+      "Epoch 153/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 1107.8627 - acc: 0.2455 - val_loss: 905.8709 - val_acc: 0.2472\n",
+      "Epoch 154/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 2501.4770 - acc: 0.2527 - val_loss: 1071.9249 - val_acc: 0.2418\n",
+      "Epoch 155/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 4705.4834 - acc: 0.2417 - val_loss: 920.0807 - val_acc: 0.2388\n",
+      "Epoch 156/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 1689.7381 - acc: 0.2458 - val_loss: 939.2820 - val_acc: 0.2396\n",
+      "Epoch 157/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 4650.7669 - acc: 0.2455 - val_loss: 935.1336 - val_acc: 0.2403\n",
+      "Epoch 158/1000\n",
+      "7471/7471 [==============================] - 2s 261us/sample - loss: 9948.1597 - acc: 0.2409 - val_loss: 941.3551 - val_acc: 0.2434\n",
+      "Epoch 159/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 1399.6638 - acc: 0.2395 - val_loss: 924.2069 - val_acc: 0.2418\n",
+      "Epoch 160/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 30953408291.1504 - acc: 0.2467 - val_loss: 929.2987 - val_acc: 0.2434\n",
+      "Epoch 161/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 1994.8542 - acc: 0.2451 - val_loss: 942.6771 - val_acc: 0.2403\n",
+      "Epoch 162/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 637.3415 - acc: 0.2388 - val_loss: 955.0109 - val_acc: 0.2418\n",
+      "Epoch 163/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 858.2982 - acc: 0.2437 - val_loss: 931.1982 - val_acc: 0.2487\n",
+      "Epoch 164/1000\n",
+      "7471/7471 [==============================] - 2s 261us/sample - loss: 2818.8302 - acc: 0.2439 - val_loss: 921.8822 - val_acc: 0.2426\n",
+      "Epoch 165/1000\n",
+      "7471/7471 [==============================] - 2s 280us/sample - loss: 817.6053 - acc: 0.2421 - val_loss: 951.7699 - val_acc: 0.2396\n",
+      "Epoch 166/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 2575.2956 - acc: 0.2404 - val_loss: 957.3188 - val_acc: 0.2426\n",
+      "Epoch 167/1000\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 3010.0305 - acc: 0.2419 - val_loss: 928.0657 - val_acc: 0.2449\n",
+      "Epoch 168/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 1827.5690 - acc: 0.2423 - val_loss: 910.2143 - val_acc: 0.2532\n",
+      "Epoch 169/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 2482.1105 - acc: 0.2453 - val_loss: 1016.7034 - val_acc: 0.2441\n",
+      "Epoch 170/1000\n",
+      "7471/7471 [==============================] - 2s 278us/sample - loss: 733.1875 - acc: 0.2427 - val_loss: 902.1205 - val_acc: 0.2403\n",
+      "Epoch 171/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 2417.6740 - acc: 0.2424 - val_loss: 916.9220 - val_acc: 0.2426\n",
+      "Epoch 172/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 2824.1067 - acc: 0.2427 - val_loss: 900.4588 - val_acc: 0.2381\n",
+      "Epoch 173/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2623.1395 - acc: 0.2417 - val_loss: 892.9856 - val_acc: 0.2358\n",
+      "Epoch 174/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 2269.5282 - acc: 0.2372 - val_loss: 920.1757 - val_acc: 0.2350\n",
+      "Epoch 175/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 1695.3801 - acc: 0.2433 - val_loss: 896.1177 - val_acc: 0.2403\n",
+      "Epoch 176/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 973.3781 - acc: 0.2416 - val_loss: 934.3664 - val_acc: 0.2365\n",
+      "Epoch 177/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 1048.5156 - acc: 0.2437 - val_loss: 921.7613 - val_acc: 0.2358\n",
+      "Epoch 178/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 242983751.2500 - acc: 0.2373 - val_loss: 937.4612 - val_acc: 0.2373\n",
+      "Epoch 179/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 2770.1159 - acc: 0.2399 - val_loss: 915.2948 - val_acc: 0.2365\n",
+      "Epoch 180/1000\n",
+      "7471/7471 [==============================] - 2s 261us/sample - loss: 2648.1181 - acc: 0.2411 - val_loss: 900.5435 - val_acc: 0.2434\n",
+      "Epoch 181/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 2779.9257 - acc: 0.2415 - val_loss: 896.7728 - val_acc: 0.2381\n",
+      "Epoch 182/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 2509.9034 - acc: 0.2443 - val_loss: 895.5264 - val_acc: 0.2449\n",
+      "Epoch 183/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 15399417983.7471 - acc: 0.2458 - val_loss: 907.1219 - val_acc: 0.2381\n",
+      "Epoch 184/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 2583.4751 - acc: 0.2403 - val_loss: 921.2360 - val_acc: 0.2388\n",
+      "Epoch 185/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 2837.4932 - acc: 0.2388 - val_loss: 917.6852 - val_acc: 0.2388\n",
+      "Epoch 186/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 2031.2580 - acc: 0.2417 - val_loss: 926.6921 - val_acc: 0.2403\n",
+      "Epoch 187/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 2401.9160 - acc: 0.2413 - val_loss: 901.5898 - val_acc: 0.2411\n",
+      "Epoch 188/1000\n",
+      "7471/7471 [==============================] - 2s 273us/sample - loss: 707.5512 - acc: 0.2448 - val_loss: 890.5833 - val_acc: 0.2426\n",
+      "Epoch 189/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 625.2530 - acc: 0.2439 - val_loss: 911.8019 - val_acc: 0.2403\n",
+      "Epoch 190/1000\n",
+      "7471/7471 [==============================] - 2s 284us/sample - loss: 2649.0386 - acc: 0.2409 - val_loss: 916.1107 - val_acc: 0.2365\n",
+      "Epoch 191/1000\n",
+      "7471/7471 [==============================] - 2s 279us/sample - loss: 2642.1941 - acc: 0.2387 - val_loss: 920.8549 - val_acc: 0.2449\n",
+      "Epoch 192/1000\n",
+      "7471/7471 [==============================] - 2s 292us/sample - loss: 2429.3751 - acc: 0.2376 - val_loss: 904.8347 - val_acc: 0.2434\n",
+      "Epoch 193/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 1343.7030 - acc: 0.2387 - val_loss: 906.0586 - val_acc: 0.2403\n",
+      "Epoch 194/1000\n",
+      "7471/7471 [==============================] - 2s 298us/sample - loss: 608.5217 - acc: 0.2387 - val_loss: 902.5253 - val_acc: 0.2434\n",
+      "Epoch 195/1000\n",
+      "7471/7471 [==============================] - 2s 282us/sample - loss: 1666.3937 - acc: 0.2400 - val_loss: 942.1149 - val_acc: 0.2418\n",
+      "Epoch 196/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 2870.1974 - acc: 0.2403 - val_loss: 913.2829 - val_acc: 0.2388\n",
+      "Epoch 197/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 1610.1504 - acc: 0.2364 - val_loss: 941.2345 - val_acc: 0.2320\n",
+      "Epoch 198/1000\n",
+      "7471/7471 [==============================] - 2s 279us/sample - loss: 3582.6075 - acc: 0.2358 - val_loss: 945.3063 - val_acc: 0.2343\n",
+      "Epoch 199/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 2608.0605 - acc: 0.2379 - val_loss: 922.1862 - val_acc: 0.2358\n",
+      "Epoch 200/1000\n",
+      "7471/7471 [==============================] - 2s 278us/sample - loss: 623.6204 - acc: 0.2372 - val_loss: 921.5983 - val_acc: 0.2343\n",
+      "Epoch 201/1000\n",
+      "7471/7471 [==============================] - 2s 287us/sample - loss: 2212.0106 - acc: 0.2387 - val_loss: 914.1040 - val_acc: 0.2320\n",
+      "Epoch 202/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 2314.9301 - acc: 0.2348 - val_loss: 914.4429 - val_acc: 0.2312\n",
+      "Epoch 203/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 1767.9038 - acc: 0.2362 - val_loss: 937.8283 - val_acc: 0.2343\n",
+      "Epoch 204/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 2817.0376 - acc: 0.2344 - val_loss: 945.3943 - val_acc: 0.2320\n",
+      "Epoch 205/1000\n",
+      "7471/7471 [==============================] - 2s 238us/sample - loss: 1036.4583 - acc: 0.2326 - val_loss: 945.2571 - val_acc: 0.2358\n",
+      "Epoch 206/1000\n",
+      "7471/7471 [==============================] - 2s 257us/sample - loss: 2834.4749 - acc: 0.2325 - val_loss: 955.4956 - val_acc: 0.2320\n",
+      "Epoch 207/1000\n",
+      "7471/7471 [==============================] - 2s 208us/sample - loss: 699.4824 - acc: 0.2358 - val_loss: 917.4992 - val_acc: 0.2335\n",
+      "Epoch 208/1000\n",
+      "7471/7471 [==============================] - 2s 246us/sample - loss: 2689.9635 - acc: 0.2346 - val_loss: 937.7088 - val_acc: 0.2267\n",
+      "Epoch 209/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 2304.1817 - acc: 0.2314 - val_loss: 951.4307 - val_acc: 0.2267\n",
+      "Epoch 210/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2901.4316 - acc: 0.2336 - val_loss: 965.4147 - val_acc: 0.2244\n",
+      "Epoch 211/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 1518.3748 - acc: 0.2314 - val_loss: 953.8307 - val_acc: 0.2312\n",
+      "Epoch 212/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 1810.7731 - acc: 0.2297 - val_loss: 957.0162 - val_acc: 0.2297\n",
+      "Epoch 213/1000\n",
+      "7471/7471 [==============================] - 2s 260us/sample - loss: 2322.0461 - acc: 0.2304 - val_loss: 957.4329 - val_acc: 0.2274\n",
+      "Epoch 214/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 2680.8297 - acc: 0.2265 - val_loss: 963.3393 - val_acc: 0.2206\n",
+      "Epoch 215/1000\n",
+      "7471/7471 [==============================] - 2s 257us/sample - loss: 2238.8633 - acc: 0.2290 - val_loss: 955.0368 - val_acc: 0.2237\n",
+      "Epoch 216/1000\n",
+      "7471/7471 [==============================] - 2s 295us/sample - loss: 1830.5201 - acc: 0.2320 - val_loss: 944.4840 - val_acc: 0.2305\n",
+      "Epoch 217/1000\n",
+      "7471/7471 [==============================] - 2s 287us/sample - loss: 999.2122 - acc: 0.2294 - val_loss: 950.3318 - val_acc: 0.2282\n",
+      "Epoch 218/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 2326.7224 - acc: 0.2283 - val_loss: 963.8691 - val_acc: 0.2252\n",
+      "Epoch 219/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 879.9751 - acc: 0.2262 - val_loss: 952.6518 - val_acc: 0.2274\n",
+      "Epoch 220/1000\n",
+      "7471/7471 [==============================] - 2s 281us/sample - loss: 2884.3691 - acc: 0.2286 - val_loss: 961.8074 - val_acc: 0.2267\n",
+      "Epoch 221/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 3034.3946 - acc: 0.2246 - val_loss: 964.2877 - val_acc: 0.2252\n",
+      "Epoch 222/1000\n",
+      "7471/7471 [==============================] - 2s 253us/sample - loss: 2807.9917 - acc: 0.2255 - val_loss: 965.4002 - val_acc: 0.2221\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 223/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2084.4215 - acc: 0.2270 - val_loss: 965.6008 - val_acc: 0.2244\n",
+      "Epoch 224/1000\n",
+      "7471/7471 [==============================] - 2s 278us/sample - loss: 2833.7456 - acc: 0.2269 - val_loss: 964.4674 - val_acc: 0.2252\n",
+      "Epoch 225/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 1245.8151 - acc: 0.2267 - val_loss: 959.0973 - val_acc: 0.2290\n",
+      "Epoch 226/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 2912.7208 - acc: 0.2288 - val_loss: 956.0221 - val_acc: 0.2297\n",
+      "Epoch 227/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 1927.3322 - acc: 0.2320 - val_loss: 950.9079 - val_acc: 0.2312\n",
+      "Epoch 228/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 830.2423 - acc: 0.2309 - val_loss: 965.3707 - val_acc: 0.2267\n",
+      "Epoch 229/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 2732.5053 - acc: 0.2309 - val_loss: 949.9988 - val_acc: 0.2320\n",
+      "Epoch 230/1000\n",
+      "7471/7471 [==============================] - 2s 205us/sample - loss: 2849.8226 - acc: 0.2301 - val_loss: 964.3353 - val_acc: 0.2274\n",
+      "Epoch 231/1000\n",
+      "7471/7471 [==============================] - 2s 222us/sample - loss: 2801.6673 - acc: 0.2292 - val_loss: 965.1819 - val_acc: 0.2199\n",
+      "Epoch 232/1000\n",
+      "7471/7471 [==============================] - 2s 250us/sample - loss: 2113.9795 - acc: 0.2296 - val_loss: 948.3802 - val_acc: 0.2388\n",
+      "Epoch 233/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 2832.3249 - acc: 0.2312 - val_loss: 960.7617 - val_acc: 0.2328\n",
+      "Epoch 234/1000\n",
+      "7471/7471 [==============================] - 2s 283us/sample - loss: 2196.9541 - acc: 0.2317 - val_loss: 950.5317 - val_acc: 0.2350\n",
+      "Epoch 235/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 1548.2305 - acc: 0.2289 - val_loss: 947.1048 - val_acc: 0.2335\n",
+      "Epoch 236/1000\n",
+      "7471/7471 [==============================] - 2s 237us/sample - loss: 2216.7657 - acc: 0.2300 - val_loss: 953.7348 - val_acc: 0.2343\n",
+      "Epoch 237/1000\n",
+      "7471/7471 [==============================] - 2s 282us/sample - loss: 2091.4906 - acc: 0.2336 - val_loss: 942.3584 - val_acc: 0.2290\n",
+      "Epoch 238/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 2772.7024 - acc: 0.2332 - val_loss: 946.4086 - val_acc: 0.2328\n",
+      "Epoch 239/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 2986.7596 - acc: 0.2298 - val_loss: 957.1928 - val_acc: 0.2290\n",
+      "Epoch 240/1000\n",
+      "7471/7471 [==============================] - 2s 256us/sample - loss: 571.4869 - acc: 0.2302 - val_loss: 951.8489 - val_acc: 0.2305\n",
+      "Epoch 241/1000\n",
+      "7471/7471 [==============================] - 2s 256us/sample - loss: 1170.5337 - acc: 0.2324 - val_loss: 923.7938 - val_acc: 0.2350\n",
+      "Epoch 242/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 2361.3979 - acc: 0.2320 - val_loss: 941.5197 - val_acc: 0.2335\n",
+      "Epoch 243/1000\n",
+      "7471/7471 [==============================] - 2s 245us/sample - loss: 2823.0309 - acc: 0.2313 - val_loss: 939.2530 - val_acc: 0.2350\n",
+      "Epoch 244/1000\n",
+      "7471/7471 [==============================] - 2s 236us/sample - loss: 1717.3945 - acc: 0.2330 - val_loss: 912.7763 - val_acc: 0.2335\n",
+      "Epoch 245/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 2398.1073 - acc: 0.2325 - val_loss: 940.6725 - val_acc: 0.2297\n",
+      "Epoch 246/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 1268.1175 - acc: 0.2306 - val_loss: 928.6536 - val_acc: 0.2350\n",
+      "Epoch 247/1000\n",
+      "7471/7471 [==============================] - 2s 283us/sample - loss: 2387.2240 - acc: 0.2333 - val_loss: 947.3687 - val_acc: 0.2320\n",
+      "Epoch 248/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 1605.4384 - acc: 0.2324 - val_loss: 943.0392 - val_acc: 0.2328\n",
+      "Epoch 249/1000\n",
+      "7471/7471 [==============================] - 2s 273us/sample - loss: 2801.7796 - acc: 0.2308 - val_loss: 947.4987 - val_acc: 0.2312\n",
+      "Epoch 250/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 812.4723 - acc: 0.2306 - val_loss: 925.4634 - val_acc: 0.2350\n",
+      "Epoch 251/1000\n",
+      "7471/7471 [==============================] - 2s 250us/sample - loss: 2938.5224 - acc: 0.2322 - val_loss: 935.5196 - val_acc: 0.2358\n",
+      "Epoch 252/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 2478.3234 - acc: 0.2322 - val_loss: 960.2717 - val_acc: 0.2274\n",
+      "Epoch 253/1000\n",
+      "7471/7471 [==============================] - 2s 304us/sample - loss: 676.9818 - acc: 0.2286 - val_loss: 951.3141 - val_acc: 0.2328\n",
+      "Epoch 254/1000\n",
+      "7471/7471 [==============================] - 2s 300us/sample - loss: 1732.4002 - acc: 0.2296 - val_loss: 965.0492 - val_acc: 0.2229\n",
+      "Epoch 255/1000\n",
+      "7471/7471 [==============================] - 2s 301us/sample - loss: 2772.8647 - acc: 0.2365 - val_loss: 964.2185 - val_acc: 0.2934\n",
+      "Epoch 256/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 1067.7834 - acc: 0.3037 - val_loss: 961.9214 - val_acc: 0.2934\n",
+      "Epoch 257/1000\n",
+      "7471/7471 [==============================] - 2s 281us/sample - loss: 2387.1261 - acc: 0.3030 - val_loss: 951.5194 - val_acc: 0.2934\n",
+      "Epoch 258/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 2758.5058 - acc: 0.3030 - val_loss: 943.6249 - val_acc: 0.2934\n",
+      "Epoch 259/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 1703.9546 - acc: 0.3030 - val_loss: 925.5991 - val_acc: 0.2934\n",
+      "Epoch 260/1000\n",
+      "7471/7471 [==============================] - 2s 292us/sample - loss: 2158.4005 - acc: 0.3033 - val_loss: 948.7080 - val_acc: 0.2934\n",
+      "Epoch 261/1000\n",
+      "7471/7471 [==============================] - 2s 304us/sample - loss: 1373.6862 - acc: 0.3033 - val_loss: 940.1926 - val_acc: 0.2934\n",
+      "Epoch 262/1000\n",
+      "7471/7471 [==============================] - 2s 298us/sample - loss: 2557.2396 - acc: 0.3021 - val_loss: 941.8655 - val_acc: 0.2934\n",
+      "Epoch 263/1000\n",
+      "7471/7471 [==============================] - 2s 310us/sample - loss: 2986.8583 - acc: 0.3033 - val_loss: 948.4969 - val_acc: 0.2934\n",
+      "Epoch 264/1000\n",
+      "7471/7471 [==============================] - 2s 306us/sample - loss: 1520.9251 - acc: 0.3026 - val_loss: 946.3689 - val_acc: 0.2934\n",
+      "Epoch 265/1000\n",
+      "7471/7471 [==============================] - 2s 305us/sample - loss: 1203.7550 - acc: 0.3030 - val_loss: 941.3471 - val_acc: 0.2934\n",
+      "Epoch 266/1000\n",
+      "7471/7471 [==============================] - 2s 301us/sample - loss: 2834.0364 - acc: 0.3036 - val_loss: 940.1258 - val_acc: 0.2934\n",
+      "Epoch 267/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 702.6372 - acc: 0.3029 - val_loss: 941.7728 - val_acc: 0.2934\n",
+      "Epoch 268/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 3036.0497 - acc: 0.3030 - val_loss: 944.0649 - val_acc: 0.2934\n",
+      "Epoch 269/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 2778.2479 - acc: 0.3033 - val_loss: 942.8930 - val_acc: 0.2934\n",
+      "Epoch 270/1000\n",
+      "7471/7471 [==============================] - 2s 303us/sample - loss: 1841.7584 - acc: 0.3037 - val_loss: 949.6973 - val_acc: 0.2934\n",
+      "Epoch 271/1000\n",
+      "7471/7471 [==============================] - 2s 280us/sample - loss: 1265.6524 - acc: 0.3038 - val_loss: 933.0072 - val_acc: 0.2934\n",
+      "Epoch 272/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 1486.7030 - acc: 0.3030 - val_loss: 940.8006 - val_acc: 0.2934\n",
+      "Epoch 273/1000\n",
+      "7471/7471 [==============================] - 2s 252us/sample - loss: 3504.5106 - acc: 0.3036 - val_loss: 945.8965 - val_acc: 0.2934\n",
+      "Epoch 274/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 2739.6262 - acc: 0.3037 - val_loss: 944.2942 - val_acc: 0.2934\n",
+      "Epoch 275/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 2787.4650 - acc: 0.3040 - val_loss: 945.5869 - val_acc: 0.2934\n",
+      "Epoch 276/1000\n",
+      "7471/7471 [==============================] - 2s 248us/sample - loss: 2212.0306 - acc: 0.3036 - val_loss: 935.0067 - val_acc: 0.2934\n",
+      "Epoch 277/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 1728.2755 - acc: 0.3030 - val_loss: 933.7707 - val_acc: 0.2934\n",
+      "Epoch 278/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 2406.8720 - acc: 0.3034 - val_loss: 930.7149 - val_acc: 0.2934\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 279/1000\n",
+      "7471/7471 [==============================] - 2s 285us/sample - loss: 1051.3045 - acc: 0.3037 - val_loss: 931.4472 - val_acc: 0.2934\n",
+      "Epoch 280/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 2782.2430 - acc: 0.3036 - val_loss: 922.7016 - val_acc: 0.2934\n",
+      "Epoch 281/1000\n",
+      "7471/7471 [==============================] - 2s 248us/sample - loss: 2789.8032 - acc: 0.3028 - val_loss: 911.6979 - val_acc: 0.2934\n",
+      "Epoch 282/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 797.8861 - acc: 0.3032 - val_loss: 912.8252 - val_acc: 0.2934\n",
+      "Epoch 283/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 3037.5594 - acc: 0.3036 - val_loss: 925.7509 - val_acc: 0.2934\n",
+      "Epoch 284/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 2784.6085 - acc: 0.3033 - val_loss: 922.8460 - val_acc: 0.2934\n",
+      "Epoch 285/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 2623.1752 - acc: 0.3037 - val_loss: 901.9208 - val_acc: 0.2934\n",
+      "Epoch 286/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 1328.6847 - acc: 0.3040 - val_loss: 906.1689 - val_acc: 0.2934\n",
+      "Epoch 287/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 3052.6211 - acc: 0.3038 - val_loss: 917.6493 - val_acc: 0.2934\n",
+      "Epoch 288/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 8837.1289 - acc: 0.3036 - val_loss: 905.1851 - val_acc: 0.2934\n",
+      "Epoch 289/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 1178.8364 - acc: 0.3036 - val_loss: 904.2290 - val_acc: 0.2934\n",
+      "Epoch 290/1000\n",
+      "7471/7471 [==============================] - 2s 280us/sample - loss: 2726.0185 - acc: 0.3037 - val_loss: 883.6542 - val_acc: 0.2934\n",
+      "Epoch 291/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 2191.1238 - acc: 0.3036 - val_loss: 890.5722 - val_acc: 0.2934\n",
+      "Epoch 292/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 1591.1373 - acc: 0.3033 - val_loss: 895.9165 - val_acc: 0.2934\n",
+      "Epoch 293/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 1012.7293 - acc: 0.3032 - val_loss: 898.8408 - val_acc: 0.2934\n",
+      "Epoch 294/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 877.3253 - acc: 0.3034 - val_loss: 890.4005 - val_acc: 0.2934\n",
+      "Epoch 295/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 2700.2461 - acc: 0.3030 - val_loss: 905.5046 - val_acc: 0.2934\n",
+      "Epoch 296/1000\n",
+      "7471/7471 [==============================] - 2s 243us/sample - loss: 1293.1067 - acc: 0.3030 - val_loss: 897.2265 - val_acc: 0.2934\n",
+      "Epoch 297/1000\n",
+      "7471/7471 [==============================] - 2s 256us/sample - loss: 1872.8364 - acc: 0.3032 - val_loss: 897.8152 - val_acc: 0.2934\n",
+      "Epoch 298/1000\n",
+      "7471/7471 [==============================] - 2s 250us/sample - loss: 3346.8217 - acc: 0.3038 - val_loss: 905.0116 - val_acc: 0.2934\n",
+      "Epoch 299/1000\n",
+      "7471/7471 [==============================] - 2s 259us/sample - loss: 2721.6728 - acc: 0.3036 - val_loss: 903.3977 - val_acc: 0.2934\n",
+      "Epoch 300/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 1431.7701 - acc: 0.3037 - val_loss: 892.9695 - val_acc: 0.2934\n",
+      "Epoch 301/1000\n",
+      "7471/7471 [==============================] - 2s 252us/sample - loss: 88292.8417 - acc: 0.3036 - val_loss: 918.2786 - val_acc: 0.2934\n",
+      "Epoch 302/1000\n",
+      "7471/7471 [==============================] - 1s 196us/sample - loss: 1030.5475 - acc: 0.3037 - val_loss: 913.5758 - val_acc: 0.2934\n",
+      "Epoch 303/1000\n",
+      "7471/7471 [==============================] - 2s 211us/sample - loss: 2301.3605 - acc: 0.3040 - val_loss: 907.9592 - val_acc: 0.2934\n",
+      "Epoch 304/1000\n",
+      "7471/7471 [==============================] - 2s 242us/sample - loss: 1979.5570 - acc: 0.3034 - val_loss: 899.6837 - val_acc: 0.2934\n",
+      "Epoch 305/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 2754.2822 - acc: 0.3033 - val_loss: 902.8747 - val_acc: 0.2934\n",
+      "Epoch 306/1000\n",
+      "7471/7471 [==============================] - 2s 240us/sample - loss: 2146.3885 - acc: 0.3040 - val_loss: 908.7855 - val_acc: 0.2934\n",
+      "Epoch 307/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 1737.5970 - acc: 0.3038 - val_loss: 905.0738 - val_acc: 0.2934\n",
+      "Epoch 308/1000\n",
+      "7471/7471 [==============================] - 2s 246us/sample - loss: 3260.9349 - acc: 0.3037 - val_loss: 911.9584 - val_acc: 0.2934\n",
+      "Epoch 309/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 2272.3713 - acc: 0.3037 - val_loss: 913.5123 - val_acc: 0.2934\n",
+      "Epoch 310/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 2832.7112 - acc: 0.3034 - val_loss: 900.8477 - val_acc: 0.2934\n",
+      "Epoch 311/1000\n",
+      "7471/7471 [==============================] - 2s 275us/sample - loss: 827.4033 - acc: 0.3030 - val_loss: 892.7001 - val_acc: 0.2934\n",
+      "Epoch 312/1000\n",
+      "7471/7471 [==============================] - 2s 267us/sample - loss: 1126.0225 - acc: 0.3037 - val_loss: 895.7485 - val_acc: 0.2934\n",
+      "Epoch 313/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 2732.5601 - acc: 0.3033 - val_loss: 902.3338 - val_acc: 0.2934\n",
+      "Epoch 314/1000\n",
+      "7471/7471 [==============================] - 2s 280us/sample - loss: 1925.6757 - acc: 0.3033 - val_loss: 888.7004 - val_acc: 0.2934\n",
+      "Epoch 315/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 4337.8446 - acc: 0.3037 - val_loss: 900.5786 - val_acc: 0.2934\n",
+      "Epoch 316/1000\n",
+      "7471/7471 [==============================] - 2s 253us/sample - loss: 1947.8597 - acc: 0.3038 - val_loss: 897.6120 - val_acc: 0.2934\n",
+      "Epoch 317/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 1720.7626 - acc: 0.3038 - val_loss: 887.5366 - val_acc: 0.2934\n",
+      "Epoch 318/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 2770.4210 - acc: 0.3038 - val_loss: 895.6971 - val_acc: 0.2934\n",
+      "Epoch 319/1000\n",
+      "7471/7471 [==============================] - 2s 262us/sample - loss: 2485.5867 - acc: 0.3034 - val_loss: 877.2493 - val_acc: 0.2934\n",
+      "Epoch 320/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 1708.1658 - acc: 0.3038 - val_loss: 885.7611 - val_acc: 0.2934\n",
+      "Epoch 321/1000\n",
+      "7471/7471 [==============================] - 2s 288us/sample - loss: 1069.5243 - acc: 0.3034 - val_loss: 868.2874 - val_acc: 0.2934\n",
+      "Epoch 322/1000\n",
+      "7471/7471 [==============================] - 2s 292us/sample - loss: 1912.6557 - acc: 0.3038 - val_loss: 905.9554 - val_acc: 0.2934\n",
+      "Epoch 323/1000\n",
+      "7471/7471 [==============================] - 2s 308us/sample - loss: 1552.0434 - acc: 0.3033 - val_loss: 888.2266 - val_acc: 0.2934\n",
+      "Epoch 324/1000\n",
+      "7471/7471 [==============================] - 2s 299us/sample - loss: 2534.1126 - acc: 0.3036 - val_loss: 895.5842 - val_acc: 0.2934\n",
+      "Epoch 325/1000\n",
+      "7471/7471 [==============================] - 2s 298us/sample - loss: 2758.0945 - acc: 0.3034 - val_loss: 880.0583 - val_acc: 0.2934\n",
+      "Epoch 326/1000\n",
+      "7471/7471 [==============================] - 2s 282us/sample - loss: 2091.7897 - acc: 0.3033 - val_loss: 877.6848 - val_acc: 0.2934\n",
+      "Epoch 327/1000\n",
+      "7471/7471 [==============================] - 2s 285us/sample - loss: 2808.7181 - acc: 0.3036 - val_loss: 888.6634 - val_acc: 0.2934\n",
+      "Epoch 328/1000\n",
+      "7471/7471 [==============================] - 2s 295us/sample - loss: 2204.3161 - acc: 0.3032 - val_loss: 891.2527 - val_acc: 0.2934\n",
+      "Epoch 329/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 2760.5410 - acc: 0.3040 - val_loss: 882.7512 - val_acc: 0.2934\n",
+      "Epoch 330/1000\n",
+      "7471/7471 [==============================] - 2s 298us/sample - loss: 1913.9323 - acc: 0.3037 - val_loss: 881.2674 - val_acc: 0.2934\n",
+      "Epoch 331/1000\n",
+      "7471/7471 [==============================] - 2s 295us/sample - loss: 1557.3085 - acc: 0.3037 - val_loss: 875.2614 - val_acc: 0.2934\n",
+      "Epoch 332/1000\n",
+      "7471/7471 [==============================] - 2s 298us/sample - loss: 1639.0696 - acc: 0.3036 - val_loss: 856.7702 - val_acc: 0.2934\n",
+      "Epoch 333/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 3252.4356 - acc: 0.3037 - val_loss: 877.9932 - val_acc: 0.2934\n",
+      "Epoch 334/1000\n",
+      "7471/7471 [==============================] - 2s 301us/sample - loss: 2773.5997 - acc: 0.3037 - val_loss: 874.0466 - val_acc: 0.2934\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 335/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 1916.4134 - acc: 0.3033 - val_loss: 874.4499 - val_acc: 0.2934\n",
+      "Epoch 336/1000\n",
+      "7471/7471 [==============================] - 2s 295us/sample - loss: 2325.5570 - acc: 0.3038 - val_loss: 864.0517 - val_acc: 0.2934\n",
+      "Epoch 337/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 1180.6217 - acc: 0.3036 - val_loss: 868.0378 - val_acc: 0.2934\n",
+      "Epoch 338/1000\n",
+      "7471/7471 [==============================] - 2s 295us/sample - loss: 1201.6576 - acc: 0.3034 - val_loss: 840.9368 - val_acc: 0.2934\n",
+      "Epoch 339/1000\n",
+      "7471/7471 [==============================] - 2s 288us/sample - loss: 2844.6925 - acc: 0.3037 - val_loss: 866.7278 - val_acc: 0.2934\n",
+      "Epoch 340/1000\n",
+      "7471/7471 [==============================] - 2s 298us/sample - loss: 510.5965 - acc: 0.3033 - val_loss: 849.5289 - val_acc: 0.2934\n",
+      "Epoch 341/1000\n",
+      "7471/7471 [==============================] - 2s 292us/sample - loss: 1350.7321 - acc: 0.3041 - val_loss: 879.6613 - val_acc: 0.2934\n",
+      "Epoch 342/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 1011.4771 - acc: 0.3034 - val_loss: 913.2400 - val_acc: 0.2934\n",
+      "Epoch 343/1000\n",
+      "7471/7471 [==============================] - 2s 289us/sample - loss: 2763.7265 - acc: 0.3037 - val_loss: 891.1762 - val_acc: 0.2934\n",
+      "Epoch 344/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 2159.0513 - acc: 0.3036 - val_loss: 851.3194 - val_acc: 0.2934\n",
+      "Epoch 345/1000\n",
+      "7471/7471 [==============================] - 2s 296us/sample - loss: 649.9585 - acc: 0.3036 - val_loss: 856.5355 - val_acc: 0.2934\n",
+      "Epoch 346/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 661.1231 - acc: 0.3036 - val_loss: 845.5014 - val_acc: 0.2934\n",
+      "Epoch 347/1000\n",
+      "7471/7471 [==============================] - 2s 298us/sample - loss: 1430.8975 - acc: 0.3040 - val_loss: 883.9101 - val_acc: 0.2934\n",
+      "Epoch 348/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 1177.3915 - acc: 0.3036 - val_loss: 890.1381 - val_acc: 0.2934\n",
+      "Epoch 349/1000\n",
+      "7471/7471 [==============================] - 2s 280us/sample - loss: 1524.1081 - acc: 0.3040 - val_loss: 872.1853 - val_acc: 0.2934\n",
+      "Epoch 350/1000\n",
+      "7471/7471 [==============================] - 2s 281us/sample - loss: 3844.6394 - acc: 0.3042 - val_loss: 881.5820 - val_acc: 0.2934\n",
+      "Epoch 351/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 693.0262 - acc: 0.3036 - val_loss: 864.6768 - val_acc: 0.2934\n",
+      "Epoch 352/1000\n",
+      "7471/7471 [==============================] - 2s 253us/sample - loss: 2764.9413 - acc: 0.3041 - val_loss: 857.2993 - val_acc: 0.2934\n",
+      "Epoch 353/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 1467.9511 - acc: 0.3040 - val_loss: 837.9330 - val_acc: 0.2934\n",
+      "Epoch 354/1000\n",
+      "7471/7471 [==============================] - 2s 263us/sample - loss: 1611.5476 - acc: 0.3040 - val_loss: 885.2949 - val_acc: 0.2934\n",
+      "Epoch 355/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 1579.7541 - acc: 0.3041 - val_loss: 869.0563 - val_acc: 0.2934\n",
+      "Epoch 356/1000\n",
+      "7471/7471 [==============================] - 2s 247us/sample - loss: 1053.2757 - acc: 0.3037 - val_loss: 831.4042 - val_acc: 0.2934\n",
+      "Epoch 357/1000\n",
+      "7471/7471 [==============================] - 2s 254us/sample - loss: 795.1317 - acc: 0.3040 - val_loss: 830.7447 - val_acc: 0.2934\n",
+      "Epoch 358/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 2765.6704 - acc: 0.3038 - val_loss: 862.9408 - val_acc: 0.2934\n",
+      "Epoch 359/1000\n",
+      "7471/7471 [==============================] - 2s 223us/sample - loss: 2708.2644 - acc: 0.3040 - val_loss: 843.2175 - val_acc: 0.2934\n",
+      "Epoch 360/1000\n",
+      "7471/7471 [==============================] - 2s 247us/sample - loss: 2280.9985 - acc: 0.3038 - val_loss: 836.6980 - val_acc: 0.2934\n",
+      "Epoch 361/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 1066.4247 - acc: 0.3037 - val_loss: 807.6967 - val_acc: 0.2934\n",
+      "Epoch 362/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2075.4585 - acc: 0.3040 - val_loss: 820.4278 - val_acc: 0.2934\n",
+      "Epoch 363/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 1567.3407 - acc: 0.3036 - val_loss: 833.2382 - val_acc: 0.2934\n",
+      "Epoch 364/1000\n",
+      "7471/7471 [==============================] - 2s 260us/sample - loss: 1537.4222 - acc: 0.3037 - val_loss: 864.7683 - val_acc: 0.2934\n",
+      "Epoch 365/1000\n",
+      "7471/7471 [==============================] - 2s 252us/sample - loss: 1817.7503 - acc: 0.3034 - val_loss: 848.9397 - val_acc: 0.2934\n",
+      "Epoch 366/1000\n",
+      "7471/7471 [==============================] - 2s 257us/sample - loss: 522.2499 - acc: 0.3036 - val_loss: 842.7390 - val_acc: 0.2934\n",
+      "Epoch 367/1000\n",
+      "7471/7471 [==============================] - 2s 253us/sample - loss: 732.7610 - acc: 0.3034 - val_loss: 853.0763 - val_acc: 0.2934\n",
+      "Epoch 368/1000\n",
+      "7471/7471 [==============================] - 2s 287us/sample - loss: 810.5990 - acc: 0.3036 - val_loss: 771.8336 - val_acc: 0.2934\n",
+      "Epoch 369/1000\n",
+      "7471/7471 [==============================] - 2s 288us/sample - loss: 1006.5216 - acc: 0.3034 - val_loss: 787.4203 - val_acc: 0.2934\n",
+      "Epoch 370/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 4706.2548 - acc: 0.3038 - val_loss: 839.8871 - val_acc: 0.2934\n",
+      "Epoch 371/1000\n",
+      "7471/7471 [==============================] - 2s 306us/sample - loss: 5886.6153 - acc: 0.3037 - val_loss: 855.9647 - val_acc: 0.2934\n",
+      "Epoch 372/1000\n",
+      "7471/7471 [==============================] - 2s 301us/sample - loss: 3405.6971 - acc: 0.3037 - val_loss: 835.4464 - val_acc: 0.2934\n",
+      "Epoch 373/1000\n",
+      "7471/7471 [==============================] - 2s 300us/sample - loss: 2506.1119 - acc: 0.3038 - val_loss: 837.1990 - val_acc: 0.2934\n",
+      "Epoch 374/1000\n",
+      "7471/7471 [==============================] - 2s 290us/sample - loss: 641.2589 - acc: 0.3034 - val_loss: 847.2129 - val_acc: 0.2934\n",
+      "Epoch 375/1000\n",
+      "7471/7471 [==============================] - 2s 300us/sample - loss: 2345.2144 - acc: 0.3038 - val_loss: 830.3282 - val_acc: 0.2934\n",
+      "Epoch 376/1000\n",
+      "7471/7471 [==============================] - 2s 318us/sample - loss: 2169.0794 - acc: 0.3034 - val_loss: 854.1930 - val_acc: 0.2934\n",
+      "Epoch 377/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 3676.1549 - acc: 0.3034 - val_loss: 879.9136 - val_acc: 0.2934\n",
+      "Epoch 378/1000\n",
+      "7471/7471 [==============================] - 2s 302us/sample - loss: 1454.2471 - acc: 0.3038 - val_loss: 861.3557 - val_acc: 0.2934\n",
+      "Epoch 379/1000\n",
+      "7471/7471 [==============================] - 2s 288us/sample - loss: 2808.7670 - acc: 0.3034 - val_loss: 845.5178 - val_acc: 0.2934\n",
+      "Epoch 380/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 1871.1956 - acc: 0.3038 - val_loss: 842.6896 - val_acc: 0.2934\n",
+      "Epoch 381/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 2506.0377 - acc: 0.3040 - val_loss: 840.7045 - val_acc: 0.2934\n",
+      "Epoch 382/1000\n",
+      "7471/7471 [==============================] - 2s 294us/sample - loss: 2430.3888 - acc: 0.3038 - val_loss: 881.0041 - val_acc: 0.2934\n",
+      "Epoch 383/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 1219.8305 - acc: 0.3037 - val_loss: 871.7503 - val_acc: 0.2934\n",
+      "Epoch 384/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 2713.0065 - acc: 0.3041 - val_loss: 846.0730 - val_acc: 0.2934\n",
+      "Epoch 385/1000\n",
+      "7471/7471 [==============================] - 2s 295us/sample - loss: 1203.9054 - acc: 0.3034 - val_loss: 855.0016 - val_acc: 0.2934\n",
+      "Epoch 386/1000\n",
+      "7471/7471 [==============================] - 2s 297us/sample - loss: 1173.8863 - acc: 0.3029 - val_loss: 853.3017 - val_acc: 0.2934\n",
+      "Epoch 387/1000\n",
+      "7471/7471 [==============================] - 2s 288us/sample - loss: 1297.9864 - acc: 0.3033 - val_loss: 831.2844 - val_acc: 0.2934\n",
+      "Epoch 388/1000\n",
+      "7471/7471 [==============================] - 2s 304us/sample - loss: 2095.0200 - acc: 0.3036 - val_loss: 855.3801 - val_acc: 0.2934\n",
+      "Epoch 389/1000\n",
+      "7471/7471 [==============================] - 2s 303us/sample - loss: 2120.4842 - acc: 0.3032 - val_loss: 853.0179 - val_acc: 0.2934\n",
+      "Epoch 390/1000\n",
+      "7471/7471 [==============================] - 2s 296us/sample - loss: 2077.1910 - acc: 0.3036 - val_loss: 827.4065 - val_acc: 0.2934\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 391/1000\n",
+      "7471/7471 [==============================] - 2s 294us/sample - loss: 1162.2257 - acc: 0.3036 - val_loss: 855.1940 - val_acc: 0.2934\n",
+      "Epoch 392/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 856.0711 - acc: 0.3037 - val_loss: 861.9984 - val_acc: 0.2934\n",
+      "Epoch 393/1000\n",
+      "7471/7471 [==============================] - 2s 300us/sample - loss: 700.9480 - acc: 0.3036 - val_loss: 830.0550 - val_acc: 0.2934\n",
+      "Epoch 394/1000\n",
+      "7471/7471 [==============================] - 2s 281us/sample - loss: 2797.2206 - acc: 0.3040 - val_loss: 862.4113 - val_acc: 0.2934\n",
+      "Epoch 395/1000\n",
+      "7471/7471 [==============================] - 2s 294us/sample - loss: 21479085410.0979 - acc: 0.3040 - val_loss: 914.4870 - val_acc: 0.2934\n",
+      "Epoch 396/1000\n",
+      "7471/7471 [==============================] - 2s 290us/sample - loss: 2394.4510 - acc: 0.3022 - val_loss: 899.4231 - val_acc: 0.2934\n",
+      "Epoch 397/1000\n",
+      "7471/7471 [==============================] - 2s 281us/sample - loss: 2540.9477 - acc: 0.3034 - val_loss: 883.2219 - val_acc: 0.2934\n",
+      "Epoch 398/1000\n",
+      "7471/7471 [==============================] - 2s 283us/sample - loss: 2772.6490 - acc: 0.3041 - val_loss: 881.5400 - val_acc: 0.2934\n",
+      "Epoch 399/1000\n",
+      "7471/7471 [==============================] - 2s 307us/sample - loss: 1939.0769 - acc: 0.3036 - val_loss: 863.2548 - val_acc: 0.2934\n",
+      "Epoch 400/1000\n",
+      "7471/7471 [==============================] - 2s 296us/sample - loss: 2807.0932 - acc: 0.3040 - val_loss: 863.4011 - val_acc: 0.2934\n",
+      "Epoch 401/1000\n",
+      "7471/7471 [==============================] - 2s 260us/sample - loss: 1157.1802 - acc: 0.3036 - val_loss: 854.3962 - val_acc: 0.2934\n",
+      "Epoch 402/1000\n",
+      "7471/7471 [==============================] - 2s 268us/sample - loss: 2939.5853 - acc: 0.3029 - val_loss: 863.0549 - val_acc: 0.2934\n",
+      "Epoch 403/1000\n",
+      "7471/7471 [==============================] - 2s 301us/sample - loss: 2561.3048 - acc: 0.3036 - val_loss: 860.8516 - val_acc: 0.2934\n",
+      "Epoch 404/1000\n",
+      "7471/7471 [==============================] - 2s 289us/sample - loss: 700.7309 - acc: 0.3045 - val_loss: 864.5075 - val_acc: 0.2934\n",
+      "Epoch 405/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 847.7142 - acc: 0.3038 - val_loss: 857.5828 - val_acc: 0.2934\n",
+      "Epoch 406/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 2857.8679 - acc: 0.3036 - val_loss: 852.3638 - val_acc: 0.2934\n",
+      "Epoch 407/1000\n",
+      "7471/7471 [==============================] - 2s 249us/sample - loss: 623.4387 - acc: 0.3030 - val_loss: 870.7871 - val_acc: 0.2934\n",
+      "Epoch 408/1000\n",
+      "7471/7471 [==============================] - 2s 260us/sample - loss: 1604.9987 - acc: 0.3028 - val_loss: 893.8345 - val_acc: 0.2934\n",
+      "Epoch 409/1000\n",
+      "7471/7471 [==============================] - 2s 230us/sample - loss: 2860.0205 - acc: 0.3037 - val_loss: 897.5095 - val_acc: 0.2934\n",
+      "Epoch 410/1000\n",
+      "7471/7471 [==============================] - 2s 209us/sample - loss: 2747.3041 - acc: 0.3037 - val_loss: 897.6114 - val_acc: 0.2934\n",
+      "Epoch 411/1000\n",
+      "7471/7471 [==============================] - 2s 239us/sample - loss: 2546.1262 - acc: 0.3038 - val_loss: 886.2008 - val_acc: 0.2934\n",
+      "Epoch 412/1000\n",
+      "7471/7471 [==============================] - 2s 227us/sample - loss: 711.9297 - acc: 0.3038 - val_loss: 878.1413 - val_acc: 0.2934\n",
+      "Epoch 413/1000\n",
+      "7471/7471 [==============================] - 2s 250us/sample - loss: 18770196224.7755 - acc: 0.3033 - val_loss: 883.6255 - val_acc: 0.2934\n",
+      "Epoch 414/1000\n",
+      "7471/7471 [==============================] - 2s 248us/sample - loss: 2757.5361 - acc: 0.3037 - val_loss: 885.8022 - val_acc: 0.2934\n",
+      "Epoch 415/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 980.1665 - acc: 0.3036 - val_loss: 867.8966 - val_acc: 0.2934\n",
+      "Epoch 416/1000\n",
+      "7471/7471 [==============================] - 2s 255us/sample - loss: 2134.6661 - acc: 0.3037 - val_loss: 855.6277 - val_acc: 0.2934\n",
+      "Epoch 417/1000\n",
+      "7471/7471 [==============================] - 2s 266us/sample - loss: 2771.9527 - acc: 0.3040 - val_loss: 870.2135 - val_acc: 0.2934\n",
+      "Epoch 418/1000\n",
+      "7471/7471 [==============================] - 2s 296us/sample - loss: 1965.7480 - acc: 0.3040 - val_loss: 856.1176 - val_acc: 0.2934\n",
+      "Epoch 419/1000\n",
+      "7471/7471 [==============================] - 2s 273us/sample - loss: 2460.8880 - acc: 0.3037 - val_loss: 837.2577 - val_acc: 0.2934\n",
+      "Epoch 420/1000\n",
+      "7471/7471 [==============================] - 2s 264us/sample - loss: 2429.9615 - acc: 0.3040 - val_loss: 837.6280 - val_acc: 0.2934\n",
+      "Epoch 421/1000\n",
+      "7471/7471 [==============================] - 2s 305us/sample - loss: 1893.2535 - acc: 0.3036 - val_loss: 839.7593 - val_acc: 0.2934\n",
+      "Epoch 422/1000\n",
+      "7471/7471 [==============================] - 2s 296us/sample - loss: 10163.0204 - acc: 0.3042 - val_loss: 891.0819 - val_acc: 0.2934\n",
+      "Epoch 423/1000\n",
+      "7471/7471 [==============================] - 2s 293us/sample - loss: 2753.6619 - acc: 0.3045 - val_loss: 888.3219 - val_acc: 0.2934\n",
+      "Epoch 424/1000\n",
+      "7471/7471 [==============================] - 2s 306us/sample - loss: 2759.7633 - acc: 0.3042 - val_loss: 886.1788 - val_acc: 0.2934\n",
+      "Epoch 425/1000\n",
+      "7471/7471 [==============================] - 2s 304us/sample - loss: 2445.1806 - acc: 0.3034 - val_loss: 891.1094 - val_acc: 0.2934\n",
+      "Epoch 426/1000\n",
+      "7471/7471 [==============================] - 2s 295us/sample - loss: 1423.5925 - acc: 0.3040 - val_loss: 875.9071 - val_acc: 0.2934\n",
+      "Epoch 427/1000\n",
+      "7471/7471 [==============================] - 2s 283us/sample - loss: 2191.0489 - acc: 0.3037 - val_loss: 872.9215 - val_acc: 0.2934\n",
+      "Epoch 428/1000\n",
+      "7471/7471 [==============================] - 2s 288us/sample - loss: 2411.7319 - acc: 0.3033 - val_loss: 856.7836 - val_acc: 0.2934\n",
+      "Epoch 429/1000\n",
+      "7471/7471 [==============================] - 2s 308us/sample - loss: 1224.3891 - acc: 0.3034 - val_loss: 858.3044 - val_acc: 0.2934\n",
+      "Epoch 430/1000\n",
+      "7471/7471 [==============================] - 2s 292us/sample - loss: 2633.2946 - acc: 0.3041 - val_loss: 856.1555 - val_acc: 0.2934\n",
+      "Epoch 431/1000\n",
+      "7471/7471 [==============================] - 2s 299us/sample - loss: 2740.3601 - acc: 0.3040 - val_loss: 867.7578 - val_acc: 0.2934\n",
+      "Epoch 432/1000\n",
+      "7471/7471 [==============================] - 2s 305us/sample - loss: 2329.1651 - acc: 0.3038 - val_loss: 840.7502 - val_acc: 0.2934\n",
+      "Epoch 433/1000\n",
+      "7471/7471 [==============================] - 2s 305us/sample - loss: 659.4860 - acc: 0.3042 - val_loss: 839.5439 - val_acc: 0.2934\n",
+      "Epoch 434/1000\n",
+      "7471/7471 [==============================] - 2s 311us/sample - loss: 13550260655.4007 - acc: 0.3037 - val_loss: 882.8001 - val_acc: 0.2934\n",
+      "Epoch 435/1000\n",
+      "7471/7471 [==============================] - 2s 288us/sample - loss: 1353.1694 - acc: 0.3029 - val_loss: 887.2053 - val_acc: 0.2934\n",
+      "Epoch 436/1000\n",
+      "7471/7471 [==============================] - 2s 302us/sample - loss: 826.6550 - acc: 0.3040 - val_loss: 861.4441 - val_acc: 0.2934\n",
+      "Epoch 437/1000\n",
+      "7471/7471 [==============================] - 2s 304us/sample - loss: 1935.8692 - acc: 0.3040 - val_loss: 873.4214 - val_acc: 0.2934\n",
+      "Epoch 438/1000\n",
+      "7471/7471 [==============================] - 2s 302us/sample - loss: 1223.6022 - acc: 0.3034 - val_loss: 855.4188 - val_acc: 0.2934\n",
+      "Epoch 439/1000\n",
+      "7471/7471 [==============================] - 2s 301us/sample - loss: 1936.3369 - acc: 0.3028 - val_loss: 882.4066 - val_acc: 0.2934\n",
+      "Epoch 440/1000\n",
+      "7471/7471 [==============================] - 2s 300us/sample - loss: 2576.9221 - acc: 0.3036 - val_loss: 879.8121 - val_acc: 0.2934\n",
+      "Epoch 441/1000\n",
+      "7471/7471 [==============================] - 2s 291us/sample - loss: 2774.9043 - acc: 0.3030 - val_loss: 883.9243 - val_acc: 0.2934\n",
+      "Epoch 442/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 2724.5030 - acc: 0.3026 - val_loss: 870.2104 - val_acc: 0.2934\n",
+      "Epoch 443/1000\n",
+      "7471/7471 [==============================] - 2s 289us/sample - loss: 2651.9503 - acc: 0.3033 - val_loss: 860.0240 - val_acc: 0.2934\n",
+      "Epoch 444/1000\n",
+      "7471/7471 [==============================] - 2s 285us/sample - loss: 951.1779 - acc: 0.3028 - val_loss: 871.4749 - val_acc: 0.2934\n",
+      "Epoch 445/1000\n",
+      "7471/7471 [==============================] - 2s 282us/sample - loss: 1627.1954 - acc: 0.3040 - val_loss: 870.1734 - val_acc: 0.2934\n",
+      "Epoch 446/1000\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "7471/7471 [==============================] - 2s 288us/sample - loss: 1444.4000 - acc: 0.3036 - val_loss: 867.7516 - val_acc: 0.2934\n",
+      "Epoch 447/1000\n",
+      "7471/7471 [==============================] - 2s 290us/sample - loss: 1569.4404 - acc: 0.3037 - val_loss: 862.0426 - val_acc: 0.2934\n",
+      "Epoch 448/1000\n",
+      "7471/7471 [==============================] - 2s 302us/sample - loss: 1389.0356 - acc: 0.3038 - val_loss: 859.4009 - val_acc: 0.2934\n",
+      "Epoch 449/1000\n",
+      "7471/7471 [==============================] - 2s 299us/sample - loss: 928.6494 - acc: 0.3036 - val_loss: 846.8223 - val_acc: 0.2934\n",
+      "Epoch 450/1000\n",
+      "7471/7471 [==============================] - 2s 286us/sample - loss: 3861.1548 - acc: 0.3036 - val_loss: 883.5733 - val_acc: 0.2934\n",
+      "Epoch 451/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 936.3870 - acc: 0.3037 - val_loss: 832.1987 - val_acc: 0.2934\n",
+      "Epoch 452/1000\n",
+      "7471/7471 [==============================] - 2s 240us/sample - loss: 2679.4578 - acc: 0.3034 - val_loss: 896.6186 - val_acc: 0.2934\n",
+      "Epoch 453/1000\n",
+      "7471/7471 [==============================] - 2s 252us/sample - loss: 1267.2781 - acc: 0.3041 - val_loss: 895.9117 - val_acc: 0.2934\n",
+      "Epoch 454/1000\n",
+      "7471/7471 [==============================] - 2s 214us/sample - loss: 934.8494 - acc: 0.3038 - val_loss: 857.5967 - val_acc: 0.2934\n",
+      "Epoch 455/1000\n",
+      "7471/7471 [==============================] - 2s 261us/sample - loss: 607.3505 - acc: 0.3045 - val_loss: 860.1240 - val_acc: 0.2934\n",
+      "Epoch 456/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 2144.4661 - acc: 0.3041 - val_loss: 882.4195 - val_acc: 0.2934\n",
+      "Epoch 457/1000\n",
+      "7471/7471 [==============================] - 2s 276us/sample - loss: 2505.5709 - acc: 0.3040 - val_loss: 870.3628 - val_acc: 0.2934\n",
+      "Epoch 458/1000\n",
+      "7471/7471 [==============================] - 2s 277us/sample - loss: 1678.9792 - acc: 0.3033 - val_loss: 863.7073 - val_acc: 0.2934\n",
+      "Epoch 459/1000\n",
+      "7471/7471 [==============================] - 2s 265us/sample - loss: 584.2248 - acc: 0.3037 - val_loss: 858.6182 - val_acc: 0.2934\n",
+      "Epoch 460/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 701.9678 - acc: 0.3033 - val_loss: 900.4899 - val_acc: 0.2934\n",
+      "Epoch 461/1000\n",
+      "7471/7471 [==============================] - 2s 274us/sample - loss: 2240.5375 - acc: 0.3038 - val_loss: 885.3589 - val_acc: 0.2934\n",
+      "Epoch 462/1000\n",
+      "7471/7471 [==============================] - 2s 286us/sample - loss: 3135.7327 - acc: 0.3034 - val_loss: 902.0569 - val_acc: 0.2934\n",
+      "Epoch 463/1000\n",
+      "7471/7471 [==============================] - 2s 290us/sample - loss: 2418.7061 - acc: 0.3034 - val_loss: 871.0058 - val_acc: 0.2934\n",
+      "Epoch 464/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 670.1307 - acc: 0.3042 - val_loss: 871.8804 - val_acc: 0.2934\n",
+      "Epoch 465/1000\n",
+      "7471/7471 [==============================] - 2s 271us/sample - loss: 2746.2170 - acc: 0.3036 - val_loss: 871.8953 - val_acc: 0.2934\n",
+      "Epoch 466/1000\n",
+      "7471/7471 [==============================] - 2s 251us/sample - loss: 2726.3872 - acc: 0.3026 - val_loss: 861.6803 - val_acc: 0.2934\n",
+      "Epoch 467/1000\n",
+      "7471/7471 [==============================] - 2s 223us/sample - loss: 1098.7468 - acc: 0.3038 - val_loss: 846.5494 - val_acc: 0.2934\n",
+      "Epoch 468/1000\n",
+      "7471/7471 [==============================] - 2s 251us/sample - loss: 1064.5890 - acc: 0.3034 - val_loss: 833.7445 - val_acc: 0.2934\n",
+      "Epoch 469/1000\n",
+      "7471/7471 [==============================] - 2s 228us/sample - loss: 2755.0348 - acc: 0.3036 - val_loss: 845.4214 - val_acc: 0.2934\n",
+      "Epoch 470/1000\n",
+      "7471/7471 [==============================] - 2s 248us/sample - loss: 2256.4626 - acc: 0.3037 - val_loss: 827.3713 - val_acc: 0.2934\n",
+      "Epoch 471/1000\n",
+      "7471/7471 [==============================] - 2s 250us/sample - loss: 2742.8887 - acc: 0.3033 - val_loss: 834.0482 - val_acc: 0.2934\n",
+      "Epoch 472/1000\n",
+      "7471/7471 [==============================] - 2s 241us/sample - loss: 2119.3599 - acc: 0.3033 - val_loss: 845.3355 - val_acc: 0.2934\n",
+      "Epoch 473/1000\n",
+      "7471/7471 [==============================] - 2s 272us/sample - loss: 2068.5579 - acc: 0.3033 - val_loss: 886.4622 - val_acc: 0.2934\n",
+      "Epoch 474/1000\n",
+      "7471/7471 [==============================] - 2s 294us/sample - loss: 1924.3926 - acc: 0.3036 - val_loss: 867.1671 - val_acc: 0.2934\n",
+      "Epoch 475/1000\n",
+      "7471/7471 [==============================] - 2s 236us/sample - loss: 2447.4945 - acc: 0.3028 - val_loss: 875.0373 - val_acc: 0.2934\n",
+      "Epoch 476/1000\n",
+      "7471/7471 [==============================] - 2s 208us/sample - loss: 1124.7125 - acc: 0.3037 - val_loss: 834.8340 - val_acc: 0.2934\n",
+      "Epoch 477/1000\n",
+      "7471/7471 [==============================] - 2s 234us/sample - loss: 1964.9250 - acc: 0.3034 - val_loss: 829.2195 - val_acc: 0.2934\n",
+      "Epoch 478/1000\n",
+      "7471/7471 [==============================] - 2s 287us/sample - loss: 1118.6552 - acc: 0.3041 - val_loss: 857.4392 - val_acc: 0.2934\n",
+      "Epoch 479/1000\n",
+      "7471/7471 [==============================] - 2s 258us/sample - loss: 1272.3549 - acc: 0.3037 - val_loss: 856.1778 - val_acc: 0.2934\n",
+      "Epoch 480/1000\n",
+      "7471/7471 [==============================] - 2s 270us/sample - loss: 2629.3520 - acc: 0.3040 - val_loss: 846.3142 - val_acc: 0.2934\n",
+      "Epoch 481/1000\n",
+      "7471/7471 [==============================] - 2s 269us/sample - loss: 2033.1426 - acc: 0.3038 - val_loss: 856.2838 - val_acc: 0.2934\n",
+      "Epoch 482/1000\n",
+      "2880/7471 [==========>...................] - ETA: 1s - loss: 449.5774 - acc: 0.3132"
+     ]
+    },
+    {
+     "ename": "KeyboardInterrupt",
+     "evalue": "",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-10-8c2041403915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(input_x_train, input_y_train, \n\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           callbacks=[WandbCallback()])\n\u001b[0m",
+      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
+      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/wandb/keras/__init__.py\u001b[0m in \u001b[0;36mnew_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
+      "\u001b[0;32m/mnt/c/Users/Gilang R Ilhami/Desktop/personal_projects/blogpost/blogpost_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
+     ]
+    }
+   ],
+   "source": [
+    "model.fit(input_x_train, input_y_train, \n",
+    "          epochs=config.epochs, validation_data=(input_x_test, input_y_test), \n",
+    "          callbacks=[WandbCallback()])"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
